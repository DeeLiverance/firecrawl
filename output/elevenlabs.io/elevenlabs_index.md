# Knowledge Base

Source: https://elevenlabs.io/docs/
Raw JSON: `elevenlabs.json`

Crawl completed: 60 documents

## Index
1. [Documentation | ElevenLabs Documentation](#doc-1) – https://elevenlabs.io/docs/ (id: 1)
2. [Prompting guide | ElevenLabs Documentation](#doc-2) – https://elevenlabs.io/docs/agents-platform/best-practices/prompting-guide (id: 2)
3. [Build | ElevenLabs Documentation](#doc-3) – https://elevenlabs.io/docs/agents-platform/build/overview (id: 3)
4. [Workflows | ElevenLabs Documentation](#doc-4) – https://elevenlabs.io/docs/agents-platform/customization/agent-workflows (id: 4)
5. [Conversation flow | ElevenLabs Documentation](#doc-5) – https://elevenlabs.io/docs/agents-platform/customization/conversation-flow (id: 5)
6. [Knowledge base | ElevenLabs Documentation](#doc-6) – https://elevenlabs.io/docs/agents-platform/customization/knowledge-base (id: 6)
7. [Knowledge base dashboard | ElevenLabs Documentation](#doc-7) – https://elevenlabs.io/docs/agents-platform/customization/knowledge-base/dashboard (id: 7)
8. [Retrieval-Augmented Generation | ElevenLabs Documentation](#doc-8) – https://elevenlabs.io/docs/agents-platform/customization/knowledge-base/rag (id: 8)
9. [Models | ElevenLabs Documentation](#doc-9) – https://elevenlabs.io/docs/agents-platform/customization/llm (id: 9)
10. [Personalization | ElevenLabs Documentation](#doc-10) – https://elevenlabs.io/docs/agents-platform/customization/personalization (id: 10)
11. [Dynamic variables | ElevenLabs Documentation](#doc-11) – https://elevenlabs.io/docs/agents-platform/customization/personalization/dynamic-variables (id: 11)
12. [Tools | ElevenLabs Documentation](#doc-12) – https://elevenlabs.io/docs/agents-platform/customization/tools (id: 12)
13. [Client tools | ElevenLabs Documentation](#doc-13) – https://elevenlabs.io/docs/agents-platform/customization/tools/client-tools (id: 13)
14. [Model Context Protocol | ElevenLabs Documentation](#doc-14) – https://elevenlabs.io/docs/agents-platform/customization/tools/mcp (id: 14)
15. [MCP integration security | ElevenLabs Documentation](#doc-15) – https://elevenlabs.io/docs/agents-platform/customization/tools/mcp/security (id: 15)
16. [Server tools | ElevenLabs Documentation](#doc-16) – https://elevenlabs.io/docs/agents-platform/customization/tools/server-tools (id: 16)
17. [System tools | ElevenLabs Documentation](#doc-17) – https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools (id: 17)
18. [Agent transfer | ElevenLabs Documentation](#doc-18) – https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/agent-transfer (id: 18)
19. [End call | ElevenLabs Documentation](#doc-19) – https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/end-call (id: 19)
20. [Language detection | ElevenLabs Documentation](#doc-20) – https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/language-detection (id: 20)
21. [Play keypad touch tone | ElevenLabs Documentation](#doc-21) – https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/play-keypad-touch-tone (id: 21)
22. [Skip turn | ElevenLabs Documentation](#doc-22) – https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/skip-turn (id: 22)
23. [Transfer to human | ElevenLabs Documentation](#doc-23) – https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/transfer-to-human (id: 23)
24. [Voicemail detection | ElevenLabs Documentation](#doc-24) – https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/voicemail-detection (id: 24)
25. [Tool Call Sounds | ElevenLabs Documentation](#doc-25) – https://elevenlabs.io/docs/agents-platform/customization/tools/tool-configuration/tool-call-sounds (id: 25)
26. [Voice customization | ElevenLabs Documentation](#doc-26) – https://elevenlabs.io/docs/agents-platform/customization/voice (id: 26)
27. [ElevenLabs Agents  voice design guide | ElevenLabs Documentation](#doc-27) – https://elevenlabs.io/docs/agents-platform/customization/voice/best-practices/conversational-voice-design (id: 27)
28. [Language | ElevenLabs Documentation](#doc-28) – https://elevenlabs.io/docs/agents-platform/customization/voice/customization/language (id: 28)
29. [Multi-voice support | ElevenLabs Documentation](#doc-29) – https://elevenlabs.io/docs/agents-platform/customization/voice/multi-voice-support (id: 29)
30. [Pronunciation dictionaries | ElevenLabs Documentation](#doc-30) – https://elevenlabs.io/docs/agents-platform/customization/voice/pronunciation-dictionary (id: 30)
31. [Speed control | ElevenLabs Documentation](#doc-31) – https://elevenlabs.io/docs/agents-platform/customization/voice/speed-control (id: 31)
32. [Agents Platform | ElevenLabs Documentation](#doc-32) – https://elevenlabs.io/docs/agents-platform/overview (id: 32)
33. [Quickstart | ElevenLabs Documentation](#doc-33) – https://elevenlabs.io/docs/agents-platform/quickstart (id: 33)
34. [Account | ElevenLabs Documentation](#doc-34) – https://elevenlabs.io/docs/overview/administration/account (id: 34)
35. [Billing | ElevenLabs Documentation](#doc-35) – https://elevenlabs.io/docs/overview/administration/billing (id: 35)
36. [Consolidated billing | ElevenLabs Documentation](#doc-36) – https://elevenlabs.io/docs/overview/administration/consolidated-billing (id: 36)
37. [Data residency | ElevenLabs Documentation](#doc-37) – https://elevenlabs.io/docs/overview/administration/data-residency (id: 37)
38. [Usage analytics | ElevenLabs Documentation](#doc-38) – https://elevenlabs.io/docs/overview/administration/usage-analytics (id: 38)
39. [Webhooks | ElevenLabs Documentation](#doc-39) – https://elevenlabs.io/docs/overview/administration/webhooks (id: 39)
40. [Workspaces | ElevenLabs Documentation](#doc-40) – https://elevenlabs.io/docs/overview/administration/workspaces/overview (id: 40)
41. [Service Accounts and API Keys | ElevenLabs Documentation](#doc-41) – https://elevenlabs.io/docs/overview/administration/workspaces/service-accounts (id: 41)
42. [Sharing resources | ElevenLabs Documentation](#doc-42) – https://elevenlabs.io/docs/overview/administration/workspaces/sharing-resources (id: 42)
43. [Single Sign-On (SSO) | ElevenLabs Documentation](#doc-43) – https://elevenlabs.io/docs/overview/administration/workspaces/sso (id: 43)
44. [User groups | ElevenLabs Documentation](#doc-44) – https://elevenlabs.io/docs/overview/administration/workspaces/user-groups (id: 44)
45. [Dubbing | ElevenLabs Documentation](#doc-45) – https://elevenlabs.io/docs/overview/capabilities/dubbing (id: 45)
46. [Forced Alignment | ElevenLabs Documentation](#doc-46) – https://elevenlabs.io/docs/overview/capabilities/forced-alignment (id: 46)
47. [Image & Video | ElevenLabs Documentation](#doc-47) – https://elevenlabs.io/docs/overview/capabilities/image-video (id: 47)
48. [Eleven Music | ElevenLabs Documentation](#doc-48) – https://elevenlabs.io/docs/overview/capabilities/music (id: 48)
49. [Best practices | ElevenLabs Documentation](#doc-49) – https://elevenlabs.io/docs/overview/capabilities/music/best-practices (id: 49)
50. [Sound effects | ElevenLabs Documentation](#doc-50) – https://elevenlabs.io/docs/overview/capabilities/sound-effects (id: 50)
51. [Transcription | ElevenLabs Documentation](#doc-51) – https://elevenlabs.io/docs/overview/capabilities/speech-to-text (id: 51)
52. [Text to Dialogue | ElevenLabs Documentation](#doc-52) – https://elevenlabs.io/docs/overview/capabilities/text-to-dialogue (id: 52)
53. [Text to Speech | ElevenLabs Documentation](#doc-53) – https://elevenlabs.io/docs/overview/capabilities/text-to-speech (id: 53)
54. [Best practices | ElevenLabs Documentation](#doc-54) – https://elevenlabs.io/docs/overview/capabilities/text-to-speech/best-practices (id: 54)
55. [Voice changer | ElevenLabs Documentation](#doc-55) – https://elevenlabs.io/docs/overview/capabilities/voice-changer (id: 55)
56. [Voice isolator | ElevenLabs Documentation](#doc-56) – https://elevenlabs.io/docs/overview/capabilities/voice-isolator (id: 56)
57. [Voice remixing | ElevenLabs Documentation](#doc-57) – https://elevenlabs.io/docs/overview/capabilities/voice-remixing (id: 57)
58. [Voices | ElevenLabs Documentation](#doc-58) – https://elevenlabs.io/docs/overview/capabilities/voices (id: 58)
59. [Documentation | ElevenLabs Documentation](#doc-59) – https://elevenlabs.io/docs/overview/intro (id: 59)
60. [Models | ElevenLabs Documentation](#doc-60) – https://elevenlabs.io/docs/overview/models (id: 60)

## Document 1 — Documentation | ElevenLabs Documentation {#doc-1}
[https://elevenlabs.io/docs/](https://elevenlabs.io/docs/)

[![](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F12097a437e55f60c199946cf59c9528eb8349d110142394833d67fe93b50e68d%2Fassets%2Fimages%2Foverview%2Fvoice-library-bg.webp&w=3840&q=75)\
\
### \
\
Creative Platform\
\
Learn how to use the ElevenLabs Creative Platform with step-by-step guides](https://elevenlabs.io/docs/creative-platform/overview)
[![](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F17a81505a62493491ead763b307b1e854825a0da67ab1a1d86b41b57ad87bc73%2Fassets%2Fimages%2Fagents%2Fagents-overview-integrate.png&w=3840&q=75)\
\
### \
\
Agents Platform\
\
Learn how to build, launch, and scale agents with ElevenLabs](https://elevenlabs.io/docs/agents-platform/overview)
[![](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F002b2432fa6ab18befc9f1a6e7fadf348f46506a5a5a72a2358ba1e7f92d8ded%2Fassets%2Fimages%2Foverview%2Fscribe-code-bg.webp&w=3840&q=75)\
\
### \
\
Developers\
\
Learn how to integrate ElevenLabs with examples and tutorials](https://elevenlabs.io/docs/developers/quickstart)

Meet the models
---------------

[Eleven v3\
\
![Alpha](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/df15d6e71021753b609c6d9f7cf3dcbff076208e67ee96a5849b3476ccf8915b/assets/icons/alpha.svg)\
\
Our most emotionally rich, expressive speech synthesis model\
\
Dramatic delivery and performance\
\
70+ languages supported\
\
5,000 character limit\
\
Support for natural multi-speaker dialogue](https://elevenlabs.io/docs/overview/models#eleven-v3-alpha)
[Eleven Multilingual v2\
\
Lifelike, consistent quality speech synthesis model\
\
Natural-sounding output\
\
29 languages supported\
\
10,000 character limit\
\
Most stable on long-form generations](https://elevenlabs.io/docs/overview/models#multilingual-v2)
[Eleven Flash v2.5\
\
Our fast, affordable speech synthesis model\
\
Ultra-low latency (~75ms†)\
\
32 languages supported\
\
40,000 character limit\
\
Faster model, 50% lower price per character](https://elevenlabs.io/docs/overview/models#flash-v25)
[Eleven Turbo v2.5\
\
High quality, low-latency model with a good balance of quality and speed\
\
High quality voice generation\
\
32 languages supported\
\
40,000 character limit\
\
Low latency (~250ms-300ms†), 50% lower price per character](https://elevenlabs.io/docs/overview/models#turbo-v25)

[Scribe v2\
\
State-of-the-art speech recognition model\
\
Accurate transcription in 90+ languages\
\
Keyterm prompting, up to 100 terms\
\
Entity detection, up to 56\
\
Precise word-level timestamps\
\
Speaker diarization, up to 48 speakers\
\
Dynamic audio tagging\
\
Smart language detection](https://elevenlabs.io/docs/overview/models#scribe-v2)
[Scribe v2 Realtime\
\
Real-time speech recognition model\
\
Accurate transcription in 90+ languages\
\
Real-time transcription\
\
Low latency (~150ms†)\
\
Precise word-level timestamps](https://elevenlabs.io/docs/overview/models#scribe-v2-realtime)

[Explore all](https://elevenlabs.io/docs/overview/models)

† Excluding application & network latency

Browse by capability
--------------------

[Text to Speech\
\
Convert text into lifelike speech](https://elevenlabs.io/docs/overview/capabilities/text-to-speech)
[Speech to Text\
\
Transcribe spoken audio into text](https://elevenlabs.io/docs/overview/capabilities/speech-to-text)
[Music\
\
Generate music from text](https://elevenlabs.io/docs/overview/capabilities/music)
[Text to Dialogue\
\
Create natural-sounding dialogue from text](https://elevenlabs.io/docs/overview/capabilities/text-to-dialogue)
[Image & Video\
\
Generate images and videos from text](https://elevenlabs.io/docs/overview/capabilities/image-video)
[Voice changer\
\
Modify and transform voices](https://elevenlabs.io/docs/overview/capabilities/voice-changer)
[Voice isolator\
\
Isolate voices from background noise](https://elevenlabs.io/docs/overview/capabilities/voice-isolator)
[Dubbing\
\
Dub audio and videos seamlessly](https://elevenlabs.io/docs/overview/capabilities/dubbing)
[Sound effects\
\
Create cinematic sound effects](https://elevenlabs.io/docs/overview/capabilities/sound-effects)
[Voices\
\
Clone and design custom voices](https://elevenlabs.io/docs/overview/capabilities/voices)
[Voice Remixing\
\
Transform and enhance existing voices](https://elevenlabs.io/docs/overview/capabilities/voice-remixing)
[Forced Alignment\
\
Align text to audio](https://elevenlabs.io/docs/overview/capabilities/forced-alignment)
[Agents Platform\
\
Deploy intelligent voice agents](https://elevenlabs.io/docs/agents-platform/overview)

## Document 2 — Prompting guide | ElevenLabs Documentation {#doc-2}
[https://elevenlabs.io/docs/agents-platform/best-practices/prompting-guide](https://elevenlabs.io/docs/agents-platform/best-practices/prompting-guide)

Introduction
------------

Effective prompting transforms [ElevenLabs Agents](https://elevenlabs.io/docs/agents-platform/overview)
 from robotic to lifelike.

![ElevenLabs Agents prompting guide](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F255df05f53675feaf54c765c4ee294fda00a7c14de1b02f155922012bf0a5433%2Fassets%2Fimages%2Fconversational-ai%2Fprompting-guide.jpg&w=3840&q=75)

A system prompt is the personality and policy blueprint of your AI agent. In enterprise use, it tends to be elaborate—defining the agent’s role, goals, allowable tools, step-by-step instructions for certain tasks, and guardrails describing what the agent should not do. The way you structure this prompt directly impacts reliability.

The system prompt controls conversational behavior and response style, but does not control conversation flow mechanics like turn-taking, or agent settings like which languages an agent can speak. These aspects are handled at the platform level.

![Enterprise agent reliability\
framework](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F18c7dd3bf58a6715656d588834a278dbc1f368eaed2cbf91aeea3e977c2631ed%2Fassets%2Fimages%2Fconversational-ai%2Fsystem-prompt-principles.png&w=3840&q=75)

Prompt engineering fundamentals
-------------------------------

A system prompt is the personality and policy blueprint of your AI agent. In enterprise use, it tends to be elaborate—defining the agent’s role, goals, allowable tools, step-by-step instructions for certain tasks, and guardrails describing what the agent should not do. The way you structure this prompt directly impacts reliability.

The following principles form the foundation of production-grade prompt engineering:

### Separate instructions into clean sections

Separating instructions into dedicated sections with markdown headings helps the model prioritize and interpret them correctly. Use whitespace and line breaks to separate instructions.

**Why this matters for reliability:** Models are tuned to pay extra attention to certain headings (especially `# Guardrails`), and clear section boundaries prevent instruction bleed where rules from one context affect another.

Less effective approachRecommended approach

|     |     |
| --- | --- |
| 1   | You are a customer service agent. Be polite and helpful. Never share sensitive data. You can look up orders and process refunds. Always verify identity first. Keep responses under 3 sentences unless the user asks for details. |

### Be as concise as possible

Keep every instruction short, clear, and action-based. Remove filler words and restate only what is essential for the model to act correctly.

**Why this matters for reliability:** Concise instructions reduce ambiguity and token usage. Every unnecessary word is a potential source of misinterpretation.

Less effective approachRecommended approach

|     |     |
| --- | --- |
| 1   | \# Tone |
| 2   |     |
| 3   | When you're talking to customers, you should try to be really friendly and approachable, making sure that you're speaking in a way that feels natural and conversational, kind of like how you'd talk to a friend, but still maintaining a professional demeanor that represents the company well. |

If you need the agent to maintain a specific tone, define it explicitly and concisely in the `# Personality` or `# Tone` section. Avoid repeating tone guidance throughout the prompt.

### Emphasize critical instructions

Highlight critical steps by adding “This step is important” at the end of the line. Repeating the most important 1-2 instructions twice in the prompt can help reinforce them.

**Why this matters for reliability:** In complex prompts, models may prioritize recent context over earlier instructions. Emphasis and repetition ensure critical rules aren’t overlooked.

Less effective approachRecommended approach

|     |     |
| --- | --- |
| 1   | \# Goal |
| 2   |     |
| 3   | Verify customer identity before accessing their account. |
| 4   | Look up order details and provide status updates. |
| 5   | Process refund requests when eligible. |

### Normalize inputs and outputs

Voice agents often misinterpret or misformat structured information such as emails, IDs, or record locators. To ensure accuracy, separate (or “normalize”) how data is spoken to the user from how it is written when used in tools or APIs.

**Why this matters for reliability:** Text-to-speech models sometimes mispronounce symbols like ”@” or ”.” naturally, for example when an agent speaks “[john@company.com](mailto:john@company.com)
” directly. Normalizing to spoken format (“john at company dot com”) creates natural, understandable speech while maintaining correct written format for tools.

Less effective approachRecommended approach

|     |     |
| --- | --- |
| 1   | When collecting the customer's email, repeat it back to them exactly as they said it, then use it in the \`lookupAccount\` tool. |

Add character normalization rules to your system prompt when agents collect emails, phone numbers, confirmation codes, or other structured identifiers that will be passed to tools.

### Provide clear examples

Include examples in the prompt to illustrate how agents should behave, use tools, or format data. Large language models follow instructions more reliably when they have concrete examples to reference.

**Why this matters for reliability:** Examples reduce ambiguity and provide a reference pattern. They’re especially valuable for complex formatting, multi-step processes, and edge cases.

Less effective approachRecommended approach

|     |     |
| --- | --- |
| 1   | When a customer provides a confirmation code, make sure to format it correctly before looking it up. |

### Dedicate a guardrails section

List all non-negotiable rules the model must always follow in a dedicated `# Guardrails` section. Models are tuned to pay extra attention to this heading.

**Why this matters for reliability:** Guardrails prevent inappropriate responses and ensure compliance with policies. Centralizing them in a dedicated section makes them easier to audit and update.

Recommended approach

|     |     |
| --- | --- |
| 1   | \# Guardrails |
| 2   |     |
| 3   | Never share customer data across conversations or reveal sensitive account information without proper verification. |
| 4   | Never process refunds over $500 without supervisor approval. |
| 5   | Never make promises about delivery dates that aren't confirmed in the order system. |
| 6   | Acknowledge when you don't know an answer instead of guessing. |
| 7   | If a customer becomes abusive, politely end the conversation and offer to escalate to a supervisor. |

To learn more about designing effective guardrails, see our guide on [safety and moderation](https://elevenlabs.io/docs/agents-platform/customization/privacy)
.

Tool configuration for reliability
----------------------------------

Agents capable of handling transactional workflows can be highly effective. To enable this, they must be equipped with tools that let them perform actions in other systems or fetch live data from them.

Equally important as prompt structure is how you describe the tools available to your agent. Clear, action-oriented tool definitions help the model invoke them correctly and recover gracefully from errors.

### Describe tools precisely with detailed parameters

When creating a tool, add descriptions to all parameters. This helps the LLM construct tool calls accurately.

**Tool description:** “Looks up customer order status by order ID and returns current status, estimated delivery date, and tracking number.”

**Parameter descriptions:**

*   `order_id` (required): “The unique order identifier, formatted as written characters (e.g., ‘ORD123456’)”
*   `include_history` (optional): “If true, returns full order history including status changes”

**Why this matters for reliability:** Parameter descriptions act as inline documentation for the model. They clarify format expectations, required vs. optional fields, and acceptable values.

### Explain when and how to use each tool in the system prompt

Clearly define in your system prompt when and how each tool should be used. Don’t rely solely on tool descriptions—provide usage context and sequencing logic.

Recommended approach

|     |     |
| --- | --- |
| 1   | \# Tools |
| 2   |     |
| 3   | You have access to the following tools: |
| 4   |     |
| 5   | \## \`getOrderStatus\` |
| 6   |     |
| 7   | Use this tool when a customer asks about their order. Always call this tool before providing order information—never rely on memory or assumptions. |
| 8   |     |
| 9   | \*\*When to use:\*\* |
| 10  |     |
| 11  | \- Customer asks "Where is my order?" |
| 12  | \- Customer provides an order number |
| 13  | \- Customer asks about delivery estimates |
| 14  |     |
| 15  | \*\*How to use:\*\* |
| 16  |     |
| 17  | 1\. Collect the order ID from the customer in spoken format |
| 18  | 2\. Convert to written format using character normalization rules |
| 19  | 3\. Call \`getOrderStatus\` with the formatted order ID |
| 20  | 4\. Present the results to the customer in natural language |
| 21  |     |
| 22  | \*\*Error handling:\*\* |
| 23  | If the tool returns "Order not found", ask the customer to verify the order number and try again. |
| 24  |     |
| 25  | \## \`processRefund\` |
| 26  |     |
| 27  | Use this tool only after verifying: |
| 28  |     |
| 29  | 1\. Customer identity has been confirmed |
| 30  | 2\. Order is eligible for refund (within 30 days, not already refunded) |
| 31  | 3\. Refund amount is under $500 (escalate to supervisor if over $500) |
| 32  |     |
| 33  | \*\*Required before calling:\*\* |
| 34  |     |
| 35  | \- Order ID (from \`getOrderStatus\`) |
| 36  | \- Refund reason code |
| 37  | \- Customer confirmation |
| 38  |     |
| 39  | This step is important: Always confirm refund details with the customer before calling this tool. |

### Use character normalization for tool inputs

When tools require structured identifiers (emails, phone numbers, codes), ensure the prompt clarifies when to use written vs. spoken formats.

Recommended approach

|     |     |
| --- | --- |
| 1   | \# Tools |
| 2   |     |
| 3   | \## \`lookupAccount\` |
| 4   |     |
| 5   | \*\*Parameters:\*\* |
| 6   |     |
| 7   | \- \`email\` (required): Customer email address in written format (e.g., "john.smith@company.com") |
| 8   |     |
| 9   | \*\*Usage:\*\* |
| 10  |     |
| 11  | 1\. Ask customer for their email in spoken format: "Can you provide your email address?" |
| 12  | 2\. Listen for spoken format: "john dot smith at company dot com" |
| 13  | 3\. Convert to written format: "john.smith@company.com" |
| 14  | 4\. Pass written format to this tool |
| 15  |     |
| 16  | \*\*Character normalization for email:\*\* |
| 17  |     |
| 18  | \- "at" → "@" |
| 19  | \- "dot" → "." |
| 20  | \- Remove spaces between words |

### Handle tool call failures gracefully

Tools can sometimes fail due to network issues, missing data, or other errors. Include clear instructions in your system prompt for recovery.

**Why this matters for reliability:** Tool failures are inevitable in production. Without explicit handling instructions, agents may hallucinate responses or provide incorrect information.

Recommended approach

|     |     |
| --- | --- |
| 1   | \# Tool error handling |
| 2   |     |
| 3   | If any tool call fails or returns an error: |
| 4   |     |
| 5   | 1\. Acknowledge the issue to the customer: "I'm having trouble accessing that information right now." |
| 6   | 2\. Do not guess or make up information |
| 7   | 3\. Offer alternatives: |
| 8   | - Try the tool again if it might be a temporary issue |
| 9   | - Offer to escalate to a human agent |
| 10  | - Provide a callback option |
| 11  | 4\. If the error persists after 2 attempts, escalate to a supervisor |
| 12  |     |
| 13  | \*\*Example responses:\*\* |
| 14  |     |
| 15  | \- "I'm having trouble looking up that order right now. Let me try again... \[retry\]" |
| 16  | \- "I'm unable to access the order system at the moment. I can transfer you to a specialist who can help, or we can schedule a callback. Which would you prefer?" |

For detailed guidance on building reliable tool integrations, see our documentation on [Client tools](https://elevenlabs.io/docs/agents-platform/customization/tools/client-tools)
, [Server tools](https://elevenlabs.io/docs/agents-platform/customization/tools/server-tools)
, and [MCP tools](https://elevenlabs.io/docs/agents-platform/customization/tools/mcp)
.

Architecture patterns for enterprise agents
-------------------------------------------

While strong prompts and tools form the foundation of agent reliability, production systems require thoughtful architectural design. Enterprise agents handle complex workflows that often exceed the scope of a single, monolithic prompt.

### Keep agents specialized

Overly broad instructions or large context windows increase latency and reduce accuracy. Each agent should have a narrow, clearly defined knowledge base and set of responsibilities.

**Why this matters for reliability:** Specialized agents have fewer edge cases to handle, clearer success criteria, and faster response times. They’re easier to test, debug, and improve.

A general-purpose “do everything” agent is harder to maintain and more likely to fail in production than a network of specialized agents with clear handoffs.

### Use orchestrator and specialist patterns

For complex tasks, design multi-agent workflows that hand off tasks between specialized agents—and to human operators when needed.

**Architecture pattern:**

1.  **Orchestrator agent:** Routes incoming requests to appropriate specialist agents based on intent classification
2.  **Specialist agents:** Handle domain-specific tasks (billing, scheduling, technical support, etc.)
3.  **Human escalation:** Defined handoff criteria for complex or sensitive cases

**Benefits of this pattern:**

*   Each specialist has a focused prompt and reduced context
*   Easier to update individual specialists without affecting the system
*   Clear metrics per domain (billing resolution rate, scheduling success rate, etc.)
*   Reduced latency per interaction (smaller prompts, faster inference)

### Define clear handoff criteria

When designing multi-agent workflows, specify exactly when and how control should transfer between agents or to human operators.

Orchestrator agent example

|     |     |
| --- | --- |
| 1   | \# Goal |
| 2   |     |
| 3   | Route customer requests to the appropriate specialist agent based on intent. |
| 4   |     |
| 5   | \## Routing logic |
| 6   |     |
| 7   | \*\*Billing specialist:\*\* Customer mentions payment, invoice, refund, charge, subscription, or account balance |
| 8   | \*\*Technical support specialist:\*\* Customer reports error, bug, issue, not working, broken |
| 9   | \*\*Scheduling specialist:\*\* Customer wants to book, reschedule, cancel, or check appointment |
| 10  | \*\*Human escalation:\*\* Customer is angry, requests supervisor, or issue is unresolved after 2 specialist attempts |
| 11  |     |
| 12  | \## Handoff process |
| 13  |     |
| 14  | 1\. Classify customer intent based on first message |
| 15  | 2\. Provide brief acknowledgment: "I'll connect you with our \[billing/technical/scheduling\] team." |
| 16  | 3\. Transfer conversation with context summary: |
| 17  | - Customer name |
| 18  | - Primary issue |
| 19  | - Any account identifiers already collected |
| 20  | 4\. Do not repeat information collection that already occurred |

Specialist agent example

|     |     |
| --- | --- |
| 1   | \# Personality |
| 2   |     |
| 3   | You are a billing specialist for Acme Corp. You handle payment issues, refunds, and subscription changes. |
| 4   |     |
| 5   | \# Goal |
| 6   |     |
| 7   | Resolve billing inquiries by: |
| 8   |     |
| 9   | 1\. Verifying customer identity |
| 10  | 2\. Looking up account and billing history |
| 11  | 3\. Processing refunds (under $500) or escalating (over $500) |
| 12  | 4\. Updating subscription settings when requested |
| 13  |     |
| 14  | \# Guardrails |
| 15  |     |
| 16  | Never access account information without identity verification. |
| 17  | Never process refunds over $500 without supervisor approval. |
| 18  | If the customer's issue is not billing-related, transfer back to the orchestrator agent. |

For detailed guidance on building multi-agent workflows, see our documentation on [Workflows](https://elevenlabs.io/docs/agents-platform/customization/agent-workflows)
.

Model selection for enterprise reliability
------------------------------------------

Selecting the right model depends on your performance requirements—particularly latency, accuracy, and tool-calling reliability. Different models offer different tradeoffs between speed, reasoning capability, and cost.

### Understand the tradeoffs

**Latency:** Smaller models (fewer parameters) generally respond faster, making them suitable for high-frequency, low-complexity interactions.

**Accuracy:** Larger models provide stronger reasoning capabilities and better handle complex, multi-step tasks, but with higher latency and cost.

**Tool-calling reliability:** Not all models handle tool/function calling with equal precision. Some excel at structured output, while others may require more explicit prompting.

### Model recommendations by use case

Based on deployments across millions of agent interactions, the following patterns emerge:

*   **GPT-4o or GLM 4.5 Air (recommended starting point):** Best for general-purpose enterprise agents where latency, accuracy, and cost must all be balanced. Offers low-to-moderate latency with strong tool-calling performance and reasonable cost per interaction. Ideal for customer support, scheduling, order management, and general inquiry handling.
    
*   **Gemini 2.5 Flash Lite (ultra-low latency):** Best for high-frequency, simple interactions where speed is critical. Provides the lowest latency with broad general knowledge, though with lower performance on complex tool-calling. Cost-effective at scale for initial routing/triage, simple FAQs, appointment confirmations, and basic data collection.
    
*   **Claude Sonnet 4 or 4.5 (complex reasoning):** Best for multi-step problem-solving, nuanced judgment, and complex tool orchestration. Offers the highest accuracy and reasoning capability with excellent tool-calling reliability, though with higher latency and cost. Ideal for tasks where mistakes are costly, such as technical troubleshooting, financial advisory, compliance-sensitive workflows, and complex refund/escalation decisions.
    

### Benchmark with your actual prompts

Model performance varies significantly based on prompt structure and task complexity. Before committing to a model:

1.  Test 2-3 candidate models with your actual system prompt
2.  Evaluate on real user queries or synthetic test cases
3.  Measure latency, accuracy, and tool-calling success rate
4.  Optimize for the best tradeoff given your specific requirements

For detailed model configuration options, see our [Models documentation](https://elevenlabs.io/docs/agents-platform/customization/llm)
.

Iteration and testing
---------------------

Reliability in production comes from continuous iteration. Even well-constructed prompts can fail in real use. What matters is learning from those failures and improving through disciplined testing.

### Configure evaluation criteria

Attach concrete evaluation criteria to each agent to monitor success over time and check for regressions.

**Key metrics to track:**

*   **Task completion rate:** Percentage of user intents successfully addressed
*   **Escalation rate:** Percentage of conversations requiring human intervention

For detailed guidance on configuring evaluation criteria in ElevenLabs, see [Success Evaluation](https://elevenlabs.io/docs/agents-platform/customization/agent-analysis/success-evaluation)
.

### Analyze failure patterns

When agents underperform, identify patterns in problematic interactions:

*   **Where does the agent provide incorrect information?** → Strengthen instructions in specific sections
*   **When does it fail to understand user intent?** → Add examples or simplify language
*   **Which user inputs cause it to break character?** → Add guardrails for edge cases
*   **Which tools fail most often?** → Improve error handling or parameter descriptions

Review conversation transcripts where user satisfaction was low or tasks weren’t completed.

### Make targeted refinements

Update specific sections of your prompt to address identified issues:

1.  **Isolate the problem:** Identify which prompt section or tool definition is causing failures
2.  **Test changes on specific examples:** Use conversations that previously failed as test cases
3.  **Make one change at a time:** Isolate improvements to understand what works
4.  **Re-evaluate with same test cases:** Verify the change fixed the issue without creating new problems

Avoid making multiple prompt changes simultaneously. This makes it impossible to attribute improvements or regressions to specific edits.

### Configure data collection

Configure your agent to summarize data from each conversation. This allows you to analyze interaction patterns, identify common user requests, and continuously improve your prompt based on real-world usage.

For detailed guidance on configuring data collection in ElevenLabs, see [Data Collection](https://elevenlabs.io/docs/agents-platform/customization/agent-analysis/data-collection)
.

### Use simulation for regression testing

Before deploying prompt changes to production, test against a set of known scenarios to catch regressions.

For guidance on testing agents programmatically, see [Simulate Conversations](https://elevenlabs.io/docs/agents-platform/guides/simulate-conversations)
.

Production considerations
-------------------------

Enterprise agents require additional safeguards beyond prompt quality. Production deployments must account for error handling, compliance, and graceful degradation.

### Handle errors across all tool integrations

Every external tool call is a potential failure point. Ensure your prompt includes explicit error handling for:

*   **Network failures:** “I’m having trouble connecting to our system. Let me try again.”
*   **Missing data:** “I don’t see that information in our system. Can you verify the details?”
*   **Timeout errors:** “This is taking longer than expected. I can escalate to a specialist or try again.”
*   **Permission errors:** “I don’t have access to that information. Let me transfer you to someone who can help.”

Example prompts
---------------

The following examples demonstrate how to apply the principles outlined in this guide to real-world enterprise use cases. Each example includes annotations highlighting which reliability principles are in use.

### Example 1: Technical support agent

Technical support specialist

|     |     |
| --- | --- |
| 1   | \# Personality |
| 2   |     |
| 3   | You are a technical support specialist for CloudTech, a B2B SaaS platform. |
| 4   | You are patient, methodical, and focused on resolving issues efficiently. |
| 5   | You speak clearly and adapt technical language based on the user's familiarity. |
| 6   |     |
| 7   | \# Environment |
| 8   |     |
| 9   | You are assisting customers via phone support. |
| 10  | Customers may be experiencing service disruptions and could be frustrated. |
| 11  | You have access to diagnostic tools and the customer account database. |
| 12  |     |
| 13  | \# Tone |
| 14  |     |
| 15  | Keep responses clear and concise (2-3 sentences unless troubleshooting requires more detail). |
| 16  | Use a calm, professional tone with brief affirmations ("I understand," "Let me check that"). |
| 17  | Adapt technical depth based on customer responses. |
| 18  | Check for understanding after complex steps: "Does that make sense?" |
| 19  |     |
| 20  | \# Goal |
| 21  |     |
| 22  | Resolve technical issues through structured troubleshooting: |
| 23  |     |
| 24  | 1\. Verify customer identity using email and account ID |
| 25  | 2\. Identify affected service and severity level |
| 26  | 3\. Run diagnostics using \`runSystemDiagnostic\` tool |
| 27  | 4\. Provide step-by-step resolution or escalate if unresolved after 2 attempts |
| 28  |     |
| 29  | This step is important: Always run diagnostics before suggesting solutions. |
| 30  |     |
| 31  | \# Guardrails |
| 32  |     |
| 33  | Never access customer accounts without identity verification. This step is important. |
| 34  | Never guess at solutions—always base recommendations on diagnostic results. |
| 35  | If an issue persists after 2 troubleshooting attempts, escalate to engineering team. |
| 36  | Acknowledge when you don't know the answer instead of speculating. |
| 37  |     |
| 38  | \# Tools |
| 39  |     |
| 40  | \## \`verifyCustomerIdentity\` |
| 41  |     |
| 42  | \*\*When to use:\*\* At the start of every conversation before accessing account data |
| 43  | \*\*Parameters:\*\* |
| 44  |     |
| 45  | \- \`email\` (required): Customer email in written format (e.g., "user@company.com") |
| 46  | \- \`account\_id\` (optional): Account ID if customer provides it |
| 47  |     |
| 48  | \*\*Usage:\*\* |
| 49  |     |
| 50  | 1\. Ask customer for email in spoken format: "Can I get the email associated with your account?" |
| 51  | 2\. Convert to written format: "john dot smith at company dot com" → "john.smith@company.com" |
| 52  | 3\. Call this tool with written email |
| 53  |     |
| 54  | \*\*Error handling:\*\* |
| 55  | If verification fails, ask customer to confirm email spelling and try again. |
| 56  |     |
| 57  | \## \`runSystemDiagnostic\` |
| 58  |     |
| 59  | \*\*When to use:\*\* After verifying identity and understanding the reported issue |
| 60  | \*\*Parameters:\*\* |
| 61  |     |
| 62  | \- \`account\_id\` (required): From \`verifyCustomerIdentity\` response |
| 63  | \- \`service\_name\` (required): Name of affected service (e.g., "api", "dashboard", "storage") |
| 64  |     |
| 65  | \*\*Usage:\*\* |
| 66  |     |
| 67  | 1\. Confirm which service is affected |
| 68  | 2\. Run diagnostic with account ID and service name |
| 69  | 3\. Review results before providing solution |
| 70  |     |
| 71  | \*\*Error handling:\*\* |
| 72  | If diagnostic fails, acknowledge the issue: "I'm having trouble running that diagnostic. Let me escalate to our engineering team." |
| 73  |     |
| 74  | \# Character normalization |
| 75  |     |
| 76  | When collecting email addresses: |
| 77  |     |
| 78  | \- Spoken: "john dot smith at company dot com" |
| 79  | \- Written: "john.smith@company.com" |
| 80  | \- Convert "@" from "at", "." from "dot", remove spaces |
| 81  |     |
| 82  | \# Error handling |
| 83  |     |
| 84  | If any tool call fails: |
| 85  |     |
| 86  | 1\. Acknowledge: "I'm having trouble accessing that information right now." |
| 87  | 2\. Do not guess or make up information |
| 88  | 3\. Offer to retry once, then escalate if failure persists |

**Principles demonstrated:**

*   ✓ Clean section separation (`# Personality`, `# Goal`, `# Tools`, etc.)
*   ✓ One action per line (see `# Goal` numbered steps)
*   ✓ Concise instructions (tone section is brief and clear)
*   ✓ Emphasized critical steps (“This step is important”)
*   ✓ Character normalization (email format conversion)
*   ✓ Clear examples (in character normalization section)
*   ✓ Dedicated guardrails section
*   ✓ Precise tool descriptions with when/how/error guidance
*   ✓ Explicit error handling instructions

### Example 2: Customer service refund agent

Refund processing specialist

|     |     |
| --- | --- |
| 1   | \# Personality |
| 2   |     |
| 3   | You are a refund specialist for RetailCo. |
| 4   | You are empathetic, solution-oriented, and efficient. |
| 5   | You balance customer satisfaction with company policy compliance. |
| 6   |     |
| 7   | \# Goal |
| 8   |     |
| 9   | Process refund requests through this workflow: |
| 10  |     |
| 11  | 1\. Verify customer identity using order number and email |
| 12  | 2\. Look up order details with \`getOrderDetails\` tool |
| 13  | 3\. Confirm refund eligibility (within 30 days, not digital download, not already refunded) |
| 14  | 4\. For refunds under $100: Process immediately with \`processRefund\` tool |
| 15  | 5\. For refunds $100-$500: Apply secondary verification, then process |
| 16  | 6\. For refunds over $500: Escalate to supervisor with case summary |
| 17  |     |
| 18  | This step is important: Never process refunds without verifying eligibility first. |
| 19  |     |
| 20  | \# Guardrails |
| 21  |     |
| 22  | Never process refunds outside the 30-day return window without supervisor approval. |
| 23  | Never process refunds over $500 without supervisor approval. This step is important. |
| 24  | Never access order information without verifying customer identity. |
| 25  | If a customer becomes aggressive, remain calm and offer supervisor escalation. |
| 26  |     |
| 27  | \# Tools |
| 28  |     |
| 29  | \## \`verifyIdentity\` |
| 30  |     |
| 31  | \*\*When to use:\*\* At the start of every conversation |
| 32  | \*\*Parameters:\*\* |
| 33  |     |
| 34  | \- \`order\_id\` (required): Order ID in written format (e.g., "ORD123456") |
| 35  | \- \`email\` (required): Customer email in written format |
| 36  |     |
| 37  | \*\*Usage:\*\* |
| 38  |     |
| 39  | 1\. Collect order ID: "Can I get your order number?" |
| 40  | - Spoken: "O R D one two three four five six" |
| 41  | - Written: "ORD123456" |
| 42  | 2\. Collect email and convert to written format |
| 43  | 3\. Call this tool with both values |
| 44  |     |
| 45  | \## \`getOrderDetails\` |
| 46  |     |
| 47  | \*\*When to use:\*\* After identity verification |
| 48  | \*\*Returns:\*\* Order date, items, total amount, refund eligibility status |
| 49  |     |
| 50  | \*\*Error handling:\*\* |
| 51  | If order not found, ask customer to verify order number and try again. |
| 52  |     |
| 53  | \## \`processRefund\` |
| 54  |     |
| 55  | \*\*When to use:\*\* Only after confirming eligibility |
| 56  | \*\*Required checks before calling:\*\* |
| 57  |     |
| 58  | \- Identity verified |
| 59  | \- Order is within 30 days |
| 60  | \- Order is eligible (not digital, not already refunded) |
| 61  | \- Refund amount is under $500 |
| 62  |     |
| 63  | \*\*Parameters:\*\* |
| 64  |     |
| 65  | \- \`order\_id\` (required): From previous verification |
| 66  | \- \`reason\_code\` (required): One of "defective", "wrong\_item", "late\_delivery", "changed\_mind" |
| 67  |     |
| 68  | \*\*Usage:\*\* |
| 69  |     |
| 70  | 1\. Confirm refund details with customer: "I'll process a $\[amount\] refund to your original payment method. It will appear in 3-5 business days. Does that work for you?" |
| 71  | 2\. Wait for customer confirmation |
| 72  | 3\. Call this tool |
| 73  |     |
| 74  | \*\*Error handling:\*\* |
| 75  | If refund processing fails, apologize and escalate: "I'm unable to process that refund right now. Let me escalate to a supervisor who can help." |
| 76  |     |
| 77  | \# Character normalization |
| 78  |     |
| 79  | Order IDs: |
| 80  |     |
| 81  | \- Spoken: "O R D one two three four five six" |
| 82  | \- Written: "ORD123456" |
| 83  | \- No spaces, all uppercase |
| 84  |     |
| 85  | Email addresses: |
| 86  |     |
| 87  | \- Spoken: "john dot smith at retailco dot com" |
| 88  | \- Written: "john.smith@retailco.com" |

**Principles demonstrated:**

*   ✓ Specialized agent scope (refunds only, not general support)
*   ✓ Clear workflow steps in `# Goal` section
*   ✓ Repeated emphasis on critical rules (refund limits, verification)
*   ✓ Detailed tool usage with “when to use” and “required checks”
*   ✓ Character normalization for structured IDs
*   ✓ Explicit error handling per tool
*   ✓ Escalation criteria clearly defined

Formatting best practices
-------------------------

How you format your prompt impacts how effectively the language model interprets it:

*   **Use markdown headings:** Structure sections with `#` for main sections, `##` for subsections
*   **Prefer bulleted lists:** Break down instructions into digestible bullet points
*   **Use whitespace:** Separate sections and instruction groups with blank lines
*   **Keep headings in sentence case:** `# Goal` not `# GOAL`
*   **Be consistent:** Use the same formatting pattern throughout the prompt

Frequently asked questions
--------------------------

###### How do I maintain consistency across multiple agents?

Create shared prompt templates for common sections like character normalization, error handling, and guardrails. Store these in a central repository and reference them across specialist agents. Use the orchestrator pattern to ensure consistent routing logic and handoff procedures.

###### What's the minimum viable prompt for production?

At minimum, include: (1) Personality/role definition, (2) Primary goal, (3) Core guardrails, and (4) Tool descriptions if tools are used. Even simple agents benefit from explicit section structure and error handling instructions.

###### How do I handle tool deprecation without breaking agents?

When deprecating a tool, add a new tool first, then update the prompt to prefer the new tool while keeping the old one as a fallback. Monitor usage, then remove the old tool once usage drops to zero. Always include error handling so agents can recover if a deprecated tool is called.

###### Should I use different prompts for different LLMs?

Generally, prompts structured with the principles in this guide work across models. However, model-specific tuning can improve performance—particularly for tool-calling format and reasoning steps. Test your prompt with multiple models and adjust if needed.

###### How long should my system prompt be?

No universal limit exists, but prompts over 2000 tokens increase latency and cost. Focus on conciseness: every line should serve a clear purpose. If your prompt exceeds 2000 tokens, consider splitting into multiple specialized agents or extracting reference material into a knowledge base.

###### How do I balance consistency with adaptability?

Define core personality traits, goals, and guardrails firmly while allowing flexibility in tone and verbosity based on user communication style. Use conditional instructions: “If the user is frustrated, acknowledge their concerns before proceeding.”

###### Can I update prompts after deployment?

Yes. System prompts can be modified at any time to adjust behavior. This is particularly useful for addressing emerging issues or refining capabilities as you learn from user interactions. Always test changes in a staging environment before deploying to production.

###### How do I prevent agents from hallucinating when tools fail?

Include explicit error handling instructions for every tool. Emphasize “never guess or make up information” in the guardrails section. Repeat this instruction in tool-specific error handling sections. Test tool failure scenarios during development to ensure agents follow recovery instructions.

Next steps
----------

This guide establishes the foundation for reliable agent behavior through prompt engineering, tool configuration, and architectural patterns. To build production-grade systems, continue with:

*   **[Workflows](https://elevenlabs.io/docs/agents-platform/customization/agent-workflows)
    :** Design multi-agent orchestration and specialist handoffs
*   **[Success Evaluation](https://elevenlabs.io/docs/agents-platform/customization/agent-analysis/success-evaluation)
    :** Configure metrics and evaluation criteria
*   **[Data Collection](https://elevenlabs.io/docs/agents-platform/customization/agent-analysis/data-collection)
    :** Capture structured insights from conversations
*   **[Testing](https://elevenlabs.io/docs/agents-platform/customization/agent-testing)
    :** Implement regression testing and simulation
*   **[Security & Privacy](https://elevenlabs.io/docs/agents-platform/customization/privacy)
    :** Ensure compliance and data protection
*   **[Our Docs Agent](https://elevenlabs.io/docs/agents-platform/guides/elevenlabs-docs-agent)
    :** See a complete case study of these principles in action

For enterprise deployment support, [contact our team](https://elevenlabs.io/contact-sales)
.

## Document 3 — Build | ElevenLabs Documentation {#doc-3}
[https://elevenlabs.io/docs/agents-platform/build/overview](https://elevenlabs.io/docs/agents-platform/build/overview)

The Build section covers everything you need to create sophisticated conversational agents, from defining their behavior and voice to connecting external tools and knowledge sources.

![Build your agent](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Fb01da89ad7994300673d0932d321cd0f53fe727b6210e6c1f00e765e498f8722%2Fassets%2Fimages%2Fagents%2Fagents-overview-build.png&w=3840&q=75)

### Design and configure

| Goal | Guide | Description |
| --- | --- | --- |
| Create conversation workflows | [Workflows](https://elevenlabs.io/docs/agents-platform/customization/agent-workflows) | Build multi-step workflows with visual workflow builder |
| Write system prompts | [System prompt](https://elevenlabs.io/docs/agents-platform/best-practices/prompting-guide) | Learn best practices for crafting effective agent prompts |
| Select language model | [Models](https://elevenlabs.io/docs/agents-platform/customization/llm) | Choose from supported LLMs or bring your own custom model |
| Control conversation flow | [Conversation flow](https://elevenlabs.io/docs/agents-platform/customization/conversation-flow) | Configure turn-taking, interruptions, and timeout settings |
| Configure voice & language | [Voice & language](https://elevenlabs.io/docs/agents-platform/customization/voice) | Select from 5k+ voices across 31 languages with customization options |
| Add knowledge to agent | [Knowledge base](https://elevenlabs.io/docs/agents-platform/customization/knowledge-base) | Upload documents and enable RAG for grounded responses |
| Connect tools | [Tools](https://elevenlabs.io/docs/agents-platform/customization/tools) | Enable agents to call clients & APIs to perform actions |
| Personalize each conversation | [Personalization](https://elevenlabs.io/docs/agents-platform/customization/personalization) | Use dynamic variables and overrides for per-conversation customization |
| Secure agent access | [Authentication](https://elevenlabs.io/docs/agents-platform/customization/authentication) | Implement custom authentication for protected agent access |

Core components
---------------

### Agent behavior

Control how your agent thinks and responds:

*   **Workflows**: Visual workflow builder for complex conversation flows
*   **System Prompt**: Define agent personality, tone, and capabilities
*   **Models**: Choose from leading LLMs or bring your own
*   **Conversation Flow**: Configure turn-taking, interruptions, and pacing

### Voice & language

Customize the agent’s voice and language capabilities:

*   **Voice Selection**: Choose from 5k+ professional voices
*   **Multi-voice Support**: Use different voices within conversations
*   **Language Support**: Deploy in 31+ languages
*   **Voice Design**: Create custom voices from text descriptions

### Knowledge & tools

Extend agent capabilities with external data and actions:

*   **Knowledge Base**: Upload documents to ground agent responses
*   **Tools**: Connect to APIs and external services
*   **MCP Integration**: Use Model Context Protocol tools
*   **System Tools**: Built-in capabilities like call transfer and voicemail detection

### Personalization

Tailor each conversation to your users:

*   **Dynamic Variables**: Inject runtime data into conversations
*   **Overrides**: Customize agent behavior per interaction
*   **Authentication**: Secure agent access with custom auth flows

Next steps
----------

[Workflows\
\
Build visual conversation flows](https://elevenlabs.io/docs/agents-platform/customization/agent-workflows)
[System Prompt\
\
Learn prompting best practices](https://elevenlabs.io/docs/agents-platform/best-practices/prompting-guide)
[Voice & Language\
\
Configure voice settings](https://elevenlabs.io/docs/agents-platform/customization/voice)
[Knowledge Base\
\
Add domain knowledge](https://elevenlabs.io/docs/agents-platform/customization/knowledge-base)

## Document 4 — Workflows | ElevenLabs Documentation {#doc-4}
[https://elevenlabs.io/docs/agents-platform/customization/agent-workflows](https://elevenlabs.io/docs/agents-platform/customization/agent-workflows)

Overview
--------

Agent Workflows provide a powerful visual interface for designing complex conversation flows in Agents Platform. Instead of relying on linear conversation paths, workflows enable you to create sophisticated, branching conversation graphs that adapt dynamically to user needs.

![Workflow Overview](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F0b5b2cf9754c67ef469c08af5d13786f70ca8e0018d10e92595861abb4ed32cb%2Fassets%2Fimages%2Fconversational-ai%2Fworkflow-overview.png&w=3840&q=75)

Node types
----------

Workflows are composed of different node types, each serving a specific purpose in your conversation flow.

![Node Types](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Fd638a84e1a6dc584a812be436f5da5e665b103b6cb5b6c53840705723bbb5a8f%2Fassets%2Fimages%2Fconversational-ai%2Fworkflow-node-types.png&w=3840&q=75)

### Subagent nodes

Subagent nodes allow you to modify agent behavior at specific points in your workflow. These modifications are applied on top of the base agent configuration, or can override the current agent’s config completely, giving you fine-grained control over each conversation phase. Any of an agent’s configuration, tools available, and attached knowledge base items can be updated/overwitten.

###### General

###### Knowledge Base

###### Tools

![Subagent Extra Agent Config](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F8ca72df8768a03adc0064281c906ab0f5710153249d17f7e7d51f465da7e9e94%2Fassets%2Fimages%2Fconversational-ai%2Fworkflow-subagent-extra-agent-config.png&w=3840&q=75)

Modify core agent settings for this specific node:

*   **System Prompt**: Append or override system instructions to guide agent behavior
*   **LLM Selection**: Choose a different language model (e.g., switch from Gemini 2.0 Flash to a more powerful model for complex reasoning tasks)
*   **Voice Configuration**: Change voice settings including speed, tone, or even switch to a different voice

**Use Cases:**

*   Use a more powerful LLM for complex decision-making nodes
*   Apply stricter conversation guidelines during sensitive information gathering
*   Change voice characteristics for different conversation phases
*   Modify agent personality for specific interaction types

### Dispatch tool node

Tool nodes execute a specific tool call during conversation flow. Unlike tools within subagents, tool nodes are dedicated execution points that guarantee the tool is called.

![Tool Node Result Edges](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F6b60603e56dfb25e89cdfe4223826f635af874e034af8936d74d3b428611e17b%2Fassets%2Fimages%2Fconversational-ai%2Fworkflow-tool-node-result-edges.png&w=3840&q=75)

**Special Edge Configuration:** Tool nodes have a unique edge type that allows routing to a new node based on the tool execution result. You can define:

*   **Success path**: Where to route when the tool executes successfully
*   **Failure path**: Where to route when the tool fails or returns an error

In future, futher branching conditions will be provided.

### Agent transfer node

Agent transfer node facilitate handoffs the conversation between different conversational agents, learn more [here](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/agent-transfer)
.

### Transfer to number node

Transfer to number nodes transitions from a conversation with an AI agent to a human agent via phone systems, learn more [here](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/transfer-to-human)

### End node

End call nodes terminate the conversation flow gracefully, learn more [here](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/transfer-to-human#:~:text=System%20tools-,End%20call,-Language%20detection)

Edges and flow control
----------------------

Edges define how conversations flow between nodes in your workflow. They support sophisticated routing logic that enables dynamic, context-aware conversation paths.

![Workflow Edges](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F02d664c9211bb8cf5452b80ab865f26b8d0b723a6acff75141ea1e9c43f7dbab%2Fassets%2Fimages%2Fconversational-ai%2Fworkflow-edges.png&w=3840&q=75)

###### Forward Edges

###### Backward Edges

Forward edges move the conversation to subsequent nodes in the workflow. They represent the primary flow of your conversation.

![Forward Edge Configuration](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F2400c66bf6f60b0d4262ecd828dea93847197f7cc00d9c8964e6486419bc90be%2Fassets%2Fimages%2Fconversational-ai%2Fworkflow-edge-forward.png&w=3840&q=75)

###### LLM Condition

###### Expression

###### None

Use LLM conditions to create dynamic conversation flows based on natural language evaluation. The LLM evaluates conditions in real-time to determine the appropriate path.

![LLM Condition Agent Transfer](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F507885879d781f291ab35b7dda84e760767a5544ffb6bf7b455e7a1cc19b78b7%2Fassets%2Fimages%2Fconversational-ai%2Fworkflow-agent-transfer-llm-condition.png&w=3840&q=75)

**Configuration Options:**

*   **Label**: Human-readable description of the edge condition (not processed by LLM)
*   **LLM Condition**: Natural language condition evaluated by the LLM

![Workflow Overview](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F0b5b2cf9754c67ef469c08af5d13786f70ca8e0018d10e92595861abb4ed32cb%2Fassets%2Fimages%2Fconversational-ai%2Fworkflow-overview.png&w=3840&q=75)

## Document 5 — Conversation flow | ElevenLabs Documentation {#doc-5}
[https://elevenlabs.io/docs/agents-platform/customization/conversation-flow](https://elevenlabs.io/docs/agents-platform/customization/conversation-flow)

Overview
--------

Conversation flow settings determine how your assistant handles periods of user silence, interruptions during speech, and turn-taking behavior. These settings help create more natural conversations and can be customized based on your use case.

[Timeouts\
\
Configure how long your assistant waits during periods of silence](https://elevenlabs.io/docs/agents-platform/customization/conversation-flow#timeouts)
[Interruptions\
\
Control whether users can interrupt your assistant while speaking](https://elevenlabs.io/docs/agents-platform/customization/conversation-flow#interruptions)
[Turn eagerness\
\
Adjust how quickly your assistant responds to user input](https://elevenlabs.io/docs/agents-platform/customization/conversation-flow#turn-eagerness)

Timeouts
--------

Timeout handling determines how long your assistant will wait during periods of user silence before prompting for a response.

### Configuration

Timeout settings can be configured in the agent’s **Advanced** tab under **Turn Timeout**.

The timeout duration is specified in seconds and determines how long the assistant will wait in silence before prompting the user. Turn timeouts must be between 1 and 30 seconds.

#### Example timeout settings

![Timeout settings](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Fb0b1941575dd4ff3c7b68f1370bd5419c0a0a1d26aa106b35264afb0a3df1329%2Fassets%2Fimages%2Fconversational-ai%2Ftimeouts.png&w=3840&q=75)

Choose an appropriate timeout duration based on your use case. Shorter timeouts create more responsive conversations but may interrupt users who need more time to respond, leading to a less natural conversation.

### Best practices for timeouts

*   Set shorter timeouts (5-10 seconds) for casual conversations where quick back-and-forth is expected
*   Use longer timeouts (10-30 seconds) when users may need more time to think or formulate complex responses
*   Consider your user context - customer service may benefit from shorter timeouts while technical support may need longer ones

Interruptions
-------------

Interruption handling determines whether users can interrupt your assistant while it’s speaking.

### Configuration

Interruption settings can be configured in the agent’s **Advanced** tab under **Client Events**.

To enable interruptions, make sure interruption is a selected client event.

#### Interruptions enabled

![Interruption allowed](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F1da794be8ea3bfed45d06241ce5db390480cd45d27c0f886943518bd52d76157%2Fassets%2Fimages%2Fconversational-ai%2Finterruptions.png&w=3840&q=75)

#### Interruptions disabled

![Interruption ignored](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F847a2ebcdfff9498501502ab4b568fc498b6995f860a5552177a7883942197ff%2Fassets%2Fimages%2Fconversational-ai%2Fno-interruption.png&w=3840&q=75)

Disable interruptions when the complete delivery of information is crucial, such as legal disclaimers or safety instructions.

### Best practices for interruptions

*   Enable interruptions for natural conversational flows where back-and-forth dialogue is expected
*   Disable interruptions when message completion is critical (e.g., terms and conditions, safety information)
*   Consider your use case context - customer service may benefit from interruptions while information delivery may not

Turn eagerness
--------------

Turn eagerness controls how quickly your assistant responds to user input during conversation. This setting determines how eager the assistant is to take turns and start speaking based on detected speech patterns.

### How it works

The assistant now includes two key improvements for more natural turn-taking:

1.  **Faster response generation** - The assistant starts speaking after receiving enough words and a comma from the language model, rather than waiting for complete sentences. This reduces latency and creates more responsive conversations, especially when the assistant has longer responses.
    
2.  **Configurable turn eagerness** - Control how quickly the assistant interprets pauses or speech patterns as opportunities to respond.
    

### Configuration

Turn eagerness can be configured in the dashboard Agent settings or via the [API](https://elevenlabs.io/docs/api-reference/agents/create#request.body.conversation_config.turn.turn_eagerness)
. Three modes are available:

*   **Eager** - The assistant responds quickly to user input, jumping in at the earliest opportunity. Best for fast-paced conversations where immediate responses are valued.
*   **Normal** - Balanced turn-taking that works well for most conversational scenarios. The assistant waits for natural conversation breaks before responding.
*   **Patient** - The assistant waits longer before taking its turn, giving users more time to complete their thoughts. Ideal for collecting detailed information or when users need time to formulate responses.

Turn eagerness is especially powerful when combined with workflows. You can dynamically adjust the assistant’s responsiveness based on context—making it jump in faster during casual conversation, or wait longer when collecting sensitive information like phone numbers or email addresses.

### Best practices for turn eagerness

*   Use **Eager** mode for customer service scenarios where quick responses improve user experience
*   Use **Patient** mode when collecting structured information like phone numbers, addresses, or email addresses
*   Use **Normal** mode as a default for general conversational flows
*   Combine with workflows to dynamically adjust turn eagerness based on conversation context
*   Test different settings with your specific use case to find the optimal balance

Recommended configurations
--------------------------

###### Customer service

*   Shorter timeouts (5-10 seconds) for responsive interactions - Enable interruptions to allow customers to interject with questions - **Eager** turn eagerness for quick, responsive conversations

###### Information collection

*   Moderate timeouts (10-15 seconds) to allow users time to gather information - Enable interruptions for natural conversation flow - **Patient** turn eagerness when collecting phone numbers, addresses, or email addresses

###### Legal disclaimers

*   Longer timeouts (15-30 seconds) to allow for complex responses - Disable interruptions to ensure full delivery of legal information - **Normal** turn eagerness to maintain steady pacing

###### Conversational EdTech

*   Longer timeouts (10-30 seconds) to allow time to think and formulate responses - Enable interruptions to allow students to interject with questions - **Patient** turn eagerness to give students adequate time to respond

## Document 6 — Knowledge base | ElevenLabs Documentation {#doc-6}
[https://elevenlabs.io/docs/agents-platform/customization/knowledge-base](https://elevenlabs.io/docs/agents-platform/customization/knowledge-base)

**Knowledge bases** allow you to equip your agent with relevant, domain-specific information.

Overview
--------

A well-curated knowledge base helps your agent go beyond its pre-trained data and deliver context-aware answers.

Here are a few examples where knowledge bases can be useful:

*   **Product catalogs**: Store product specifications, pricing, and other essential details.
*   **HR or corporate policies**: Provide quick answers about vacation policies, employee benefits, or onboarding procedures.
*   **Technical documentation**: Equip your agent with in-depth guides or API references to assist developers.
*   **Customer FAQs**: Answer common inquiries consistently.

The agent on this page is configured with full knowledge of ElevenLabs’ documentation and sitemap. Go ahead and ask it about anything about ElevenLabs.

Usage
-----

###### Build a knowledge base via the API

###### Build a knowledge base via the web dashboard

PythonTypeScript

|     |     |
| --- | --- |
| 1   | \# First create the document from text |
| 2   | knowledge\_base\_document\_text = elevenlabs.conversational\_ai.knowledge\_base.documents.create\_from\_text( |
| 3   | text="The airspeed velocity of an unladen swallow (European) is 24 miles per hour or roughly 11 meters per second.", |
| 4   | name="Unladen Swallow facts", |
| 5   | )   |
| 6   |     |
| 7   | \# Alternatively, you can create a document from a URL |
| 8   | knowledge\_base\_document\_url = elevenlabs.conversational\_ai.knowledge\_base.documents.create\_from\_url( |
| 9   | url="https://en.wikipedia.org/wiki/Unladen\_swallow", |
| 10  | name="Unladen Swallow Wikipedia page", |
| 11  | )   |
| 12  |     |
| 13  | \# Or create a document from a file |
| 14  | knowledge\_base\_document\_file = elevenlabs.conversational\_ai.knowledge\_base.documents.create\_from\_file( |
| 15  | file=open("/path/to/unladen-swallow-facts.txt", "rb"), |
| 16  | name="Unladen Swallow Facts", |
| 17  | )   |
| 18  |     |
| 19  | \# Then add the document to the agent |
| 20  | agent = elevenlabs.conversational\_ai.agents.update( |
| 21  | agent\_id="agent-id", |
| 22  | conversation\_config={ |
| 23  | "agent": { |
| 24  | "prompt": { |
| 25  | "knowledge\_base": \[ |\
| 26  | {   |\
| 27  | "type": "text", |\
| 28  | "name": knowledge\_base\_document\_text.name, |\
| 29  | "id": knowledge\_base\_document\_text.id, |\
| 30  | },  |\
| 31  | {   |\
| 32  | "type": "url", |\
| 33  | "name": knowledge\_base\_document\_url.name, |\
| 34  | "id": knowledge\_base\_document\_url.id, |\
| 35  | },  |\
| 36  | {   |\
| 37  | "type": "file", |\
| 38  | "name": knowledge\_base\_document\_file.name, |\
| 39  | "id": knowledge\_base\_document\_file.id, |\
| 40  | }   |\
| 41  | \]  |
| 42  | }   |
| 43  | }   |
| 44  | },  |
| 45  | )   |
| 46  |     |
| 47  | print("Agent updated:", agent) |

Best practices
--------------

#### Content quality

Provide clear, well-structured information that’s relevant to your agent’s purpose.

#### Size management

Break large documents into smaller, focused pieces for better processing.

#### Regular updates

Regularly review and update the agent’s knowledge base to ensure the information remains current and accurate.

#### Identify knowledge gaps

Review conversation transcripts to identify popular topics, queries and areas where users struggle to find information. Note any knowledge gaps and add the missing context to the knowledge base.

Enterprise features
-------------------

Non-enterprise accounts have a maximum of 20MB or 300k characters.

Need higher limits? [Contact our sales team](https://elevenlabs.io/contact-sales)
 to discuss enterprise plans with expanded knowledge base capabilities.

## Document 7 — Knowledge base dashboard | ElevenLabs Documentation {#doc-7}
[https://elevenlabs.io/docs/agents-platform/customization/knowledge-base/dashboard](https://elevenlabs.io/docs/agents-platform/customization/knowledge-base/dashboard)

Overview
--------

The [knowledge base dashboard](https://elevenlabs.io/app/agents/knowledge-base)
 provides a centralized way to manage documents and track their usage across your AI agents. This guide explains how to navigate and use the knowledge base dashboard effectively.

![Knowledge base main interface showing list of\
documents](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Fd0c15168af40c776865e2ef48c39234287e30eeeb2752c74083704749074924f%2Fassets%2Fimages%2Fconversational-ai%2Fkb-content.png&w=3840&q=75)

Adding existing documents to agents
-----------------------------------

When configuring an agent’s knowledge base, you can easily add existing documents to an agent.

1.  Navigate to the agent’s [configuration](https://elevenlabs.io/app/agents/)
    
2.  Click “Add document” in the knowledge base section of the “Agent” tab.
3.  The option to select from your existing knowledge base documents or upload a new document will appear.

![Interface for adding documents to an\
agent](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F8eda3a9e686e87705e0d1f1e82a9d01b4960a2c00f597253995b589421650995%2Fassets%2Fimages%2Fconversational-ai%2Fkb-add-doc-items.png&w=3840&q=75)

Documents can be reused across multiple agents, making it efficient to maintain consistent knowledge across your workspace.

Document dependencies
---------------------

Each document in your knowledge base includes a “Agents” tab that shows which agents currently depend on that document.

![Dependent agents tab showing which agents use a\
document](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F6bed2a22b4ea13dc6e1048b94398c5dd438fc5fed87ca27e575c4351c83181de%2Fassets%2Fimages%2Fconversational-ai%2Fkb-dependent-agents.png&w=3840&q=75)

It is not possible to delete a document if any agent depends on it.

## Document 8 — Retrieval-Augmented Generation | ElevenLabs Documentation {#doc-8}
[https://elevenlabs.io/docs/agents-platform/customization/knowledge-base/rag](https://elevenlabs.io/docs/agents-platform/customization/knowledge-base/rag)

Overview
--------

**Retrieval-Augmented Generation (RAG)** enables your agent to access and use large knowledge bases during conversations. Instead of loading entire documents into the context window, RAG retrieves only the most relevant information for each user query, allowing your agent to:

*   Access much larger knowledge bases than would fit in a prompt
*   Provide more accurate, knowledge-grounded responses
*   Reduce hallucinations by referencing source material
*   Scale knowledge without creating multiple specialized agents

RAG is ideal for agents that need to reference large documents, technical manuals, or extensive knowledge bases that would exceed the context window limits of traditional prompting. RAG adds on slight latency to the response time of your agent, around 500ms.

How RAG works
-------------

When RAG is enabled, your agent processes user queries through these steps:

1.  **Query processing**: The user’s question is analyzed and reformulated for optimal retrieval.
2.  **Embedding generation**: The processed query is converted into a vector embedding that represents the user’s question.
3.  **Retrieval**: The system finds the most semantically similar content from your knowledge base.
4.  **Response generation**: The agent generates a response using both the conversation context and the retrieved information.

This process ensures that relevant information to the user’s query is passed to the LLM to generate a factually correct answer.

Guide
-----

### Prerequisites

*   An [ElevenLabs account](https://elevenlabs.io/)
    
*   A configured ElevenLabs [Conversational Agent](https://elevenlabs.io/docs/agents-platform/quickstart)
    
*   At least one document added to your agent’s knowledge base

[1](https://elevenlabs.io/docs/agents-platform/customization/knowledge-base/rag#enable-rag-for-your-agent)

### Enable RAG for your agent

In your agent’s settings, navigate to the **Knowledge Base** section and toggle on the **Use RAG** option.

![Toggle switch to enable RAG in the agent settings](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Fbdcdf9c15b8eb653248909e11149d0c383fb45f761f3bfbc47ba56feb25d8899%2Fassets%2Fimages%2Fconversational-ai%2Frag-enabled.png&w=3840&q=75)

[2](https://elevenlabs.io/docs/agents-platform/customization/knowledge-base/rag#configure-rag-settings-optional)

### Configure RAG settings (optional)

After enabling RAG, you’ll see additional configuration options in the **Advanced** tab:

*   **Embedding model**: Select the model that will convert text into vector embeddings
*   **Maximum document chunks**: Set the maximum amount of retrieved content per query
*   **Maximum vector distance**: Set the maximum distance between the query and the retrieved chunks

These parameters could impact latency. They also could impact LLM cost. For example, retrieving more chunks increases cost. Increasing vector distance allows for more context to be passed, but potentially less relevant context. This may affect quality and you should experiment with different parameters to find the best results.

![RAG configuration options including embedding model selection](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Fc9a0f81555c8fadd0324637c672f08950889e19b9cf56f971d60fcc520a06d0a%2Fassets%2Fimages%2Fconversational-ai%2Frag-config.png&w=3840&q=75)

[3](https://elevenlabs.io/docs/agents-platform/customization/knowledge-base/rag#knowledge-base-indexing)

### Knowledge base indexing

Each document in your knowledge base needs to be indexed before it can be used with RAG. This process happens automatically when a document is added to an agent with RAG enabled.

Indexing may take a few minutes for large documents. You can check the indexing status in the knowledge base list.

[4](https://elevenlabs.io/docs/agents-platform/customization/knowledge-base/rag#configure-document-usage-modes-optional)

### Configure document usage modes (optional)

For each document in your knowledge base, you can choose how it’s used:

*   **Auto (default)**: The document is only retrieved when relevant to the query
*   **Prompt**: The document is always included in the system prompt, regardless of relevance, but can also be retrieved by RAG

![Document usage mode options in the knowledge base](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Ffb47404f1ac2a698ed9dfedaa5bd75ea9d4b00c701a97e8db0caec8d18ef60ce%2Fassets%2Fimages%2Fconversational-ai%2Frag-prompt.png&w=3840&q=75)

Setting too many documents to “Prompt” mode may exceed context limits. Use this option sparingly for critical information.

[5](https://elevenlabs.io/docs/agents-platform/customization/knowledge-base/rag#test-your-rag-enabled-agent)

### Test your RAG-enabled agent

After saving your configuration, test your agent by asking questions related to your knowledge base. The agent should now be able to retrieve and reference specific information from your documents.

Usage limits
------------

To ensure fair resource allocation, ElevenLabs enforces limits on the total size of documents that can be indexed for RAG per workspace, based on subscription tier.

The limits are as follows:

| Subscription Tier | Total Document Size Limit | Notes |
| --- | --- | --- |
| Free | 1MB | Indexes may be deleted after inactivity. |
| Starter | 2MB |     |
| Creator | 20MB |     |
| Pro | 100MB |     |
| Scale | 500MB |     |
| Business | 1GB |     |
| Enterprise | 1GB | Higher limits available based on tier and agreement. |

**Note:**

*   These limits apply to the total **original file size** of documents indexed for RAG, not the internal storage size of the RAG index itself (which can be significantly larger).
*   Documents smaller than 500 bytes cannot be indexed for RAG and will automatically be used in the prompt instead.

API implementation
------------------

You can also implement RAG through the [API](https://elevenlabs.io/docs/api-reference/knowledge-base/compute-rag-index)
:

PythonJavaScript

|     |     |
| --- | --- |
| 1   | from elevenlabs import ElevenLabs |
| 2   | import time |
| 3   |     |
| 4   | \# Initialize the ElevenLabs client |
| 5   | elevenlabs \= ElevenLabs(api\_key\="your-api-key") |
| 6   |     |
| 7   | \# First, index a document for RAG |
| 8   | document\_id \= "your-document-id" |
| 9   | embedding\_model \= "e5\_mistral\_7b\_instruct" |
| 10  |     |
| 11  | \# Trigger RAG indexing |
| 12  | response \= elevenlabs.conversational\_ai.knowledge\_base.document.compute\_rag\_index( |
| 13  | documentation\_id\=document\_id, |
| 14  | model\=embedding\_model |
| 15  | )   |
| 16  |     |
| 17  | \# Check indexing status |
| 18  | while response.status not in \["SUCCEEDED", "FAILED"\]: |
| 19  | time.sleep(5)  # Wait 5 seconds before checking status again |
| 20  | response \= elevenlabs.conversational\_ai.knowledge\_base.document.compute\_rag\_index( |
| 21  | documentation\_id\=document\_id, |
| 22  | model\=embedding\_model |
| 23  | )   |
| 24  |     |
| 25  | \# Then update agent configuration to use RAG |
| 26  | agent\_id \= "your-agent-id" |
| 27  |     |
| 28  | \# Get the current agent configuration |
| 29  | agent\_config \= elevenlabs.conversational\_ai.agents.get(agent\_id\=agent\_id) |
| 30  |     |
| 31  | \# Enable RAG in the agent configuration |
| 32  | agent\_config.agent.prompt.rag = { |
| 33  | "enabled": True, |
| 34  | "embedding\_model": "e5\_mistral\_7b\_instruct", |
| 35  | "max\_documents\_length": 10000 |
| 36  | }   |
| 37  |     |
| 38  | \# Update document usage mode if needed |
| 39  | for i, doc in enumerate(agent\_config.agent.prompt.knowledge\_base): |
| 40  | if doc.id == document\_id: |
| 41  | agent\_config.agent.prompt.knowledge\_base\[i\].usage\_mode = "auto" |
| 42  |     |
| 43  | \# Update the agent configuration |
| 44  | elevenlabs.conversational\_ai.agents.update( |
| 45  | agent\_id\=agent\_id, |
| 46  | conversation\_config\=agent\_config.agent |
| 47  | )   |

Ask AI

Assistant

Hi, I'm an AI assistant with access to documentation and other content.

Tip: You can toggle this pane with

⌘

+

/

![Toggle switch to enable RAG in the agent settings](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Fbdcdf9c15b8eb653248909e11149d0c383fb45f761f3bfbc47ba56feb25d8899%2Fassets%2Fimages%2Fconversational-ai%2Frag-enabled.png&w=3840&q=75)

![RAG configuration options including embedding model selection](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Fc9a0f81555c8fadd0324637c672f08950889e19b9cf56f971d60fcc520a06d0a%2Fassets%2Fimages%2Fconversational-ai%2Frag-config.png&w=3840&q=75)

![Document usage mode options in the knowledge base](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Ffb47404f1ac2a698ed9dfedaa5bd75ea9d4b00c701a97e8db0caec8d18ef60ce%2Fassets%2Fimages%2Fconversational-ai%2Frag-prompt.png&w=3840&q=75)

## Document 9 — Models | ElevenLabs Documentation {#doc-9}
[https://elevenlabs.io/docs/agents-platform/customization/llm](https://elevenlabs.io/docs/agents-platform/customization/llm)

The ElevenLabs Agents Platform provides a unified interface to connect your agent to multiple models and providers, offering flexibility, reliability, and cost optimization.

Key features
------------

*   **Unified access**: Switch between providers and models with minimal code changes
*   **High reliability**: Automatically cascade from one provider to another if one fails
*   **Spend monitoring**: Monitor your spending across different models

Supported models
----------------

Currently, the following models are natively supported and can be configured via the agent settings:

| Provider | Model |
| --- | --- |
| **ElevenLabs** | GLM-4.5-Air |
|     | Qwen3-30B-A3B |
|     | GPT-OSS-120B |
| **Google** | Gemini 2.5 Flash |
|     | Gemini 2.5 Flash Lite |
|     | Gemini 2.0 Flash |
|     | Gemini 2.0 Flash Lite |
| **OpenAI** | GPT-5 |
|     | GPT-5 Mini |
|     | GPT-5 Nano |
|     | GPT-4.1 |
|     | GPT-4.1 Mini |
|     | GPT-4.1 Nano |
|     | GPT-4o |
|     | GPT-4o Mini |
|     | GPT-4 Turbo |
|     | GPT-3.5 Turbo |
| **Anthropic** | Claude Sonnet 4.5 |
|     | Claude Sonnet 4 |
|     | Claude Haiku 4.5 |
|     | Claude 3.7 Sonnet |
|     | Claude 3.5 Sonnet |
|     | Claude 3 Haiku |

Pricing is typically denoted in USD per 1 million tokens unless specified otherwise. A token is a fundamental unit of text data for LLMs, roughly equivalent to 4 characters on average.

### Custom LLM

Using your own custom LLM is supported by specifying the endpoint we should make requests to and providing credentials through our secure secret storage. Learn more about [custom LLM integration](https://elevenlabs.io/docs/agents-platform/customization/llm/custom-llm)
.

With EU data residency enabled, a small number of older Gemini and Claude LLMs are not available in ElevenLabs Agents to maintain compliance with EU data residency. Custom LLMs and OpenAI LLMs remain fully available. For more information please see [GDPR and data residency](https://elevenlabs.io/docs/overview/administration/data-residency)
.

Choosing a model
----------------

Selecting the most suitable LLM for your application involves considering several factors:

*   **Task complexity**: More demanding or nuanced tasks generally benefit from more powerful models (e.g., OpenAI’s GPT-4 series, Anthropic’s Claude Sonnet 4, Google’s Gemini 2.5 models)
*   **Latency requirements**: For applications requiring real-time or near real-time responses, such as live voice conversations, models optimized for speed are preferable (e.g., Google’s Gemini Flash series, Anthropic’s Claude Haiku, OpenAI’s GPT-4o-mini)
*   **Context window size**: If your application needs to process, understand, or recall information from long conversations or extensive documents, select models with larger context windows
*   **Cost-effectiveness**: Balance the desired performance and features against your budget. LLM prices can vary significantly, so analyze the pricing structure (input, output, and cache tokens) in relation to your expected usage patterns
*   **HIPAA compliance**: If your application involves Protected Health Information (PHI), it is crucial to use an LLM that is designated as HIPAA compliant and ensure your entire data handling process meets regulatory standards

The maximum system prompt size is 2MB, which includes your agent’s instructions, knowledge base content, and other system-level context.

Model configuration
-------------------

### Temperature

Temperature controls the randomness of model responses. Lower values produce more consistent, focused outputs while higher values increase creativity and variation.

*   **Low (0.0-0.3)**: Deterministic, consistent responses for structured interactions
*   **Medium (0.4-0.7)**: Balanced creativity and consistency
*   **High (0.8-1.0)**: Creative, varied responses for dynamic conversations

### Backup LLM configuration

Configure backup LLMs to ensure conversation continuity when the primary LLM fails or becomes unavailable.

**Configuration options:**

*   **Default**: Uses ElevenLabs’ recommended fallback sequence
*   **Custom**: Define your own cascading sequence of backup models
*   **Disabled**: No fallback (strongly discouraged for production)

Disabling backup LLMs means conversations will end abruptly if your primary LLM fails or becomes unavailable. This is strongly discouraged for production use.

Learn more about [LLM cascading](https://elevenlabs.io/docs/agents-platform/customization/llm/llm-cascading)
.

### Thinking budget

Control how many internal reasoning tokens the model can use before responding. More tokens improve answer quality but slow down response time.

**Options:**

*   **Disabled**: Fastest replies with no internal reasoning overhead
*   **Low**: Minimal reasoning for quick responses
*   **Medium**: Balanced reasoning and speed
*   **High**: Maximum reasoning for complex queries

### Reasoning effort

Some models support configurable reasoning effort levels (None, Low, Medium, High).

**For conversational use-cases:**

Keep reasoning effort set to **None** to avoid the agent thinking too long, which can disrupt natural conversation flow.

**For workflow steps:**

Reasoning effort is perfect for workflow steps that require complex thought or decision-making where response time is less critical.

Understanding pricing
---------------------

*   **Tokens**: LLM usage is typically billed based on the number of tokens processed. As a general guideline for English text, 100 tokens is approximately equivalent to 75 words
*   **Input vs. output pricing**: Providers often differentiate pricing for input tokens (the data you send to the model) and output tokens (the data the model generates in response)
*   **Cache pricing**:
    *   `input_cache_read`: This refers to the cost associated with retrieving previously processed input data from a cache. Utilizing cached data can lead to cost savings if identical inputs are processed multiple times
    *   `input_cache_write`: This is the cost associated with storing input data into a cache. Some LLM providers may charge for this operation
*   The prices listed in this document are per 1 million tokens and are based on the information available at the time of writing. These prices are subject to change by the LLM providers

For the most accurate and current information on model capabilities, pricing, and terms of service, always consult the official documentation from the respective LLM providers (OpenAI, Google, Anthropic).

HIPAA compliance
----------------

Certain LLMs available on our platform may be suitable for use in environments requiring HIPAA compliance, please see the [HIPAA compliance docs](https://elevenlabs.io/docs/agents-platform/legal/hipaa)
 for more details.

Related resources
-----------------

*   [Custom LLM integration](https://elevenlabs.io/docs/agents-platform/customization/llm/custom-llm)
    
*   [LLM cascading](https://elevenlabs.io/docs/agents-platform/customization/llm/llm-cascading)
    
*   [Optimizing costs](https://elevenlabs.io/docs/agents-platform/customization/llm/optimizing-costs)

## Document 10 — Personalization | ElevenLabs Documentation {#doc-10}
[https://elevenlabs.io/docs/agents-platform/customization/personalization](https://elevenlabs.io/docs/agents-platform/customization/personalization)

Overview
--------

Personalization allows you to adapt your agent’s behavior for each individual user, enabling more natural and contextually relevant conversations. ElevenLabs offers multiple approaches to personalization:

1.  **Dynamic Variables** - Inject runtime values into prompts and messages
2.  **Overrides** - Completely replace system prompts or messages
3.  **Twilio Integration** - Personalize inbound call experiences via webhooks

Personalization Methods
-----------------------

[Dynamic Variables\
\
Define runtime values using `{{ var_name }}` syntax to personalize your agent’s messages, system prompts, and tools.](https://elevenlabs.io/docs/agents-platform/customization/personalization/dynamic-variables)
[Overrides\
\
Completely replace system prompts, first messages, language, or voice settings for each conversation.](https://elevenlabs.io/docs/agents-platform/customization/personalization/overrides)
[Twilio Integration\
\
Dynamically personalize inbound Twilio calls using webhook data.](https://elevenlabs.io/docs/agents-platform/customization/personalization/twilio-personalization)

Conversation Initiation Client Data Structure
---------------------------------------------

The `conversation_initiation_client_data` object defines what can be customized when starting a conversation:

|     |     |
| --- | --- |
| 1   | {   |
| 2   | "type": "conversation\_initiation\_client\_data", |
| 3   | "conversation\_config\_override": { |
| 4   | "agent": { |
| 5   | "prompt": { |
| 6   | "prompt": "overriding system prompt" |
| 7   | },  |
| 8   | "first\_message": "overriding first message", |
| 9   | "language": "en" |
| 10  | },  |
| 11  | "tts": { |
| 12  | "voice\_id": "voice-id-here" |
| 13  | }   |
| 14  | },  |
| 15  | "custom\_llm\_extra\_body": { |
| 16  | "temperature": 0.7, |
| 17  | "max\_tokens": 100 |
| 18  | },  |
| 19  | "dynamic\_variables": { |
| 20  | "string\_var": "text value", |
| 21  | "number\_var": 1.2, |
| 22  | "integer\_var": 123, |
| 23  | "boolean\_var": true |
| 24  | },  |
| 25  | "user\_id": "your\_custom\_user\_id" |
| 26  | }   |

Choosing the Right Approach
---------------------------

| Method | Best For | Implementation |
| --- | --- | --- |
| **Dynamic Variables** | *   Inserting user-specific data into templated content - Maintaining consistent agent behavior with personalized details - Personalizing tool parameters | Define variables with `{{ variable_name }}` and pass values at runtime |
| **Overrides** | *   Completely changing agent behavior per user - Switching languages or voices - Legacy applications (consider migrating to Dynamic Variables) | Enable specific override permissions in security settings and pass complete replacement content |

Learn More
----------

*   [Dynamic Variables Documentation](https://elevenlabs.io/docs/agents-platform/customization/personalization/dynamic-variables)
    
*   [Overrides Documentation](https://elevenlabs.io/docs/agents-platform/customization/personalization/overrides)
    
*   [Twilio Integration Documentation](https://elevenlabs.io/docs/agents-platform/customization/personalization/twilio-personalization)

## Document 11 — Dynamic variables | ElevenLabs Documentation {#doc-11}
[https://elevenlabs.io/docs/agents-platform/customization/personalization/dynamic-variables](https://elevenlabs.io/docs/agents-platform/customization/personalization/dynamic-variables)

**Dynamic variables** allow you to inject runtime values into your agent’s messages, system prompts, and tools. This enables you to personalize each conversation with user-specific data without creating multiple agents.

Overview
--------

Dynamic variables can be integrated into multiple aspects of your agent:

*   **System prompts** to customize behavior and context
*   **First messages** to personalize greetings
*   **Tool parameters and headers** to pass user-specific data

Here are a few examples where dynamic variables are useful:

*   **Personalizing greetings** with user names
*   **Including account details** in responses
*   **Passing data** to tool calls
*   **Customizing behavior** based on subscription tiers
*   **Accessing system information** like conversation ID or call duration

Dynamic variables are ideal for injecting user-specific data that shouldn’t be hardcoded into your agent’s configuration.

System dynamic variables
------------------------

Your agent has access to these automatically available system variables:

*   `system__agent_id` - Unique identifier of the agent that initiated the conversation (stays stable throughout the conversation)
*   `system__current_agent_id` - Unique identifier of the currently active agent (changes after agent transfers)
*   `system__caller_id` - Caller’s phone number (voice calls only)
*   `system__called_number` - Destination phone number (voice calls only)
*   `system__call_duration_secs` - Call duration in seconds
*   `system__time_utc` - Current UTC time (ISO format)
*   `system__time` - Current time in the specified timezone (human-readable format, e.g., “Friday, 12:33 12 December 2025”)
*   `system__timezone` - User-provided timezone (must be valid for tzinfo)
*   `system__conversation_id` - ElevenLabs’ unique conversation identifier
*   `system__call_sid` - Call SID (twilio calls only)

System variables:

*   Are available without runtime configuration
*   Are prefixed with `system__` (reserved prefix)
*   In system prompts: Set once at conversation start (value remains static)
*   In tool calls: Updated at execution time (value reflects current state)

Custom dynamic variables cannot use the reserved `system__` prefix.

Secret dynamic variables
------------------------

Secret dynamic variables are populated in the same way as normal dynamic variables but indicate to our Agents platform that these should only be used in dynamic variable headers and never sent to an LLM provider as part of an agent’s system prompt or first message.

We recommend using these for auth tokens or private IDs that should not be sent to an LLM. To create a secret dynamic variable, simply prefix the dynamic variable with `secret__`.

Updating dynamic variables from tools
-------------------------------------

[Tool calls](https://elevenlabs.io/docs/agents-platform/customization/tools)
 can create or update dynamic variables if they return a valid JSON object. To specify what should be extracted, set the object path(s) using dot notation. If the field or path doesn’t exist, nothing is updated.

Example of a response object and dot notation:

*   Status corresponds to the path: `response.status`
*   The first user’s email in the users array corresponds to the path: `response.users.0.email`

JSON

|     |     |
| --- | --- |
| 1   | {   |
| 2   | "response": { |
| 3   | "status": 200, |
| 4   | "message": "Successfully found 5 users", |
| 5   | "users": \[ |\
| 6   | "user\_1": { |\
| 7   | "user\_name": "test\_user\_1", |\
| 8   | "email": "test\_user\_1@email.com" |\
| 9   | }   |\
| 10  | \]  |
| 11  | }   |
| 12  | }   |

To update a dynamic variable to be the first user’s email, set the assignment like so.

![Query parameters](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F95ff0cae8613eafa8bc4312e7cafa39ac0eab34d2fd2b21f0894a30775366110%2Fassets%2Fimages%2Fconversational-ai%2Fdv-assignment.png&w=3840&q=75)

Assignments are a field of each server tool, that can be found documented [here](https://elevenlabs.io/docs/agents-platform/api-reference/tools/create#response.body.tool_config.SystemToolConfig.assignments)
.

Guide
-----

### Prerequisites

*   An [ElevenLabs account](https://elevenlabs.io/)
    
*   A configured ElevenLabs Conversational Agent ([create one here](https://elevenlabs.io/docs/agents-platform/quickstart)
    )

[1](https://elevenlabs.io/docs/agents-platform/customization/personalization/dynamic-variables#define-dynamic-variables-in-prompts)

### Define dynamic variables in prompts

Add variables using double curly braces `{{variable_name}}` in your:

*   System prompts
*   First messages
*   Tool parameters

![Dynamic variables in messages](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F84a9870018a436215fe8d6563a47f42b1b45e005c56dffff3b37dbfe8a25adf3%2Fassets%2Fimages%2Fconversational-ai%2Fdynamic-vars-first-message.png&w=3840&q=75)

![Dynamic variables in messages](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F052ab733ff3ecb2512218c70d82a4337764577b0082cdc4b2fb4415d273d2cbe%2Fassets%2Fimages%2Fconversational-ai%2Fdynamic-vars-system-prompt.png&w=3840&q=75)

[2](https://elevenlabs.io/docs/agents-platform/customization/personalization/dynamic-variables#define-dynamic-variables-in-tools)

### Define dynamic variables in tools

You can also define dynamic variables in the tool configuration. To create a new dynamic variable, set the value type to Dynamic variable and click the `+` button.

![Setting placeholders](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F7da35479f409f1505dc528e78d782d74e226fa51b2a5d3b6ede3929359be8ddd%2Fassets%2Fimages%2Fconversational-ai%2Fdynamic-vars-config.png&w=3840&q=75)

![Setting placeholders](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F21856929c95fe77b274dc1a849668c144554719cd468a225d110295ae94c7713%2Fassets%2Fimages%2Fconversational-ai%2Fdynamic-vars-path-params.png&w=3840&q=75)

[3](https://elevenlabs.io/docs/agents-platform/customization/personalization/dynamic-variables#set-placeholders)

### Set placeholders

Configure default values in the web interface for testing:

![Setting placeholders](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Fdbea803919e315240202a3cb355ef7f62f25a5182d9aa99d0916177349c70ae4%2Fassets%2Fimages%2Fconversational-ai%2Fdynamic-vars-presets.png&w=3840&q=75)

[4](https://elevenlabs.io/docs/agents-platform/customization/personalization/dynamic-variables#pass-variables-at-runtime)

### Pass variables at runtime

When starting a conversation, provide the dynamic variables in your code:

Ensure you have the latest [SDK](https://elevenlabs.io/docs/agents-platform/libraries/python)
 installed.

PythonJavaScriptSwiftWidget

|     |     |
| --- | --- |
| 1   | import os |
| 2   | import signal |
| 3   | from elevenlabs.client import ElevenLabs |
| 4   | from elevenlabs.conversational\_ai.conversation import Conversation, ConversationInitiationData |
| 5   | from elevenlabs.conversational\_ai.default\_audio\_interface import DefaultAudioInterface |
| 6   |     |
| 7   | agent\_id = os.getenv("AGENT\_ID") |
| 8   | api\_key = os.getenv("ELEVENLABS\_API\_KEY") |
| 9   | elevenlabs = ElevenLabs(api\_key=api\_key) |
| 10  |     |
| 11  | dynamic\_vars = { |
| 12  | "user\_name": "Angelo", |
| 13  | }   |
| 14  |     |
| 15  | config = ConversationInitiationData( |
| 16  | dynamic\_variables=dynamic\_vars |
| 17  | )   |
| 18  |     |
| 19  | conversation = Conversation( |
| 20  | elevenlabs, |
| 21  | agent\_id, |
| 22  | config=config, |
| 23  | # Assume auth is required when API\_KEY is set. |
| 24  | requires\_auth=bool(api\_key), |
| 25  | # Use the default audio interface. |
| 26  | audio\_interface=DefaultAudioInterface(), |
| 27  | # Simple callbacks that print the conversation to the console. |
| 28  | callback\_agent\_response=lambda response: print(f"Agent: {response}"), |
| 29  | callback\_agent\_response\_correction=lambda original, corrected: print(f"Agent: {original} -> {corrected}"), |
| 30  | callback\_user\_transcript=lambda transcript: print(f"User: {transcript}"), |
| 31  | # Uncomment the below if you want to see latency measurements. |
| 32  | # callback\_latency\_measurement=lambda latency: print(f"Latency: {latency}ms"), |
| 33  | )   |
| 34  |     |
| 35  | conversation.start\_session() |
| 36  |     |
| 37  | signal.signal(signal.SIGINT, lambda sig, frame: conversation.end\_session()) |

Public Talk-to Page Integration
-------------------------------

The public talk-to page supports dynamic variables through URL parameters, enabling you to personalize conversations when sharing agent links. This is particularly useful for embedding personalized agents in websites, emails, or marketing campaigns.

### URL Parameter Methods

There are two methods to pass dynamic variables to the public talk-to page:

#### Method 1: Base64-Encoded JSON

Pass variables as a base64-encoded JSON object using the `vars` parameter:

https://elevenlabs.io/app/talk-to?agent\_id=your\_agent\_id&vars=eyJ1c2VyX25hbWUiOiJKb2huIiwiYWNjb3VudF90eXBlIjoicHJlbWl1bSJ9

The `vars` parameter contains base64-encoded JSON:

|     |     |
| --- | --- |
| 1   | { "user\_name": "John", "account\_type": "premium" } |

#### Method 2: Individual Query Parameters

Pass variables using `var_` prefixed query parameters:

https://elevenlabs.io/app/talk-to?agent\_id=your\_agent\_id&var\_user\_name=John&var\_account\_type=premium

### Parameter Precedence

When both methods are used simultaneously, individual `var_` parameters take precedence over the base64-encoded variables to prevent conflicts:

https://elevenlabs.io/app/talk-to?agent\_id=your\_agent\_id&vars=eyJ1c2VyX25hbWUiOiJKYW5lIn0=&var\_user\_name=John

In this example, `user_name` will be “John” (from `var_user_name`) instead of “Jane” (from the base64-encoded `vars`).

### Implementation Examples

###### JavaScript URL Generation

###### Python URL Generation

###### Manual URL Construction

|     |     |
| --- | --- |
| 1   | // Method 1: Base64-encoded JSON |
| 2   | function generateTalkToURL(agentId, variables) { |
| 3   | const baseURL = 'https://elevenlabs.io/app/talk-to'; |
| 4   | const encodedVars = btoa(JSON.stringify(variables)); |
| 5   | return \`${baseURL}?agent\_id=${agentId}&vars=${encodedVars}\`; |
| 6   | }   |
| 7   |     |
| 8   | // Method 2: Individual parameters |
| 9   | function generateTalkToURLWithParams(agentId, variables) { |
| 10  | const baseURL = 'https://elevenlabs.io/app/talk-to'; |
| 11  | const params = new URLSearchParams({ agent\_id: agentId }); |
| 12  |     |
| 13  | Object.entries(variables).forEach((\[key, value\]) => { |
| 14  | params.append(\`var\_${key}\`, encodeURIComponent(value)); |
| 15  | }); |
| 16  |     |
| 17  | return \`${baseURL}?${params.toString()}\`; |
| 18  | }   |
| 19  |     |
| 20  | // Usage |
| 21  | const variables = { |
| 22  | user\_name: "John Doe", |
| 23  | account\_type: "premium", |
| 24  | session\_id: "sess\_123" |
| 25  | };  |
| 26  |     |
| 27  | const urlMethod1 = generateTalkToURL("your\_agent\_id", variables); |
| 28  | const urlMethod2 = generateTalkToURLWithParams("your\_agent\_id", variables); |

Supported Types
---------------

Dynamic variables support these value types:

String

Text values

Number

Numeric values

Boolean

True/false values

Troubleshooting
---------------

###### Variables not replacing

Verify that:

*   Variable names match exactly (case-sensitive)
*   Variables use double curly braces: `{{ variable_name }}`
*   Variables are included in your dynamic\_variables object

###### Type errors

Ensure that:

*   Variable values match the expected type
*   Values are strings, numbers, or booleans only

## Document 12 — Tools | ElevenLabs Documentation {#doc-12}
[https://elevenlabs.io/docs/agents-platform/customization/tools](https://elevenlabs.io/docs/agents-platform/customization/tools)

Overview
--------

Tools allow ElevenLabs agents to perform actions beyond generating text responses. They enable agents to interact with external systems, execute custom logic, or access specific functionalities during a conversation. This allows for richer, more capable interactions tailored to specific use cases.

ElevenLabs Agents supports the following kinds of tools:

[Client Tools\
\
Tools executed directly on the client-side application (e.g., web browser, mobile app).](https://elevenlabs.io/docs/agents-platform/customization/tools/client-tools)
[Server Tools\
\
Custom tools executed on your server-side infrastructure via API calls.](https://elevenlabs.io/docs/agents-platform/customization/tools/server-tools)
[MCP Tools\
\
Model Context Protocol servers that provide tools and resources to agents.](https://elevenlabs.io/docs/agents-platform/customization/tools/mcp)
[System Tools\
\
Built-in tools provided by the platform for common actions.](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools)

Tool Features
-------------

[Tool Call Sounds\
\
Add ambient audio during tool execution to enhance user experience.](https://elevenlabs.io/docs/agents-platform/customization/tools/tool-configuration/tool-call-sounds)

## Document 13 — Client tools | ElevenLabs Documentation {#doc-13}
[https://elevenlabs.io/docs/agents-platform/customization/tools/client-tools](https://elevenlabs.io/docs/agents-platform/customization/tools/client-tools)

**Client tools** enable your assistant to execute client-side functions. Unlike [server-side tools](https://elevenlabs.io/docs/agents-platform/customization/tools)
, client tools allow the assistant to perform actions such as triggering browser events, running client-side functions, or sending notifications to a UI.

Overview
--------

Applications may require assistants to interact directly with the user’s environment. Client-side tools give your assistant the ability to perform client-side operations.

Here are a few examples where client tools can be useful:

*   **Triggering UI events**: Allow an assistant to trigger browser events, such as alerts, modals or notifications.
*   **Interacting with the DOM**: Enable an assistant to manipulate the Document Object Model (DOM) for dynamic content updates or to guide users through complex interfaces.

To perform operations server-side, use [server-tools](https://elevenlabs.io/docs/agents-platform/customization/tools/server-tools)
 instead.

Guide
-----

### Prerequisites

*   An [ElevenLabs account](https://elevenlabs.io/)
    
*   A configured ElevenLabs Conversational Agent ([create one here](https://elevenlabs.io/app/agents)
    )

[1](https://elevenlabs.io/docs/agents-platform/customization/tools/client-tools#create-a-new-client-side-tool)

### Create a new client-side tool

Navigate to your agent dashboard. In the **Tools** section, click **Add Tool**. Ensure the **Tool Type** is set to **Client**. Then configure the following:

| Setting | Parameter |
| --- | --- |
| Name | logMessage |
| Description | Use this client-side tool to log a message to the user’s client. |

Then create a new parameter `message` with the following configuration:

| Setting | Parameter |
| --- | --- |
| Data Type | String |
| Identifier | message |
| Required | true |
| Description | The message to log in the console. Ensure the message is informative and relevant. |

![logMessage client-tool setup](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Ff7ed25d49a2a814b76112f3e385d471e0dc8444705e11f2f6fad0bd23f1eae12%2Fassets%2Fimages%2Fconversational-ai%2Fclient-tool-example.jpg&w=3840&q=75)

[2](https://elevenlabs.io/docs/agents-platform/customization/tools/client-tools#register-the-client-tool-in-your-code)

### Register the client tool in your code

Unlike server-side tools, client tools need to be registered in your code.

Use the following code to register the client tool:

PythonJavaScriptSwift

|     |     |
| --- | --- |
| 1   | from elevenlabs import ElevenLabs |
| 2   | from elevenlabs.conversational\_ai.conversation import Conversation, ClientTools |
| 3   |     |
| 4   | def log\_message(parameters): |
| 5   | message = parameters.get("message") |
| 6   | print(message) |
| 7   |     |
| 8   | client\_tools = ClientTools() |
| 9   | client\_tools.register("logMessage", log\_message) |
| 10  |     |
| 11  | conversation = Conversation( |
| 12  | client=ElevenLabs(api\_key="your-api-key"), |
| 13  | agent\_id="your-agent-id", |
| 14  | client\_tools=client\_tools, |
| 15  | # ... |
| 16  | )   |
| 17  |     |
| 18  | conversation.start\_session() |

The tool and parameter names in the agent configuration are case-sensitive and **must** match those registered in your code.

[3](https://elevenlabs.io/docs/agents-platform/customization/tools/client-tools#testing)

### Testing

Initiate a conversation with your agent and say something like:

> _Log a message to the console that says Hello World_

You should see a `Hello World` log appear in your console.

[4](https://elevenlabs.io/docs/agents-platform/customization/tools/client-tools#next-steps)

### Next steps

Now that you’ve set up a basic client-side event, you can:

*   Explore more complex client tools like opening modals, navigating to pages, or interacting with the DOM.
*   Combine client tools with server-side webhooks for full-stack interactions.
*   Use client tools to enhance user engagement and provide real-time feedback during conversations.

### Passing client tool results to the conversation context

When you want your agent to receive data back from a client tool, ensure that you tick the **Wait for response** option in the tool configuration.

![Wait for response option in client tool configuration](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F0ecc615fc9f25446b67369fd3e010e34b39549a22146a2483ea17251206caf1e%2Fassets%2Fimages%2Fconversational-ai%2Fwait-until-tool-result.png&w=3840&q=75)

Once the client tool is added, when the function is called the agent will wait for its response and append the response to the conversation context.

PythonJavaScript

|     |     |
| --- | --- |
| 1   | def get\_customer\_details(): |
| 2   | # Fetch customer details (e.g., from an API or database) |
| 3   | customer\_data = { |
| 4   | "id": 123, |
| 5   | "name": "Alice", |
| 6   | "subscription": "Pro" |
| 7   | }   |
| 8   | # Return the customer data; it can also be a JSON string if needed. |
| 9   | return customer\_data |
| 10  |     |
| 11  | client\_tools = ClientTools() |
| 12  | client\_tools.register("getCustomerDetails", get\_customer\_details) |
| 13  |     |
| 14  | conversation = Conversation( |
| 15  | client=ElevenLabs(api\_key="your-api-key"), |
| 16  | agent\_id="your-agent-id", |
| 17  | client\_tools=client\_tools, |
| 18  | # ... |
| 19  | )   |
| 20  |     |
| 21  | conversation.start\_session() |

In this example, when the agent calls **getCustomerDetails**, the function will execute on the client and the agent will receive the returned data, which is then used as part of the conversation context. The values from the response can also optionally be assigned to dynamic variables, similar to [server tools](https://elevenlabs.io/docs/agents-platform/customization/tools/server-tools)
. Note system tools cannot update dynamic variables.

### Troubleshooting

###### Tools not being triggered

*   Ensure the tool and parameter names in the agent configuration match those registered in your code.
*   View the conversation transcript in the agent dashboard to verify the tool is being executed.

###### Console errors

*   Open the browser console to check for any errors.
*   Ensure that your code has necessary error handling for undefined or unexpected parameters.

Best practices
--------------

#### Name tools intuitively, with detailed descriptions

If you find the assistant does not make calls to the correct tools, you may need to update your tool names and descriptions so the assistant more clearly understands when it should select each tool. Avoid using abbreviations or acronyms to shorten tool and argument names.

You can also include detailed descriptions for when a tool should be called. For complex tools, you should include descriptions for each of the arguments to help the assistant know what it needs to ask the user to collect that argument.

#### Name tool parameters intuitively, with detailed descriptions

Use clear and descriptive names for tool parameters. If applicable, specify the expected format for a parameter in the description (e.g., YYYY-mm-dd or dd/mm/yy for a date).

#### 

Consider providing additional information about how and when to call tools in your assistant’s system prompt

Providing clear instructions in your system prompt can significantly improve the assistant’s tool calling accuracy. For example, guide the assistant with instructions like the following:

Use \`check\_order\_status\` when the user inquires about the status of their order, such as 'Where is my order?' or 'Has my order shipped yet?'.

Provide context for complex scenarios. For example:

Before scheduling a meeting with \`schedule\_meeting\`, check the user's calendar for availability using check\_availability to avoid conflicts.

#### LLM selection

When using tools, we recommend picking high intelligence models like GPT-4o mini or Claude 3.5 Sonnet and avoiding Gemini 1.5 Flash.

It’s important to note that the choice of LLM matters to the success of function calls. Some LLMs can struggle with extracting the relevant parameters from the conversation.

Ask AI

Assistant

Hi, I'm an AI assistant with access to documentation and other content.

Tip: You can toggle this pane with

⌘

+

/

![logMessage client-tool setup](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Ff7ed25d49a2a814b76112f3e385d471e0dc8444705e11f2f6fad0bd23f1eae12%2Fassets%2Fimages%2Fconversational-ai%2Fclient-tool-example.jpg&w=3840&q=75)

![Wait for response option in client tool configuration](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F0ecc615fc9f25446b67369fd3e010e34b39549a22146a2483ea17251206caf1e%2Fassets%2Fimages%2Fconversational-ai%2Fwait-until-tool-result.png&w=3840&q=75)

## Document 14 — Model Context Protocol | ElevenLabs Documentation {#doc-14}
[https://elevenlabs.io/docs/agents-platform/customization/tools/mcp](https://elevenlabs.io/docs/agents-platform/customization/tools/mcp)

##### User Responsibility

You are responsible for the security, compliance, and behavior of any third-party MCP server you integrate with your ElevenLabs conversational agents. ElevenLabs provides the platform for integration but does not manage, endorse, or secure external MCP servers.

Overview
--------

The [Model Context Protocol (MCP)](https://modelcontextprotocol.io/)
 is an open standard that defines how applications provide context to Large Language Models (LLMs). Think of MCP as a universal connector that enables AI models to seamlessly interact with diverse data sources and tools. By integrating servers that implement MCP, you can significantly extend the capabilities of your ElevenLabs conversational agents.

MCP support is not currently available for users on Zero Retention Mode or those requiring HIPAA compliance.

ElevenLabs allows you to connect your conversational agents to external MCP servers. This enables your agents to:

*   Access and process information from various data sources via the MCP server
*   Utilize specialized tools and functionalities exposed by the MCP server
*   Create more dynamic, knowledgeable, and interactive conversational experiences

Getting started
---------------

ElevenLabs supports both SSE (Server-Sent Events) and HTTP streamable transport MCP servers.

1.  Retrieve the URL of your MCP server. In this example, we’ll use [Zapier MCP](https://zapier.com/mcp)
    , which lets you connect Agents Platform to hundreds of tools and services.
    
2.  Navigate to the [MCP server integrations dashboard](https://elevenlabs.io/app/agents/integrations)
     and click “Add Custom MCP Server”.
    
    ![Creating your first MCP server](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F335d1d8fced3cd82eed4ed43ba0058d3a8dbc431f81cdb387bf3d5964e5ead80%2Fassets%2Fimages%2Fconversational-ai%2Fmcp-create.png&w=3840&q=75)
    
3.  Configure the MCP server with the following details:
    
    *   **Name**: The name of the MCP server (e.g., “Zapier MCP Server”)
    *   **Description**: A description of what the MCP server can do (e.g., “An MCP server with access to Zapier’s tools and services”)
    *   **Server URL**: The URL of the MCP server. In some cases this contains a secret key, treat it like a password and store it securely as a workspace secret.
    *   **Secret Token (Optional)**: If the MCP server requires a secret token (Authorization header), enter it here.
    *   **HTTP Headers (Optional)**: If the MCP server requires additional HTTP headers, enter them here.
4.  Click “Add Integration” to save the integration and test the connection to list available tools.
    
    ![Zapier example tools](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F7c9d83baedd0e2dc74014d4a690159333bbef0eec37c36fa9ee0fd6cd455df0e%2Fassets%2Fimages%2Fconversational-ai%2Fmcp-zapier.png&w=3840&q=75)
    
5.  The MCP server is now available to add to your agents. MCP support is available for both public and private agents.
    
    ![Adding the MCP server to an agent](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Fb18756710b4466a5fa462080d6f3b237d84f76f898d5c94d4583f31d2fa0395b%2Fassets%2Fimages%2Fconversational-ai%2Fmcp-add.png&w=3840&q=75)
    

Tool approval modes
-------------------

ElevenLabs provides flexible approval controls to manage how agents request permission to use tools from MCP servers. You can configure approval settings at both the MCP server level and individual tool level for maximum security control.

![Tool approval mode settings](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F9f4c208459131622478de5010e2234d3af8a03dc9bc5b7ec2fcab48a7be3bde3%2Fassets%2Fimages%2Fconversational-ai%2Fmcp-approval.png&w=3840&q=75)

### Available approval modes

*   **Always Ask (Recommended)**: Maximum security. The agent will request your permission before each tool use.
*   **Fine-Grained Tool Approval**: Disable and pre-select tools which can run automatically and those requiring approval.
*   **No Approval**: The assistant can use any tool without approval.

### Fine-grained tool control

The Fine-Grained Tool Approval mode allows you to configure individual tools with different approval requirements, giving you precise control over which tools can run automatically and which require explicit permission.

![Fine-grained tool approval\
settings](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F041cad753319ed92189ee8d4f70a9d3ed07177303bf71777ce36a90270a5dd24%2Fassets%2Fimages%2Fconversational-ai%2Fmcp-finegrained-approvals.png&w=3840&q=75)

For each tool, you can set:

*   **Auto-approved**: Tool runs automatically without requiring permission
*   **Requires approval**: Tool requires explicit permission before execution
*   **Disabled**: Tool is completely disabled and cannot be used

Use Fine-Grained Tool Approval to allow low-risk read-only tools to run automatically while requiring approval for tools that modify data or perform sensitive operations.

Key considerations for ElevenLabs integration
---------------------------------------------

*   **External servers**: You are responsible for selecting the external MCP servers you wish to integrate. ElevenLabs provides the means to connect to them.
*   **Supported features**: ElevenLabs supports MCP servers that communicate over SSE (Server-Sent Events) and HTTP streamable transports for real-time interactions.
*   **Dynamic tools**: The tools and capabilities available from an integrated MCP server are defined by that external server and can change if the server’s configuration is updated.

Security and disclaimer
-----------------------

Integrating external MCP servers can expose your agents and data to third-party services. It is crucial to understand the security implications.

##### Important Disclaimer

By enabling MCP server integrations, you acknowledge that this may involve data sharing with third-party services not controlled by ElevenLabs. This could incur additional security risks. Please ensure you fully understand the implications, vet the security of any MCP server you integrate, and review our [MCP Integration Security Guidelines](https://elevenlabs.io/docs/agents-platform/customization/tools/mcp/security)
 before proceeding.

Refer to our [MCP Integration Security Guidelines](https://elevenlabs.io/docs/agents-platform/customization/tools/mcp/security)
 for detailed best practices.

Finding or building MCP servers
-------------------------------

*   Utilize publicly available MCP servers from trusted providers
*   Develop your own MCP server to expose your proprietary data or tools
*   Explore the Model Context Protocol community and resources for examples and server implementations

### Resources

*   [Anthropic’s MCP server examples](https://docs.anthropic.com/en/docs/agents-and-tools/remote-mcp-servers#remote-mcp-server-examples)
     - A list of example servers by Anthropic
*   [Awesome Remote MCP Servers](https://github.com/jaw9c/awesome-remote-mcp-servers)
     - A curated, open-source list of remote MCP servers
*   [Remote MCP Server Directory](https://remote-mcp.com/)
     - A searchable list of Remote MCP servers

## Document 15 — MCP integration security | ElevenLabs Documentation {#doc-15}
[https://elevenlabs.io/docs/agents-platform/customization/tools/mcp/security](https://elevenlabs.io/docs/agents-platform/customization/tools/mcp/security)

##### User Responsibility

You are responsible for the security, compliance, and behavior of any third-party MCP server you integrate with your ElevenLabs conversational agents. ElevenLabs provides the platform for integration but does not manage, endorse, or secure external MCP servers.

Overview
--------

Integrating external servers via the Model Context Protocol (MCP) can greatly enhance your ElevenLabs conversational agents. However, this also means connecting to systems outside of ElevenLabs’ direct control, which introduces important security considerations. As a user, you are responsible for the security and trustworthiness of any third-party MCP server you choose to integrate.

This guide outlines key security practices to consider when using MCP server integrations within ElevenLabs.

Tool approval controls
----------------------

ElevenLabs provides built-in security controls through tool approval modes that help you manage the security risks associated with MCP tool usage. These controls allow you to balance functionality with security based on your specific needs.

![Tool approval mode settings](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F9f4c208459131622478de5010e2234d3af8a03dc9bc5b7ec2fcab48a7be3bde3%2Fassets%2Fimages%2Fconversational-ai%2Fmcp-approval.png&w=3840&q=75)

### Approval mode options

*   **Always Ask (Recommended)**: Provides maximum security by requiring explicit approval for every tool execution. This mode ensures you maintain full control over all MCP tool usage.
*   **Fine-Grained Tool Approval**: Allows you to configure approval requirements on a per-tool basis, enabling automatic execution of trusted tools while requiring approval for sensitive operations.
*   **No Approval**: Permits unrestricted tool usage without approval prompts. Only use this mode with thoroughly vetted and highly trusted MCP servers.

### Fine-grained security controls

Fine-Grained Tool Approval mode provides the most flexible security configuration, allowing you to classify each tool based on its risk profile:

![Fine-grained tool approval\
settings](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F041cad753319ed92189ee8d4f70a9d3ed07177303bf71777ce36a90270a5dd24%2Fassets%2Fimages%2Fconversational-ai%2Fmcp-finegrained-approvals.png&w=3840&q=75)

*   **Auto-approved tools**: Suitable for low-risk, read-only operations or tools you completely trust
*   **Approval-required tools**: For tools that modify data, access sensitive information, or perform potentially risky operations
*   **Disabled tools**: Completely block tools that are unnecessary or pose security risks

Even with approval controls in place, carefully evaluate the trustworthiness of MCP servers and understand what each tool can access or modify before integration.

Security tips
-------------

### 1\. Vet your MCP servers

*   **Trusted Sources**: Only integrate MCP servers from sources you trust and have verified. Understand who operates the server and their security posture.
*   **Understand Capabilities**: Before integrating, thoroughly review the tools and data resources the MCP server exposes. Be aware of what actions its tools can perform (e.g., accessing files, calling external APIs, modifying data). The MCP `destructiveHint` and `readOnlyHint` annotations can provide clues but should not be solely relied upon for security decisions.
*   **Review Server Security**: If possible, review the security practices of the MCP server provider. For MCP servers you develop, ensure you follow general server security best practices and the MCP-specific security guidelines.

### 2\. Data sharing and privacy

*   **Data Flow**: Be aware that when your agent uses an integrated MCP server, data from the conversation (which may include user inputs) will be sent to that external server.
*   **Sensitive Information**: Exercise caution when allowing agents to send Personally Identifiable Information (PII) or other sensitive data to an MCP server. Ensure the server handles such data securely and in compliance with relevant privacy regulations.
*   **Purpose Limitation**: Configure your agents and prompts to only share the necessary information with MCP server tools to perform their tasks.

### 3\. Credential and connection security

*   **Secure Storage**: If an MCP server requires API keys or other secrets for authentication, use any available secret management features within the ElevenLabs platform to store these credentials securely. Avoid hardcoding secrets.
*   **HTTPS**: Ensure connections to MCP servers are made over HTTPS to encrypt data in transit.
*   **Network Access**: If the MCP server is on a private network, ensure appropriate firewall rules and network ACLs are in place.

#### IP whitelisting

For additional security, you can whitelist the following static egress IPs from which ElevenLabs requests originate:

| Region | IP Address |
| --- | --- |
| US (Default) | 34.67.146.145 |
| US (Default) | 34.59.11.47 |
| EU  | 35.204.38.71 |
| EU  | 34.147.113.54 |
| Asia | 35.185.187.110 |
| Asia | 35.247.157.189 |

If you are using a [data residency region](https://elevenlabs.io/docs/overview/administration/data-residency)
 then the following IPs will be used:

| Region | IP Address |
| --- | --- |
| EU Residency | 34.77.234.246 |
| EU Residency | 34.140.184.144 |
| India Residency | 34.93.26.174 |
| India Residency | 34.93.252.69 |

If your infrastructure requires strict IP-based access controls, adding these IPs to your firewall allowlist will ensure you only receive requests from ElevenLabs’ systems.

These static IPs are used across all ElevenLabs services including webhooks and MCP server requests, and will remain consistent.

### 4\. Understand code execution risks

*   **Remote Execution**: Tools exposed by an MCP server execute code on that server. While this is the basis of their functionality, it’s a critical security consideration. Malicious or poorly secured tools could pose a risk.
*   **Input Validation**: Although the MCP server is responsible for validating inputs to its tools, be mindful of the data your agent might send. The LLM should be guided to use tools as intended.

### 5\. Add guardrails

*   **Prompt Injections**: Connecting to untrusted external MCP servers exposes the risk of prompt injection attacks. Ensure to add thorough guardrails to your system prompt to reduce the risk of exposure to a malicious attack.
*   **Tool Approval Configuration**: Use the appropriate approval mode for your security requirements. Start with “Always Ask” for new integrations and only move to less restrictive modes after thorough testing and trust establishment.

### 6\. Monitor and review

*   **Logging (Server-Side)**: If you control the MCP server, implement comprehensive logging of tool invocations and data access.
*   **Regular Review**: Periodically review your integrated MCP servers. Check if their security posture has changed or if new tools have been added that require re-assessment.
*   **Approval Patterns**: Monitor tool approval requests to identify unusual patterns that might indicate security issues or misuse.

Disclaimer
----------

##### Important Disclaimer

By enabling MCP server integrations, you acknowledge that this may involve data sharing with third-party services not controlled by ElevenLabs. This could incur additional security risks. Please ensure you fully understand the implications, vet the security of any MCP server you integrate, and adhere to these security guidelines before proceeding.

For general information on the Model Context Protocol, refer to official MCP documentation and community resources.

## Document 16 — Server tools | ElevenLabs Documentation {#doc-16}
[https://elevenlabs.io/docs/agents-platform/customization/tools/server-tools](https://elevenlabs.io/docs/agents-platform/customization/tools/server-tools)

**Tools** enable your assistant to connect to external data and systems. You can define a set of tools that the assistant has access to, and the assistant will use them where appropriate based on the conversation.

Overview
--------

Many applications require assistants to call external APIs to get real-time information. Tools give your assistant the ability to make external function calls to third party apps so you can get real-time information.

Here are a few examples where tools can be useful:

*   **Fetching data**: enable an assistant to retrieve real-time data from any REST-enabled database or 3rd party integration before responding to the user.
*   **Taking action**: allow an assistant to trigger authenticated actions based on the conversation, like scheduling meetings or initiating order returns.

To interact with Application UIs or trigger client-side events use [client tools](https://elevenlabs.io/docs/agents-platform/customization/tools/client-tools)
 instead.

Tool configuration
------------------

ElevenLabs agents can be equipped with tools to interact with external APIs. Unlike traditional requests, the assistant generates query, body, and path parameters dynamically based on the conversation and parameter descriptions you provide.

All tool configurations and parameter descriptions help the assistant determine **when** and **how** to use these tools. To orchestrate tool usage effectively, update the assistant’s system prompt to specify the sequence and logic for making these calls. This includes:

*   **Which tool** to use and under what conditions.
*   **What parameters** the tool needs to function properly.
*   **How to handle** the responses.

  

###### Configuration

###### Authentication

###### Headers

###### Path parameters

###### Body parameters

###### Query parameters

###### Dynamic variable assignment

Define a high-level `Name` and `Description` to describe the tool’s purpose. This helps the LLM understand the tool and know when to call it.

If the API requires path parameters, include variables in the URL path by wrapping them in curly braces `{}`, for example: `/api/resource/{id}` where `id` is a path parameter.

![Configuration](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Ffb6e6619e4e7a5f19c2a86f9c2a489f5cb33cfb0883c14a06f0eec3cb35d71d5%2Fassets%2Fimages%2Fconversational-ai%2Ftool-configuration.jpg&w=3840&q=75)

Guide
-----

In this guide, we’ll create a weather assistant that can provide real-time weather information for any location. The assistant will use its geographic knowledge to convert location names into coordinates and fetch accurate weather data.

[1](https://elevenlabs.io/docs/agents-platform/customization/tools/server-tools#configure-the-weather-tool)

### Configure the weather tool

First, on the **Agent** section of your agent settings page, choose **Add Tool**. Select **Webhook** as the Tool Type, then configure the weather API integration:

###### Weather Tool Configuration

###### Configuration

###### Path Parameters

| Field | Value |
| --- | --- |
| Name | get\_weather |
| Description | Gets the current weather forecast for a location |
| Method | GET |
| URL | [https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature\_2m,wind\_speed\_10m&hourly=temperature\_2m,relative\_humidity\_2m,wind\_speed\_10m](https://api.open-meteo.com/v1/forecast?latitude=%7Blatitude%7D&longitude=%7Blongitude%7D&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m) |

An API key is not required for this tool. If one is required, this should be passed in the headers and stored as a secret.

[2](https://elevenlabs.io/docs/agents-platform/customization/tools/server-tools#orchestration)

### Orchestration

Configure your assistant to handle weather queries intelligently with this system prompt:

System prompt

|     |
| --- |
| You are a helpful conversational agent with access to a weather tool. When users ask about |
| weather conditions, use the get\_weather tool to fetch accurate, real-time data. The tool requires |
| a latitude and longitude - use your geographic knowledge to convert location names to coordinates |
| accurately. |
|     |
| Never ask users for coordinates - you must determine these yourself. Always report weather |
| information conversationally, referring to locations by name only. For weather requests: |
|     |
| 1\. Extract the location from the user's message |
| 2\. Convert the location to coordinates and call get\_weather |
| 3\. Present the information naturally and helpfully |
|     |
| For non-weather queries, provide friendly assistance within your knowledge boundaries. Always be |
| concise, accurate, and helpful. |
|     |
| First message: "Hey, how can I help you today?" |

Test your assistant by asking about the weather in different locations. The assistant should handle specific locations (“What’s the weather in Tokyo?”) and ask for clarification after general queries (“How’s the weather looking today?”).

Supported Authentication Methods
--------------------------------

ElevenLabs Agents supports multiple authentication methods to securely connect your tools with external APIs. Authentication methods are configured in your agent settings and then connected to individual tools as needed.

![Workspace Auth Connection](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F131f7e017f01eaba444cddc672efdb77415db1a4c3a39f05b92a1678c5cd68f1%2Fassets%2Fimages%2Fconversational-ai%2Fworkspace-auth-connection.png&w=3840&q=75)

Once configured, you can connect these authentication methods to your tools and manage custom headers in the tool configuration:

![Tool Auth Connection](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Fc018c5bf11266512a6d6157e33274b1a12f8c20768e5ab18da0f20f933f8aea9%2Fassets%2Fimages%2Fconversational-ai%2Ftool-auth-config.png&w=3840&q=75)

#### OAuth2 Client Credentials

Automatically handles the OAuth2 client credentials flow. Configure with your client ID, client secret, and token URL (e.g., `https://api.example.com/oauth/token`). Optionally specify scopes as comma-separated values and additional JSON parameters. Set up by clicking **Add Auth** on **Workspace Auth Connections** on the **Agent** section of your agent settings page.

#### OAuth2 JWT

Uses JSON Web Token authentication for OAuth 2.0 JWT Bearer flow. Requires your JWT signing secret, token URL, and algorithm (default: HS256). Configure JWT claims including issuer, audience, and subject. Optionally set key ID, expiration (default: 3600 seconds), scopes, and extra parameters. Set up by clicking **Add Auth** on **Workspace Auth Connections** on the **Agent** section of your agent settings page.

#### Basic Authentication

Simple username and password authentication for APIs that support HTTP Basic Auth. Set up by clicking **Add Auth** on **Workspace Auth Connections** in the **Agent** section of your agent settings page.

#### Bearer Tokens

Token-based authentication that adds your bearer token value to the request header. Configure by adding a header to the tool configuration, selecting **Secret** as the header type, and clicking **Create New Secret**.

#### Custom Headers

Add custom authentication headers with any name and value for proprietary authentication methods. Configure by adding a header to the tool configuration and specifying its **name** and **value**.

Best practices
--------------

#### Name tools intuitively, with detailed descriptions

If you find the assistant does not make calls to the correct tools, you may need to update your tool names and descriptions so the assistant more clearly understands when it should select each tool. Avoid using abbreviations or acronyms to shorten tool and argument names.

You can also include detailed descriptions for when a tool should be called. For complex tools, you should include descriptions for each of the arguments to help the assistant know what it needs to ask the user to collect that argument.

#### Name tool parameters intuitively, with detailed descriptions

Use clear and descriptive names for tool parameters. If applicable, specify the expected format for a parameter in the description (e.g., YYYY-mm-dd or dd/mm/yy for a date).

#### 

Consider providing additional information about how and when to call tools in your assistant’s system prompt

Providing clear instructions in your system prompt can significantly improve the assistant’s tool calling accuracy. For example, guide the assistant with instructions like the following:

Use \`check\_order\_status\` when the user inquires about the status of their order, such as 'Where is my order?' or 'Has my order shipped yet?'.

Provide context for complex scenarios. For example:

Before scheduling a meeting with \`schedule\_meeting\`, check the user's calendar for availability using check\_availability to avoid conflicts.

#### LLM selection

When using tools, we recommend picking high intelligence models like GPT-4o mini or Claude 3.5 Sonnet and avoiding Gemini 1.5 Flash.

It’s important to note that the choice of LLM matters to the success of function calls. Some LLMs can struggle with extracting the relevant parameters from the conversation.

Tool Call Sounds
----------------

You can configure ambient audio to play during tool execution to enhance the user experience. Learn more about [Tool Call Sounds](https://elevenlabs.io/docs/agents-platform/customization/tools/tool-configuration/tool-call-sounds)
.

## Document 17 — System tools | ElevenLabs Documentation {#doc-17}
[https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools)

**System tools** enable your assistant to update the internal state of a conversation. Unlike [server tools](https://elevenlabs.io/docs/agents-platform/customization/tools/server-tools)
 or [client tools](https://elevenlabs.io/docs/agents-platform/customization/tools/client-tools)
, system tools don’t make external API calls or trigger client-side functions—they modify the internal state of the conversation without making external calls.

Overview
--------

Some applications require agents to control the flow or state of a conversation. System tools provide this capability by allowing the assistant to perform actions related to the state of the call that don’t require communicating with external servers or the client.

### Available system tools

[End call\
\
Let your agent automatically terminate a conversation when appropriate conditions are met.](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/end-call)
[Language detection\
\
Enable your agent to automatically switch to the user’s language during conversations.](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/language-detection)
[Agent transfer\
\
Seamlessly transfer conversations between AI agents based on defined conditions.](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/agent-transfer)
[Transfer to human\
\
Seamlessly transfer the user to a human operator.](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/transfer-to-human)
[Skip turn\
\
Enable the agent to skip their turns if the LLM detects the agent should not speak yet.](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/skip-turn)
[Play keypad touch tone\
\
Enable agents to play DTMF tones to interact with automated phone systems and navigate menus.](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/play-keypad-touch-tone)
[Voicemail detection\
\
Enable agents to automatically detect voicemail systems and optionally leave messages.](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/voicemail-detection)

Implementation
--------------

When creating an agent via API, you can add system tools to your agent configuration. Here’s how to implement both the end call and language detection tools:

Custom LLM integration
----------------------

When using a custom LLM with ElevenLabs agents, system tools are exposed as function definitions that your LLM can call. Each system tool has specific parameters and trigger conditions:

### Available system tools

###### End call

**Purpose**: Automatically terminate conversations when appropriate conditions are met.

**Trigger conditions**: The LLM should call this tool when:

*   The main task has been completed and user is satisfied
*   The conversation reached natural conclusion with mutual agreement
*   The user explicitly indicates they want to end the conversation

**Parameters**:

*   `reason` (string, required): The reason for ending the call
*   `message` (string, optional): A farewell message to send to the user before ending the call

**Function call format**:

|     |     |
| --- | --- |
| 1   | {   |
| 2   | "type": "function", |
| 3   | "function": { |
| 4   | "name": "end\_call", |
| 5   | "arguments": "{\\"reason\\": \\"Task completed successfully\\", \\"message\\": \\"Thank you for using our service. Have a great day!\\"}" |
| 6   | }   |
| 7   | }   |

**Implementation**: Configure as a system tool in your agent settings. The LLM will receive detailed instructions about when to call this function.

Learn more: [End call tool](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/end-call)

###### Language detection

**Purpose**: Automatically switch to the user’s detected language during conversations.

**Trigger conditions**: The LLM should call this tool when:

*   User speaks in a different language than the current conversation language
*   User explicitly requests to switch languages
*   Multi-language support is needed for the conversation

**Parameters**:

*   `reason` (string, required): The reason for the language switch
*   `language` (string, required): The language code to switch to (must be in supported languages list)

**Function call format**:

|     |     |
| --- | --- |
| 1   | {   |
| 2   | "type": "function", |
| 3   | "function": { |
| 4   | "name": "language\_detection", |
| 5   | "arguments": "{\\"reason\\": \\"User requested Spanish\\", \\"language\\": \\"es\\"}" |
| 6   | }   |
| 7   | }   |

**Implementation**: Configure supported languages in agent settings and add the language detection system tool. The agent will automatically switch voice and responses to match detected languages.

Learn more: [Language detection tool](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/language-detection)

###### Agent transfer

**Purpose**: Transfer conversations between specialized AI agents based on user needs.

**Trigger conditions**: The LLM should call this tool when:

*   User request requires specialized knowledge or different agent capabilities
*   Current agent cannot adequately handle the query
*   Conversation flow indicates need for different agent type

**Parameters**:

*   `reason` (string, optional): The reason for the agent transfer
*   `agent_number` (integer, required): Zero-indexed number of the agent to transfer to (based on configured transfer rules)

**Function call format**:

|     |     |
| --- | --- |
| 1   | {   |
| 2   | "type": "function", |
| 3   | "function": { |
| 4   | "name": "transfer\_to\_agent", |
| 5   | "arguments": "{\\"reason\\": \\"User needs billing support\\", \\"agent\_number\\": 0}" |
| 6   | }   |
| 7   | }   |

**Implementation**: Define transfer rules mapping conditions to specific agent IDs. Configure which agents the current agent can transfer to. Agents are referenced by zero-indexed numbers in the transfer configuration.

Learn more: [Agent transfer tool](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/agent-transfer)

###### Transfer to human

**Purpose**: Seamlessly hand off conversations to human operators when AI assistance is insufficient.

**Trigger conditions**: The LLM should call this tool when:

*   Complex issues requiring human judgment
*   User explicitly requests human assistance
*   AI reaches limits of capability for the specific request
*   Escalation protocols are triggered

**Parameters**:

*   `reason` (string, optional): The reason for the transfer
*   `transfer_number` (string, required): The phone number to transfer to (must match configured numbers)
*   `client_message` (string, required): Message read to the client while waiting for transfer
*   `agent_message` (string, required): Message for the human operator receiving the call

**Function call format**:

|     |     |
| --- | --- |
| 1   | {   |
| 2   | "type": "function", |
| 3   | "function": { |
| 4   | "name": "transfer\_to\_number", |
| 5   | "arguments": "{\\"reason\\": \\"Complex billing issue\\", \\"transfer\_number\\": \\"+15551234567\\", \\"client\_message\\": \\"I'm transferring you to a billing specialist who can help with your account.\\", \\"agent\_message\\": \\"Customer has a complex billing dispute about order #12345 from last month.\\"}" |
| 6   | }   |
| 7   | }   |

**Implementation**: Configure transfer phone numbers and conditions. Define messages for both customer and receiving human operator. Works with both Twilio and SIP trunking.

Learn more: [Transfer to human tool](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/transfer-to-human)

###### Skip turn

**Purpose**: Allow the agent to pause and wait for user input without speaking.

**Trigger conditions**: The LLM should call this tool when:

*   User indicates they need a moment (“Give me a second”, “Let me think”)
*   User requests pause in conversation flow
*   Agent detects user needs time to process information

**Parameters**:

*   `reason` (string, optional): Free-form reason explaining why the pause is needed

**Function call format**:

|     |     |
| --- | --- |
| 1   | {   |
| 2   | "type": "function", |
| 3   | "function": { |
| 4   | "name": "skip\_turn", |
| 5   | "arguments": "{\\"reason\\": \\"User requested time to think\\"}" |
| 6   | }   |
| 7   | }   |

**Implementation**: No additional configuration needed. The tool simply signals the agent to remain silent until the user speaks again.

Learn more: [Skip turn tool](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/skip-turn)

###### Play keypad touch tone

**Parameters**:

*   `reason` (string, optional): The reason for playing the DTMF tones (e.g., “navigating to extension”, “entering PIN”)
*   `dtmf_tones` (string, required): The DTMF sequence to play. Valid characters: 0-9, \*, #, w (0.5s pause), W (1s pause)

**Function call format**:

|     |     |
| --- | --- |
| 1   | {   |
| 2   | "type": "function", |
| 3   | "function": { |
| 4   | "name": "play\_keypad\_touch\_tone", |
| 5   | "arguments": "{"reason": "Navigating to customer service", "dtmf\_tones": "2"}" |
| 6   | }   |
| 7   | }   |

Learn more: [Play keypad touch tone tool](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/play-keypad-touch-tone)

###### Voicemail detection

**Parameters**:

*   `reason` (string, required): The reason for detecting voicemail (e.g., “automated greeting detected”, “no human response”)

**Function call format**:

|     |     |
| --- | --- |
| 1   | {   |
| 2   | "type": "function", |
| 3   | "function": { |
| 4   | "name": "voicemail\_detection", |
| 5   | "arguments": "{\\"reason\\": \\"Automated greeting detected with request to leave message\\"}" |
| 6   | }   |
| 7   | }   |

Learn more: [Voicemail detection tool](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/voicemail-detection)

PythonJavaScript

|     |     |
| --- | --- |
| 1   | from elevenlabs import ( |
| 2   | ConversationalConfig, |
| 3   | ElevenLabs, |
| 4   | AgentConfig, |
| 5   | PromptAgent, |
| 6   | PromptAgentInputToolsItem\_System, |
| 7   | )   |
| 8   |     |
| 9   | \# Initialize the client |
| 10  | elevenlabs = ElevenLabs(api\_key="YOUR\_API\_KEY") |
| 11  |     |
| 12  | \# Create system tools |
| 13  | end\_call\_tool = PromptAgentInputToolsItem\_System( |
| 14  | name="end\_call", |
| 15  | description=""  # Optional: Customize when the tool should be triggered |
| 16  | )   |
| 17  |     |
| 18  | language\_detection\_tool = PromptAgentInputToolsItem\_System( |
| 19  | name="language\_detection", |
| 20  | description=""  # Optional: Customize when the tool should be triggered |
| 21  | )   |
| 22  |     |
| 23  | \# Create the agent configuration with both tools |
| 24  | conversation\_config = ConversationalConfig( |
| 25  | agent=AgentConfig( |
| 26  | prompt=PromptAgent( |
| 27  | tools=\[end\_call\_tool, language\_detection\_tool\] |
| 28  | )   |
| 29  | )   |
| 30  | )   |
| 31  |     |
| 32  | \# Create the agent |
| 33  | response = elevenlabs.conversational\_ai.agents.create( |
| 34  | conversation\_config=conversation\_config |
| 35  | )   |

FAQ
---

###### Can system tools be combined with other tool types?

Yes, system tools can be used alongside server tools and client tools in the same assistant. This allows for comprehensive functionality that combines internal state management with external interactions.

## Document 18 — Agent transfer | ElevenLabs Documentation {#doc-18}
[https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/agent-transfer](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/agent-transfer)

Overview
--------

Agent-agent transfer allows a ElevenLabs agent to hand off the ongoing conversation to another designated agent when specific conditions are met. This enables the creation of sophisticated, multi-layered conversational workflows where different agents handle specific tasks or levels of complexity.

For example, an initial agent (Orchestrator) could handle general inquiries and then transfer the call to a specialized agent based on the conversation’s context. Transfers can also be nested:

|     |
| --- |
| Orchestrator Agent (Initial Qualification) |
| │   |
| ├───> Agent 1 (e.g., Availability Inquiries) |
| │   |
| ├───> Agent 2 (e.g., Technical Support) |
| │     │ |
| │     └───> Agent 2a (e.g., Hardware Support) |
| │   |
| └───> Agent 3 (e.g., Billing Issues) |

Example Agent Transfer Hierarchy

We recommend using the `gpt-4o` or `gpt-4o-mini` models when using agent-agent transfers due to better tool calling.

**Purpose**: Transfer conversations between specialized AI agents based on user needs.

**Trigger conditions**: The LLM should call this tool when:

*   User request requires specialized knowledge or different agent capabilities
*   Current agent cannot adequately handle the query
*   Conversation flow indicates need for different agent type

**Parameters**:

*   `reason` (string, optional): The reason for the agent transfer
*   `agent_number` (integer, required): Zero-indexed number of the agent to transfer to (based on configured transfer rules)

**Function call format**:

|     |     |
| --- | --- |
| 1   | {   |
| 2   | "type": "function", |
| 3   | "function": { |
| 4   | "name": "transfer\_to\_agent", |
| 5   | "arguments": "{\\"reason\\": \\"User needs billing support\\", \\"agent\_number\\": 0}" |
| 6   | }   |
| 7   | }   |

**Implementation**: Define transfer rules mapping conditions to specific agent IDs. Configure which agents the current agent can transfer to. Agents are referenced by zero-indexed numbers in the transfer configuration.

Enabling agent transfer
-----------------------

Agent transfer is configured using the `transfer_to_agent` system tool.

[1](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/agent-transfer#add-the-transfer-tool)

### Add the transfer tool

Enable agent transfer by selecting the `transfer_to_agent` system tool in your agent’s configuration within the `Agent` tab. Choose “Transfer to AI Agent” when adding a tool.

![Add Transfer Tool](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F3e2756669b192395680d48a17c622493faee9fb61649f491bbe66820a91d46ef%2Fassets%2Fimages%2Fconversational-ai%2Ftransfertool.png&w=3840&q=75)

[2](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/agent-transfer#configure-tool-description-optional)

### Configure tool description (optional)

You can provide a custom description to guide the LLM on when to trigger a transfer. If left blank, a default description encompassing the defined transfer rules will be used.

![Transfer Tool Description](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F6b916200ef66cd0c6f5af7f6ba51b4b48eb1a266c7b10e863ae874c5a11452ae%2Fassets%2Fimages%2Fconversational-ai%2Ftransferconfig.png&w=3840&q=75)

[3](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/agent-transfer#define-transfer-rules)

### Define transfer rules

Configure the specific rules for transferring to other agents. For each rule, specify:

*   **Agent**: The target agent to transfer the conversation to.
*   **Condition**: A natural language description of the circumstances under which the transfer should occur (e.g., “User asks about billing details”, “User requests technical support for product X”).
*   **Delay before transfer (milliseconds)**: The minimum delay (in milliseconds) before the transfer occurs. Defaults to 0 for immediate transfer.
*   **Transfer Message**: An optional custom message to play during the transfer. If left blank, the transfer will occur silently.
*   **Enable First Message**: Whether the transferred agent should play its first message after the transfer. Defaults to off.

The LLM will use these conditions, along with the tool description, to decide when and to which agent (by number) to transfer.

![Transfer Rules Configuration](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Fb1b7e0f58ae757640af46630fde962a78ef4a164a380f974297bb643ccc29443%2Fassets%2Fimages%2Fconversational-ai%2Ftransferrule.png&w=3840&q=75)

Ensure that the user account creating the agent has at least viewer permissions for any target agents specified in the transfer rules.

API Implementation
------------------

You can configure the `transfer_to_agent` system tool when creating or updating an agent via the API.

PythonJavaScript

|     |     |
| --- | --- |
| 1   | from elevenlabs import ( |
| 2   | ConversationalConfig, |
| 3   | ElevenLabs, |
| 4   | AgentConfig, |
| 5   | PromptAgent, |
| 6   | PromptAgentInputToolsItem\_System, |
| 7   | SystemToolConfigInputParams\_TransferToAgent, |
| 8   | AgentTransfer |
| 9   | )   |
| 10  |     |
| 11  | \# Initialize the client |
| 12  | elevenlabs = ElevenLabs(api\_key="YOUR\_API\_KEY") |
| 13  |     |
| 14  | \# Define transfer rules with new options |
| 15  | transfer\_rules = \[ |\
| 16  | AgentTransfer( |\
| 17  | agent\_id="AGENT\_ID\_1", |\
| 18  | condition="When the user asks for billing support.", |\
| 19  | delay\_ms=1000,  # 1 second delay |\
| 20  | transfer\_message="I'm connecting you to our billing specialist.", |\
| 21  | enable\_transferred\_agent\_first\_message=True |\
| 22  | ),  |\
| 23  | AgentTransfer( |\
| 24  | agent\_id="AGENT\_ID\_2", |\
| 25  | condition="When the user requests advanced technical help.", |\
| 26  | delay\_ms=0,  # Immediate transfer |\
| 27  | transfer\_message=None,  # Silent transfer |\
| 28  | enable\_transferred\_agent\_first\_message=False |\
| 29  | )   |\
| 30  | \]  |
| 31  |     |
| 32  | \# Create the transfer tool configuration |
| 33  | transfer\_tool = PromptAgentInputToolsItem\_System( |
| 34  | type="system", |
| 35  | name="transfer\_to\_agent", |
| 36  | description="Transfer the user to a specialized agent based on their request.", # Optional custom description |
| 37  | params=SystemToolConfigInputParams\_TransferToAgent( |
| 38  | transfers=transfer\_rules |
| 39  | )   |
| 40  | )   |
| 41  |     |
| 42  | \# Create the agent configuration |
| 43  | conversation\_config = ConversationalConfig( |
| 44  | agent=AgentConfig( |
| 45  | prompt=PromptAgent( |
| 46  | prompt="You are a helpful assistant.", |
| 47  | first\_message="Hi, how can I help you today?", |
| 48  | tools=\[transfer\_tool\], |
| 49  | )   |
| 50  | )   |
| 51  | )   |
| 52  |     |
| 53  | \# Create the agent |
| 54  | response = elevenlabs.conversational\_ai.agents.create( |
| 55  | conversation\_config=conversation\_config |
| 56  | )   |
| 57  |     |
| 58  | print(response) |

## Document 19 — End call | ElevenLabs Documentation {#doc-19}
[https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/end-call](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/end-call)

The **End Call** tool is added to agents created in the ElevenLabs dashboard by default. For agents created via API or SDK, if you would like to enable the End Call tool, you must add it manually as a system tool in your agent configuration. [See API Implementation below](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/end-call#api-implementation)
 for details.

![End call](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Ff260bfd4e43f7fb0374ce12d2ed984efb946a8ca9ebaa1c2d4b3e6d5f52a7851%2Fassets%2Fimages%2Fconversational-ai%2Fend-call-tool.png&w=3840&q=75)

Overview
--------

The **End Call** tool allows your conversational agent to terminate a call with the user. This is a system tool that provides flexibility in how and when calls are ended.

Functionality
-------------

*   **Default behavior**: The tool can operate without any user-defined prompts, ending the call when the conversation naturally concludes.
*   **Custom prompts**: Users can specify conditions under which the call should end. For example:
    *   End the call if the user says “goodbye.”
    *   Conclude the call when a specific task is completed.

**Purpose**: Automatically terminate conversations when appropriate conditions are met.

**Trigger conditions**: The LLM should call this tool when:

*   The main task has been completed and user is satisfied
*   The conversation reached natural conclusion with mutual agreement
*   The user explicitly indicates they want to end the conversation

**Parameters**:

*   `reason` (string, required): The reason for ending the call
*   `message` (string, optional): A farewell message to send to the user before ending the call

**Function call format**:

|     |     |
| --- | --- |
| 1   | {   |
| 2   | "type": "function", |
| 3   | "function": { |
| 4   | "name": "end\_call", |
| 5   | "arguments": "{\\"reason\\": \\"Task completed successfully\\", \\"message\\": \\"Thank you for using our service. Have a great day!\\"}" |
| 6   | }   |
| 7   | }   |

**Implementation**: Configure as a system tool in your agent settings. The LLM will receive detailed instructions about when to call this function.

### API Implementation

When creating an agent via API, you can add the End Call tool to your agent configuration. It should be defined as a system tool:

PythonJavaScriptBash

|     |     |
| --- | --- |
| 1   | from elevenlabs import ( |
| 2   | ConversationalConfig, |
| 3   | ElevenLabs, |
| 4   | AgentConfig, |
| 5   | PromptAgent, |
| 6   | PromptAgentInputToolsItem\_System |
| 7   | )   |
| 8   |     |
| 9   | \# Initialize the client |
| 10  | elevenlabs = ElevenLabs(api\_key="YOUR\_API\_KEY") |
| 11  |     |
| 12  | \# Create the end call tool |
| 13  | end\_call\_tool = PromptAgentInputToolsItem\_System( |
| 14  | name="end\_call", |
| 15  | description=""  # Optional: Customize when the tool should be triggered |
| 16  | )   |
| 17  |     |
| 18  | \# Create the agent configuration |
| 19  | conversation\_config = ConversationalConfig( |
| 20  | agent=AgentConfig( |
| 21  | prompt=PromptAgent( |
| 22  | tools=\[end\_call\_tool\] |
| 23  | )   |
| 24  | )   |
| 25  | )   |
| 26  |     |
| 27  | \# Create the agent |
| 28  | response = elevenlabs.conversational\_ai.agents.create( |
| 29  | conversation\_config=conversation\_config |
| 30  | )   |

Leave the description blank to use the default end call prompt.

Example prompts
---------------

**Example 1: Basic End Call**

End the call when the user says goodbye, thank you, or indicates they have no more questions.

**Example 2: End Call with Custom Prompt**

End the call when the user says goodbye, thank you, or indicates they have no more questions. You can only end the call after all their questions have been answered. Please end the call only after confirming that the user doesn't need any additional assistance.

## Document 20 — Language detection | ElevenLabs Documentation {#doc-20}
[https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/language-detection](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/language-detection)

Overview
--------

The `language detection` system tool allows your ElevenLabs agent to switch its output language to any the agent supports. This system tool is not enabled automatically. Its description can be customized to accommodate your specific use case.

Where possible, we recommend enabling all languages for an agent and enabling the language detection system tool.

Our language detection tool triggers language switching in two cases, both based on the received audio’s detected language and content:

*   `detection` if a user speaks a different language than the current output language, a switch will be triggered
*   `content` if the user asks in the current language to change to a new language, a switch will be triggered

**Purpose**: Automatically switch to the user’s detected language during conversations.

**Trigger conditions**: The LLM should call this tool when:

*   User speaks in a different language than the current conversation language
*   User explicitly requests to switch languages
*   Multi-language support is needed for the conversation

**Parameters**:

*   `reason` (string, required): The reason for the language switch
*   `language` (string, required): The language code to switch to (must be in supported languages list)

**Function call format**:

|     |     |
| --- | --- |
| 1   | {   |
| 2   | "type": "function", |
| 3   | "function": { |
| 4   | "name": "language\_detection", |
| 5   | "arguments": "{\\"reason\\": \\"User requested Spanish\\", \\"language\\": \\"es\\"}" |
| 6   | }   |
| 7   | }   |

**Implementation**: Configure supported languages in agent settings and add the language detection system tool. The agent will automatically switch voice and responses to match detected languages.

Enabling language detection
---------------------------

[1](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/language-detection#configure-supported-languages)

### Configure supported languages

The languages that the agent can switch to must be defined in the `Agent` settings tab.

![Agent languages](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F633707d54276febd3baa054c4f41b186225b74f606307a46e8262607befc8381%2Fassets%2Fimages%2Fconversational-ai%2Fagent-languages.png&w=3840&q=75)

[2](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/language-detection#add-the-language-detection-tool)

### Add the language detection tool

Enable language detection by selecting the pre-configured system tool to your agent’s tools in the `Agent` tab. This is automatically available as an option when selecting `add tool`.

![System tool](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F3d17048ab2bc1a0547c49056abb98f624caf866927ff1714af27f898b06ab18f%2Fassets%2Fimages%2Fconversational-ai%2Flanguage-detection-preconfig.png&w=3840&q=75)

[3](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/language-detection#configure-tool-description)

### Configure tool description

Add a description that specifies when to call the tool

![Description](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F3bebc9fe07dba7121bce7793f3793858c710fc8ea1c221347604eb05929271df%2Fassets%2Fimages%2Fconversational-ai%2Flanguage_detection.png&w=3840&q=75)

### API Implementation

When creating an agent via API, you can add the `language detection` tool to your agent configuration. It should be defined as a system tool:

PythonJavaScriptBash

|     |     |
| --- | --- |
| 1   | from elevenlabs import ( |
| 2   | ConversationalConfig, |
| 3   | ElevenLabs, |
| 4   | AgentConfig, |
| 5   | PromptAgent, |
| 6   | PromptAgentInputToolsItem\_System, |
| 7   | LanguagePresetInput, |
| 8   | ConversationConfigClientOverrideInput, |
| 9   | AgentConfigOverride, |
| 10  | )   |
| 11  |     |
| 12  | \# Initialize the client |
| 13  | elevenlabs = ElevenLabs(api\_key="YOUR\_API\_KEY") |
| 14  |     |
| 15  | \# Create the language detection tool |
| 16  | language\_detection\_tool = PromptAgentInputToolsItem\_System( |
| 17  | name="language\_detection", |
| 18  | description=""  # Optional: Customize when the tool should be triggered |
| 19  | )   |
| 20  |     |
| 21  | \# Create language presets |
| 22  | language\_presets = { |
| 23  | "nl": LanguagePresetInput( |
| 24  | overrides=ConversationConfigClientOverrideInput( |
| 25  | agent=AgentConfigOverride( |
| 26  | prompt=None, |
| 27  | first\_message="Hoi, hoe gaat het met je?", |
| 28  | language=None |
| 29  | ),  |
| 30  | tts=None |
| 31  | ),  |
| 32  | first\_message\_translation=None |
| 33  | ),  |
| 34  | "fi": LanguagePresetInput( |
| 35  | overrides=ConversationConfigClientOverrideInput( |
| 36  | agent=AgentConfigOverride( |
| 37  | first\_message="Hei, kuinka voit?", |
| 38  | ),  |
| 39  | tts=None |
| 40  | ),  |
| 41  | ),  |
| 42  | "tr": LanguagePresetInput( |
| 43  | overrides=ConversationConfigClientOverrideInput( |
| 44  | agent=AgentConfigOverride( |
| 45  | prompt=None, |
| 46  | first\_message="Merhaba, nasılsın?", |
| 47  | language=None |
| 48  | ),  |
| 49  | tts=None |
| 50  | ),  |
| 51  | ),  |
| 52  | "ru": LanguagePresetInput( |
| 53  | overrides=ConversationConfigClientOverrideInput( |
| 54  | agent=AgentConfigOverride( |
| 55  | prompt=None, |
| 56  | first\_message="Привет, как ты?", |
| 57  | language=None |
| 58  | ),  |
| 59  | tts=None |
| 60  | ),  |
| 61  | ),  |
| 62  | "pt": LanguagePresetInput( |
| 63  | overrides=ConversationConfigClientOverrideInput( |
| 64  | agent=AgentConfigOverride( |
| 65  | prompt=None, |
| 66  | first\_message="Oi, como você está?", |
| 67  | language=None |
| 68  | ),  |
| 69  | tts=None |
| 70  | ),  |
| 71  | )   |
| 72  | }   |
| 73  |     |
| 74  | \# Create the agent configuration |
| 75  | conversation\_config = ConversationalConfig( |
| 76  | agent=AgentConfig( |
| 77  | prompt=PromptAgent( |
| 78  | tools=\[language\_detection\_tool\], |
| 79  | first\_message="Hi how are you?" |
| 80  | )   |
| 81  | ),  |
| 82  | language\_presets=language\_presets |
| 83  | )   |
| 84  |     |
| 85  | \# Create the agent |
| 86  | response = elevenlabs.conversational\_ai.agents.create( |
| 87  | conversation\_config=conversation\_config |
| 88  | )   |

Leave the description blank to use the default language detection prompt.

Ask AI

Assistant

Hi, I'm an AI assistant with access to documentation and other content.

Tip: You can toggle this pane with

⌘

+

/

![Agent languages](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F633707d54276febd3baa054c4f41b186225b74f606307a46e8262607befc8381%2Fassets%2Fimages%2Fconversational-ai%2Fagent-languages.png&w=3840&q=75)

![System tool](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F3d17048ab2bc1a0547c49056abb98f624caf866927ff1714af27f898b06ab18f%2Fassets%2Fimages%2Fconversational-ai%2Flanguage-detection-preconfig.png&w=3840&q=75)

![Description](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F3bebc9fe07dba7121bce7793f3793858c710fc8ea1c221347604eb05929271df%2Fassets%2Fimages%2Fconversational-ai%2Flanguage_detection.png&w=3840&q=75)

## Document 21 — Play keypad touch tone | ElevenLabs Documentation {#doc-21}
[https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/play-keypad-touch-tone](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/play-keypad-touch-tone)

Overview
--------

The keypad touch tone tool allows ElevenLabs agents to play DTMF (Dual-Tone Multi-Frequency) tones during phone calls; these are the tones that are played when you press numbers on your keypad. This enables agents to interact with automated phone systems, navigate voice menus, enter extensions, input PIN codes, and perform other touch-tone operations that would typically require a human caller to press keys on their phone keypad.

This system tool supports standard DTMF tones (0-9, \*, #) as well as pause commands for timing control. It works seamlessly with both Twilio and SIP trunking phone integrations, automatically generating the appropriate audio tones for the underlying telephony infrastructure.

Functionality
-------------

*   **Standard DTMF tones**: Supports all standard keypad characters (0-9, \*, #)
*   **Pause control**: Includes pause commands for precise timing (w = 0.5s, W = 1.0s)
*   **Multi-provider support**: Works with both Twilio and SIP trunking integrations

This system tool can be used to navigate phone menus, enter extensions and input codes. The LLM determines when and what tones to play based on conversation context.

The default tool description explains to the LLM powering the conversation that it has access to play these tones, and we recommend updating your agent’s system prompt to explain when the agent should call this tool.

**Parameters**:

*   `reason` (string, optional): The reason for playing the DTMF tones (e.g., “navigating to extension”, “entering PIN”)
*   `dtmf_tones` (string, required): The DTMF sequence to play. Valid characters: 0-9, \*, #, w (0.5s pause), W (1s pause)

**Function call format**:

|     |     |
| --- | --- |
| 1   | {   |
| 2   | "type": "function", |
| 3   | "function": { |
| 4   | "name": "play\_keypad\_touch\_tone", |
| 5   | "arguments": "{"reason": "Navigating to customer service", "dtmf\_tones": "2"}" |
| 6   | }   |
| 7   | }   |

Supported characters
--------------------

The tool supports the following DTMF characters and commands:

*   **Digits**: `0`, `1`, `2`, `3`, `4`, `5`, `6`, `7`, `8`, `9`
*   **Special tones**: `*` (star), `#` (pound/hash)
*   **Pause commands**:
    *   `w` - Short pause (0.5 seconds)
    *   `W` - Long pause (1.0 second)

API Implementation
------------------

You can configure the `play_keypad_touch_tone` system tool when creating or updating an agent via the API. This tool requires no additional configuration parameters beyond enabling it.

PythonJavaScript

|     |     |
| --- | --- |
| 1   | from elevenlabs import ( |
| 2   | ConversationalConfig, |
| 3   | ElevenLabs, |
| 4   | AgentConfig, |
| 5   | PromptAgent, |
| 6   | PromptAgentInputToolsItem\_System, |
| 7   | SystemToolConfigInputParams\_PlayKeypadTouchTone, |
| 8   | )   |
| 9   |     |
| 10  | \# Initialize the client |
| 11  | elevenlabs = ElevenLabs(api\_key="YOUR\_API\_KEY") |
| 12  |     |
| 13  | \# Create the keypad touch tone tool configuration |
| 14  | keypad\_tool = PromptAgentInputToolsItem\_System( |
| 15  | type="system", |
| 16  | name="play\_keypad\_touch\_tone", |
| 17  | description="Play DTMF tones to interact with automated phone systems.", # Optional custom description |
| 18  | params=SystemToolConfigInputParams\_PlayKeypadTouchTone( |
| 19  | system\_tool\_type="play\_keypad\_touch\_tone" |
| 20  | )   |
| 21  | )   |
| 22  |     |
| 23  | \# Create the agent configuration |
| 24  | conversation\_config = ConversationalConfig( |
| 25  | agent=AgentConfig( |
| 26  | prompt=PromptAgent( |
| 27  | prompt="You are a helpful assistant that can interact with phone systems.", |
| 28  | first\_message="Hi, I can help you navigate phone systems. How can I assist you today?", |
| 29  | tools=\[keypad\_tool\], |
| 30  | )   |
| 31  | )   |
| 32  | )   |
| 33  |     |
| 34  | \# Create the agent |
| 35  | response = elevenlabs.conversational\_ai.agents.create( |
| 36  | conversation\_config=conversation\_config |
| 37  | )   |

The tool only works during active phone calls powered by Twilio or SIP trunking. It will return an error if called outside of a phone conversation context.

## Document 22 — Skip turn | ElevenLabs Documentation {#doc-22}
[https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/skip-turn](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/skip-turn)

Overview
--------

The **Skip Turn** tool allows your conversational agent to explicitly pause and wait for the user to speak or act before continuing. This system tool is useful when the user indicates they need a moment, for example, by saying “Give me a second,” “Let me think,” or “One moment please.”

Functionality
-------------

*   **User-Initiated Pause**: The tool is designed to be invoked by the LLM when it detects that the user needs a brief pause without interruption.
*   **No Verbal Response**: After this tool is called, the assistant will not speak. It waits for the user to re-engage or for another turn-taking condition to be met.
*   **Seamless Conversation Flow**: It helps maintain a natural conversational rhythm by respecting the user’s need for a short break without ending the interaction or the agent speaking unnecessarily.

**Purpose**: Allow the agent to pause and wait for user input without speaking.

**Trigger conditions**: The LLM should call this tool when:

*   User indicates they need a moment (“Give me a second”, “Let me think”)
*   User requests pause in conversation flow
*   Agent detects user needs time to process information

**Parameters**:

*   `reason` (string, optional): Free-form reason explaining why the pause is needed

**Function call format**:

|     |     |
| --- | --- |
| 1   | {   |
| 2   | "type": "function", |
| 3   | "function": { |
| 4   | "name": "skip\_turn", |
| 5   | "arguments": "{\\"reason\\": \\"User requested time to think\\"}" |
| 6   | }   |
| 7   | }   |

**Implementation**: No additional configuration needed. The tool simply signals the agent to remain silent until the user speaks again.

### API implementation

When creating an agent via API, you can add the Skip Turn tool to your agent configuration. It should be defined as a system tool, with the name `skip_turn`.

PythonJavaScript

|     |     |
| --- | --- |
| 1   | from elevenlabs import ( |
| 2   | ConversationalConfig, |
| 3   | ElevenLabs, |
| 4   | AgentConfig, |
| 5   | PromptAgent, |
| 6   | PromptAgentInputToolsItem\_System |
| 7   | )   |
| 8   |     |
| 9   | \# Initialize the client |
| 10  | elevenlabs = ElevenLabs(api\_key="YOUR\_API\_KEY") |
| 11  |     |
| 12  | \# Create the skip turn tool |
| 13  | skip\_turn\_tool = PromptAgentInputToolsItem\_System( |
| 14  | name="skip\_turn", |
| 15  | description=""  # Optional: Customize when the tool should be triggered, or leave blank for default. |
| 16  | )   |
| 17  |     |
| 18  | \# Create the agent configuration |
| 19  | conversation\_config = ConversationalConfig( |
| 20  | agent=AgentConfig( |
| 21  | prompt=PromptAgent( |
| 22  | tools=\[skip\_turn\_tool\] |
| 23  | )   |
| 24  | )   |
| 25  | )   |
| 26  |     |
| 27  | \# Create the agent |
| 28  | response = elevenlabs.conversational\_ai.agents.create( |
| 29  | conversation\_config=conversation\_config |
| 30  | )   |

UI configuration
----------------

You can also configure the Skip Turn tool directly within the Agent’s UI, in the tools section.

[1](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/skip-turn#step-1-add-a-new-tool)

### Step 1: Add a new tool

Navigate to your agent’s configuration page. In the “Tools” section, click on “Add tool”, the `Skip Turn` option will already be available.

![Add Skip Turn Tool Option](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F8b95051da94ada3dae7e42121148ad4509b49413e73bd16351142372ca26a68d%2Fassets%2Fimages%2Fconversational-ai%2Fskip-turn-option.png&w=3840&q=75)

[2](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/skip-turn#step-2-configure-the-tool)

### Step 2: Configure the tool

You can optionally provide a description to customize when the LLM should trigger this tool, or leave it blank to use the default behavior.

![Configure Skip Turn Tool](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Fcbb419db630beb4feefc390ef3f3576f7a6dc2df098faec4fbd2d1d5e703364f%2Fassets%2Fimages%2Fconversational-ai%2Fskip-turn-config.png&w=3840&q=75)

[3](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/skip-turn#step-3-enable-the-tool)

### Step 3: Enable the tool

Once configured, the `Skip Turn` tool will appear in your agent’s list of enabled tools and the agent will be able to skip turns. .

![Skip Turn Tool Enabled](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Fbdb738dc85ff91107cafa5899aad116420aabe9fe794b648bc1f0751729ba5af%2Fassets%2Fimages%2Fconversational-ai%2Fskip-turn-enabled.png&w=3840&q=75)

## Document 23 — Transfer to human | ElevenLabs Documentation {#doc-23}
[https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/transfer-to-human](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/transfer-to-human)

Overview
--------

Human transfer allows a ElevenLabs agent to transfer the ongoing call to a specified phone number or SIP URI when certain conditions are met. This enables agents to hand off complex issues, specific requests, or situations requiring human intervention to a live operator.

This feature utilizes the `transfer_to_number` system tool which supports transfers via Twilio and SIP trunk numbers. When triggered, the agent can provide a message to the user while they wait and a separate message summarizing the situation for the human operator receiving the call.

The `transfer_to_number` system tool is only available for phone calls and is not available in the chat widget.

Transfer Types
--------------

The system supports two types of transfers:

*   **Conference Transfer**: Default behavior that calls the destination and adds the participant to a conference room, then removes the AI agent so only the caller and transferred participant remain.
*   **SIP REFER Transfer**: Uses the SIP REFER protocol to transfer calls directly to the destination. Works with both phone numbers and SIP URIs, but only available when using SIP protocol during the conversation and requires your SIP Trunk to allow transfer via SIP REFER.

**Purpose**: Seamlessly hand off conversations to human operators when AI assistance is insufficient.

**Trigger conditions**: The LLM should call this tool when:

*   Complex issues requiring human judgment
*   User explicitly requests human assistance
*   AI reaches limits of capability for the specific request
*   Escalation protocols are triggered

**Parameters**:

*   `reason` (string, optional): The reason for the transfer
*   `transfer_number` (string, required): The phone number to transfer to (must match configured numbers)
*   `client_message` (string, required): Message read to the client while waiting for transfer
*   `agent_message` (string, required): Message for the human operator receiving the call

**Function call format**:

|     |     |
| --- | --- |
| 1   | {   |
| 2   | "type": "function", |
| 3   | "function": { |
| 4   | "name": "transfer\_to\_number", |
| 5   | "arguments": "{\\"reason\\": \\"Complex billing issue\\", \\"transfer\_number\\": \\"+15551234567\\", \\"client\_message\\": \\"I'm transferring you to a billing specialist who can help with your account.\\", \\"agent\_message\\": \\"Customer has a complex billing dispute about order #12345 from last month.\\"}" |
| 6   | }   |
| 7   | }   |

**Implementation**: Configure transfer phone numbers and conditions. Define messages for both customer and receiving human operator. Works with both Twilio and SIP trunking.

Numbers that can be transferred to
----------------------------------

Human transfer supports transferring to external phone numbers using both [SIP trunking](https://elevenlabs.io/docs/agents-platform/phone-numbers/sip-trunking)
 and [Twilio phone numbers](https://elevenlabs.io/docs/agents-platform/phone-numbers/twilio-integration/native-integration)
.

Enabling human transfer
-----------------------

Human transfer is configured using the `transfer_to_number` system tool.

[1](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/transfer-to-human#add-the-transfer-tool)

### Add the transfer tool

Enable human transfer by selecting the `transfer_to_number` system tool in your agent’s configuration within the `Agent` tab. Choose “Transfer to Human” when adding a tool.

![Add Human Transfer Tool](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F1b0a0985dd32cab9532b38b80aaa90873b4076bb26e7a47ce2e12832dd78ea45%2Fassets%2Fimages%2Fconversational-ai%2Ftransfer_human.png&w=3840&q=75)

Select 'Transfer to Human' tool

[2](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/transfer-to-human#configure-tool-description-optional)

### Configure tool description (optional)

You can provide a custom description to guide the LLM on when to trigger a transfer. If left blank, a default description encompassing the defined transfer rules will be used.

![Human Transfer Tool Description](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F23b446030f915db9a10693153e44f95ab59c313e01df2b95291e40e12f2f4bde%2Fassets%2Fimages%2Fconversational-ai%2Ftransfer_human_tool.png&w=3840&q=75)

Configure transfer tool description

[3](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/transfer-to-human#define-transfer-rules)

### Define transfer rules

Configure the specific rules for transferring to phone numbers or SIP URIs. For each rule, specify:

*   **Transfer Type**: Choose between Conference (default) or SIP REFER transfer methods
*   **Number Type**: Select Phone for regular phone numbers or SIP URI for SIP addresses
*   **Phone Number/SIP URI**: The target destination in the appropriate format:
    *   Phone numbers: E.164 format (e.g., +12125551234)
    *   SIP URIs: SIP format (e.g., sip:[1234567890@example.com](mailto:1234567890@example.com)
        )
*   **Condition**: A natural language description of the circumstances under which the transfer should occur (e.g., “User explicitly requests to speak to a human”, “User needs to update sensitive account information”).

The LLM will use these conditions, along with the tool description, to decide when and to which destination to transfer.

**SIP REFER transfers** require SIP protocol during the conversation and your SIP Trunk must allow transfer via SIP REFER. Only SIP REFER supports transferring to a SIP URI.

![Human Transfer Rules Configuration](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F86e46148dc133fe68e6b517752d38bf493bef8347fee91a577e9f27600bafa5d%2Fassets%2Fimages%2Fconversational-ai%2Ftransfer_human_rule.png&w=3840&q=75)

Define transfer rules with phone number and condition

Ensure destinations are correctly formatted:

*   Phone numbers: E.164 format and associated with a properly configured account
*   SIP URIs: Valid SIP format (sip:user@domain or sips:user@domain)

API Implementation
------------------

You can configure the `transfer_to_number` system tool when creating or updating an agent via the API. The tool allows specifying messages for both the client (user being transferred) and the agent (human operator receiving the call).

PythonJavaScript

|     |     |
| --- | --- |
| 1   | from elevenlabs import ( |
| 2   | ConversationalConfig, |
| 3   | ElevenLabs, |
| 4   | AgentConfig, |
| 5   | PromptAgent, |
| 6   | PromptAgentInputToolsItem\_System, |
| 7   | SystemToolConfigInputParams\_TransferToNumber, |
| 8   | PhoneNumberTransfer, |
| 9   | )   |
| 10  |     |
| 11  | \# Initialize the client |
| 12  | elevenlabs = ElevenLabs(api\_key="YOUR\_API\_KEY") |
| 13  |     |
| 14  | \# Define transfer rules |
| 15  | transfer\_rules = \[ |\
| 16  | PhoneNumberTransfer( |\
| 17  | transfer\_destination={"type": "phone", "phone\_number": "+15551234567"}, |\
| 18  | condition="When the user asks for billing support.", |\
| 19  | transfer\_type="conference" |\
| 20  | ),  |\
| 21  | PhoneNumberTransfer( |\
| 22  | transfer\_destination={"type": "sip\_uri", "sip\_uri": "sip:support@example.com"}, |\
| 23  | condition="When the user requests to file a formal complaint.", |\
| 24  | transfer\_type="sip\_refer" |\
| 25  | )   |\
| 26  | \]  |
| 27  |     |
| 28  | \# Create the transfer tool configuration |
| 29  | transfer\_tool = PromptAgentInputToolsItem\_System( |
| 30  | type="system", |
| 31  | name="transfer\_to\_human", |
| 32  | description="Transfer the user to a specialized agent based on their request.", # Optional custom description |
| 33  | params=SystemToolConfigInputParams\_TransferToNumber( |
| 34  | transfers=transfer\_rules |
| 35  | )   |
| 36  | )   |
| 37  |     |
| 38  | \# Create the agent configuration |
| 39  | conversation\_config = ConversationalConfig( |
| 40  | agent=AgentConfig( |
| 41  | prompt=PromptAgent( |
| 42  | prompt="You are a helpful assistant.", |
| 43  | first\_message="Hi, how can I help you today?", |
| 44  | tools=\[transfer\_tool\], |
| 45  | )   |
| 46  | )   |
| 47  | )   |
| 48  |     |
| 49  | \# Create the agent |
| 50  | response = elevenlabs.conversational\_ai.agents.create( |
| 51  | conversation\_config=conversation\_config |
| 52  | )   |
| 53  |     |
| 54  | \# Note: When the LLM decides to call this tool, it needs to provide: |
| 55  | \# - transfer\_number: The phone number to transfer to (must match one defined in rules). |
| 56  | \# - client\_message: Message read to the user during transfer. |
| 57  | \# - agent\_message: Message read to the human operator receiving the call. |

## Document 24 — Voicemail detection | ElevenLabs Documentation {#doc-24}
[https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/voicemail-detection](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/voicemail-detection)

Overview
--------

The **Voicemail Detection** tool allows your ElevenLabs agent to automatically identify when a call has been answered by a voicemail system rather than a human. This system tool enables agents to handle automated voicemail scenarios gracefully by either leaving a pre-configured message or ending the call immediately.

Functionality
-------------

*   **Automatic Detection**: The LLM analyzes conversation patterns to identify voicemail systems based on automated greetings and prompts
*   **Configurable Response**: Choose to either leave a custom voicemail message or end the call immediately when voicemail is detected
*   **Call Termination**: After detection and optional message delivery, the call is automatically terminated
*   **Status Tracking**: Voicemail detection events are logged and can be viewed in conversation history and batch call results

**Parameters**:

*   `reason` (string, required): The reason for detecting voicemail (e.g., “automated greeting detected”, “no human response”)

**Function call format**:

|     |     |
| --- | --- |
| 1   | {   |
| 2   | "type": "function", |
| 3   | "function": { |
| 4   | "name": "voicemail\_detection", |
| 5   | "arguments": "{\\"reason\\": \\"Automated greeting detected with request to leave message\\"}" |
| 6   | }   |
| 7   | }   |

Configuration Options
---------------------

The voicemail detection tool can be configured with the following options:

![Voicemail detection configuration\
interface](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Ff2f9a87e27ce0d5f631d2d294163e8df39e6dee5b3a98aaba692589c66364550%2Fassets%2Fimages%2Fconversational-ai%2Fvoicemail_detection.png&w=3840&q=75)

*   **Voicemail Message**: You can configure an optional custom message to be played when voicemail is detected. This message supports [dynamic variables](https://elevenlabs.io/docs/agents-platform/customization/personalization/dynamic-variables)
    , allowing you to personalize voicemail messages with runtime values such as `{{user_name}}` or `{{appointment_time}}`

API Implementation
------------------

When creating an agent via API, you can add the Voicemail Detection tool to your agent configuration. It should be defined as a system tool:

PythonJavaScript

|     |     |
| --- | --- |
| 1   | from elevenlabs import ( |
| 2   | ConversationalConfig, |
| 3   | ElevenLabs, |
| 4   | AgentConfig, |
| 5   | PromptAgent, |
| 6   | PromptAgentInputToolsItem\_System |
| 7   | )   |
| 8   |     |
| 9   | \# Initialize the client |
| 10  | elevenlabs = ElevenLabs(api\_key="YOUR\_API\_KEY") |
| 11  |     |
| 12  | \# Create the voicemail detection tool |
| 13  | voicemail\_detection\_tool = PromptAgentInputToolsItem\_System( |
| 14  | name="voicemail\_detection", |
| 15  | description=""  # Optional: Customize when the tool should be triggered |
| 16  | )   |
| 17  |     |
| 18  | \# Create the agent configuration |
| 19  | conversation\_config = ConversationalConfig( |
| 20  | agent=AgentConfig( |
| 21  | prompt=PromptAgent( |
| 22  | tools=\[voicemail\_detection\_tool\] |
| 23  | )   |
| 24  | )   |
| 25  | )   |
| 26  |     |
| 27  | \# Create the agent |
| 28  | response = elevenlabs.conversational\_ai.agents.create( |
| 29  | conversation\_config=conversation\_config |
| 30  | )   |

## Document 25 — Tool Call Sounds | ElevenLabs Documentation {#doc-25}
[https://elevenlabs.io/docs/agents-platform/customization/tools/tool-configuration/tool-call-sounds](https://elevenlabs.io/docs/agents-platform/customization/tools/tool-configuration/tool-call-sounds)

Overview
--------

Tool call sounds provide ambient audio feedback during tool execution, creating a more natural and engaging conversation experience. When your agent executes a tool - such as fetching data from an API or processing a request - these sounds help fill moments of silence and indicate to users that the agent is actively working.

ElevenLabs Agents supports multiple built-in ambient sounds that you can configure at both the tool level and integration level, giving you fine-grained control over when and how sounds are played during your conversations.

Use Cases
---------

Tool call sounds are particularly effective in scenarios where:

*   **API calls take time to complete**: Play ambient music or typing sounds while fetching data from external services
*   **Long-running operations**: Provide audio feedback during database queries, complex calculations, or third-party integrations
*   **Natural conversation flow**: Fill gaps in the conversation to prevent awkward silences
*   **User expectations**: Signal to users that the agent is processing their request rather than experiencing technical issues

Tool call sounds are optional. If you prefer silent tool execution, simply leave the tool call sound setting as “None”.

Available Sounds
----------------

ElevenLabs provides the following ambient audio options:

| Sound Type | Description | Best For |
| --- | --- | --- |
| None | No sound during tool execution | Quick operations, silent workflows |
| Typing | Keyboard typing sound effect | Search queries, text processing |
| Elevator Music 1-4 | Light background music (upbeat) | Longer wait times, general use |

You can preview each sound in the dashboard by clicking the play button next to the dropdown when configuring tool call sounds.

Configuration
-------------

Tool call sounds can be configured in two places, with tool-level configuration taking precedence:

### Tool-Level Configuration

Configure sounds for individual tools in your agent’s tool settings:

[1](https://elevenlabs.io/docs/agents-platform/customization/tools/tool-configuration/tool-call-sounds#navigate-to-tool-configuration)

### Navigate to tool configuration

In the **Agent** section of your agent settings, select the tool you want to configure or create a new tool.

[2](https://elevenlabs.io/docs/agents-platform/customization/tools/tool-configuration/tool-call-sounds#configure-tool-call-sound)

### Configure tool call sound

Scroll to the **Tool Call Sound** section at the bottom of the tool configuration.

![tool call sound dropdown](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F093fd6c380e2d51f0b8dbe152377a35e1e4e0e422638519910016b9f1551233d%2Fassets%2Fimages%2Fagents%2Ftool-call-sounds-1.png&w=1920&q=75)

Select a sound from the dropdown menu:

*   **None**: No sound will play
*   **Typing**: Keyboard typing effect
*   **Elevator Music 1-4**: Various ambient background music options

[3](https://elevenlabs.io/docs/agents-platform/customization/tools/tool-configuration/tool-call-sounds#configure-sound-behavior)

### Configure sound behavior

If you’ve selected a sound (not “None”), you’ll see an additional **Sound Behavior** option:

![sound behavior dropdown](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F00c7f6ddfb1b0240a900c8d6814c8706e045500d92f6a6c263d4a7c962e24fbe%2Fassets%2Fimages%2Fagents%2Ftool-call-sounds-2.png&w=1920&q=75)

Choose when the sound should play:

*   **With pre-speech**: Sound plays only when the agent speaks before executing the tool
*   **Always play**: Sound plays during every tool execution, regardless of whether the agent speaks first

[4](https://elevenlabs.io/docs/agents-platform/customization/tools/tool-configuration/tool-call-sounds#save-your-configuration)

### Save your configuration

Click **Save** to apply your tool call sound settings.

### Integration-Level Configuration

For tools created through integrations (MCP servers, API integrations, etc.), you can set default tool call sounds at the integration level:

[1](https://elevenlabs.io/docs/agents-platform/customization/tools/tool-configuration/tool-call-sounds#navigate-to-integration-settings)

### Navigate to integration settings

Go to **Agent Settings > Integrations** and select your integration.

[2](https://elevenlabs.io/docs/agents-platform/customization/tools/tool-configuration/tool-call-sounds#configure-default-sound)

### Configure default sound

In the integration overview, locate the **Tool Call Sound** settings.

![tool call sound integrations dropdown](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F9c4c33501c814c3d124d76060853ee6eb8c59c8b4f0cc494d16267e9fe5cf56b%2Fassets%2Fimages%2Fagents%2Ftool-call-sounds-3.png&w=1920&q=75)

Select a default sound that will apply to all tools from this integration unless overridden at the tool level.

[3](https://elevenlabs.io/docs/agents-platform/customization/tools/tool-configuration/tool-call-sounds#override-at-tool-level-optional)

### Override at tool level (optional)

You can override the integration-level default for specific tools by configuring tool call sounds in the individual tool settings.

Sound Behavior Options
----------------------

The sound behavior setting determines when tool call sounds play during a conversation:

### With Pre-Speech (Auto)

**Default behavior**. The sound plays only when the agent speaks before using the tool.

**Example scenario:**

|     |
| --- |
| User: "What's the weather in Tokyo?" |
| Agent: "Let me check that for you..." \[typing sound plays while fetching weather data\] |
| Agent: "It's currently 72 degrees and sunny in Tokyo." |

**When to use:**

*   When you want sounds to play only after the agent has acknowledged the request
*   For a more natural conversational flow where silence after user speech would be awkward
*   When tool execution immediately follows agent speech

### Always Play

The sound plays during every tool execution, regardless of whether the agent speaks beforehand.

**Example scenario:**

|     |
| --- |
| User: "Check my order status" |
| \[typing sound plays immediately while fetching order data\] |
| Agent: "Your order #12345 shipped yesterday and will arrive tomorrow." |

**When to use:**

*   For operations that always take noticeable time
*   When you want consistent audio feedback for every tool call
*   For background operations that may not require agent acknowledgment

If you select “Always play”, the sound will play even when the agent doesn’t speak before using the tool. This can result in sounds playing immediately after the user finishes speaking, which may feel abrupt in some conversational contexts.

Best Practices
--------------

#### Match sounds to tool execution time

For quick operations (< 1 second), consider using “None” or the typing sound. For longer operations (> 3 seconds), elevator music options provide better user experience.

#### Use “With pre-speech” for acknowledged actions

When your agent explicitly acknowledges a request before executing it, use the “With pre-speech” behavior to create a natural conversation flow:

System prompt

|     |
| --- |
| When users request information, first acknowledge their request before using tools. |
| For example: "Let me check that for you..." then call the appropriate tool. |

#### Configure at integration level for consistency

If you have multiple tools from the same integration, configure tool call sounds at the integration level to ensure consistent audio feedback across all related tools.

#### Test with realistic latencies

Test your tool call sounds with realistic API latencies to ensure the audio feedback matches user expectations. Consider:

*   Network latency to your APIs
*   API processing time
*   Geographic distribution of your users

#### Balance with interruption handling

If your agent supports user interruptions, tool call sounds will be automatically stopped when the user interrupts. Ensure your conversation flow handles this gracefully.

Example Configurations
----------------------

### Search Agent

**Use case**: Agent that searches a knowledge base or database

**Configuration**:

*   Tool call sound: **Typing**
*   Sound behavior: **With pre-speech**
*   System prompt includes: _“When users ask questions, say ‘Let me search for that’ before calling the search tool.”_

### Customer Service Agent

**Use case**: Agent that checks order status, inventory, or customer records

**Configuration**:

*   Tool call sound: **Elevator Music 3**
*   Sound behavior: **Always play**
*   Rationale: Operations may take 2-5 seconds, and background music provides better experience than silence

### Multi-Step Workflow

**Use case**: Agent that performs multiple sequential tool calls

**Configuration**:

*   First tool: **Typing** with **With pre-speech**
*   Subsequent tools: **Elevator Music 1** with **Always play**
*   System prompt orchestrates: _“First acknowledge the request, then execute tools sequentially.”_

Programmatic Configuration
--------------------------

While tool call sounds are primarily configured through the dashboard, you can also set them programmatically when creating agents via the API:

|     |     |
| --- | --- |
| 1   | {   |
| 2   | "conversation\_config": { |
| 3   | "agent": { |
| 4   | "tools": \[ |\
| 5   | {   |\
| 6   | "type": "webhook", |\
| 7   | "name": "get\_weather", |\
| 8   | "description": "Gets current weather data", |\
| 9   | "url": "https://api.weather.com/data", |\
| 10  | "tool\_call\_sound": "typing", |\
| 11  | "tool\_call\_sound\_behavior": "auto" |\
| 12  | }   |\
| 13  | \]  |
| 14  | }   |
| 15  | }   |
| 16  | }   |

Refer to the [API Reference](https://elevenlabs.io/docs/api-reference/agents/create)
 for complete schema details.

Troubleshooting
---------------

### Sound not playing

**Possible causes:**

1.  **No pre-speech**: If using “With pre-speech” behavior, the agent must speak before the tool executes. Update your system prompt to include acknowledgment.
2.  **Quick execution**: If the tool completes in less than ~500ms, the sound may not have time to play noticeably.

Related Resources
-----------------

*   [Server Tools](https://elevenlabs.io/docs/agents-platform/customization/tools/server-tools)
     - Learn how to configure webhook tools
*   [Client Tools](https://elevenlabs.io/docs/agents-platform/customization/tools/client-tools)
     - Understand client-side tool execution
*   [System Tools](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools)
     - Explore built-in platform tools
*   [Best Practices](https://elevenlabs.io/docs/developers/guides/cookbooks/multi-context-web-socket)
     - General best practices for building conversational agents

## Document 26 — Voice customization | ElevenLabs Documentation {#doc-26}
[https://elevenlabs.io/docs/agents-platform/customization/voice](https://elevenlabs.io/docs/agents-platform/customization/voice)

Overview
--------

You can customize various aspects of your AI agent’s voice to create a more natural and engaging conversation experience. This includes controlling pronunciation, speaking speed, and language-specific voice settings.

Available customizations
------------------------

[Multi-voice support\
\
Enable your agent to switch between different voices for multi-character conversations, storytelling, and language tutoring.](https://elevenlabs.io/docs/agents-platform/customization/voice/multi-voice-support)
[](https://elevenlabs.io/docs/agents-platform/customization/voice/pronunciation-dictionary)

[](https://elevenlabs.io/docs/agents-platform/customization/voice/pronunciation-dictionary)

[Pronunciation dictionary](https://elevenlabs.io/docs/agents-platform/customization/voice/pronunciation-dictionary)

[](https://elevenlabs.io/docs/agents-platform/customization/voice/pronunciation-dictionary)

[Control how your agent pronounces specific words and phrases using](https://elevenlabs.io/docs/agents-platform/customization/voice/pronunciation-dictionary)
 [IPA](https://en.wikipedia.org/wiki/International_Phonetic_Alphabet)
 or [CMU](https://en.wikipedia.org/wiki/CMU_Pronouncing_Dictionary)
 notation.

[Speed control\
\
Adjust how quickly or slowly your agent speaks, with values ranging from 0.7x to 1.2x.](https://elevenlabs.io/docs/agents-platform/customization/voice/speed-control)
[Expressive mode\
\
Enable expressive tags like \[laughs\], \[whispers\], and \[slow\] for enhanced emotional delivery.](https://elevenlabs.io/docs/agents-platform/customization/voice/expressive-mode)
[Language-specific voices\
\
Configure different voices for each supported language to ensure natural pronunciation.](https://elevenlabs.io/docs/agents-platform/customization/voice/customization/language)

Best practices
--------------

###### Voice selection

Choose voices that match your target language and region for the most natural pronunciation. Consider testing multiple voices to find the best fit for your use case.

###### Speed optimization

Start with the default speed (1.0) and adjust based on your specific needs. Test different speeds with your content to find the optimal balance between clarity and natural flow.

###### Pronunciation dictionaries

Focus on terms specific to your business or use case that need consistent pronunciation and are not widely used in everyday conversation. Test pronunciations with your chosen voice and model combination.

Some voice customization features may be model-dependent. For example, phoneme-based pronunciation control is only available with the Turbo v2 model.

## Document 27 — ElevenLabs Agents  voice design guide | ElevenLabs Documentation {#doc-27}
[https://elevenlabs.io/docs/agents-platform/customization/voice/best-practices/conversational-voice-design](https://elevenlabs.io/docs/agents-platform/customization/voice/best-practices/conversational-voice-design)

Overview
--------

Selecting the right voice is crucial for creating an effective voice agent. The voice you choose should align with your agent’s personality, tone, and purpose.

Voice design
------------

If you need a voice that doesn’t exist in our library, [Voice Design](https://elevenlabs.io/docs/creative-platform/voices/voice-design)
 lets you create custom voices from text descriptions. Define characteristics like age, accent, tone, and pacing to generate voices tailored to your agent’s personality and use case.

Library voices
--------------

These voices offer a range of styles and characteristics that work well for different agent types:

*   `kdmDKE6EkgrWrrykO9Qt` - **Alexandra:** A super realistic, young female voice that likes to chat
*   `L0Dsvb3SLTyegXwtm47J` - **Archer:** Grounded and friendly young British male with charm
*   `g6xIsTj2HwM6VR4iXFCw` - **Jessica Anne Bogart:** Empathetic and expressive, great for wellness coaches
*   `OYTbf65OHHFELVut7v2H` - **Hope:** Bright and uplifting, perfect for positive interactions
*   `dj3G1R1ilKoFKhBnWOzG` - **Eryn:** Friendly and relatable, ideal for casual interactions
*   `HDA9tsk27wYi3uq0fPcK` - **Stuart:** Professional & friendly Aussie, ideal for technical assistance
*   `1SM7GgM6IMuvQlz2BwM3` - **Mark:** Relaxed and laid back, suitable for non chalant chats
*   `PT4nqlKZfc06VW1BuClj` - **Angela:** Raw and relatable, great listener and down to earth
*   `vBKc2FfBKJfcZNyEt1n6` - **Finn:** Tenor pitched, excellent for podcasts and light chats
*   `56AoDkrOh6qfVPDXZ7Pt` - **Cassidy:** Engaging and energetic, good for entertainment contexts
*   `NOpBlnGInO9m6vDvFkFC` - **Grandpa Spuds Oxley:** Distinctive character voice for unique agents

Voice settings
--------------

![Voice settings](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F007b0ec153f9210ce48a6e6ed3184c4d004476222a2be2b5bf6ab6afdecf4804%2Fassets%2Fimages%2Fconversational-ai%2Fvoice-parameters.png&w=3840&q=75)

Voice settings dramatically affect how your agent is perceived:

*   **Stability:** Lower values (0.30-0.50) create more emotional, dynamic delivery but may occasionally sound unstable. Higher values (0.60-0.85) produce more consistent but potentially monotonous output.
    
*   **Similarity:** Higher values will boost the overall clarity and consistency of the voice. Very high values may lead to sound distortions. Adjusting this value to find the right balance is recommended.
    
*   **Speed:** Most natural conversations occur at 0.9-1.1x speed. Depending on the voice, adjust slower for complex topics or faster for routine information.
    

Test your agent with different voice settings using the same prompt to find the optimal combination. Small adjustments can dramatically change the perceived personality of your agent.

## Document 28 — Language | ElevenLabs Documentation {#doc-28}
[https://elevenlabs.io/docs/agents-platform/customization/voice/customization/language](https://elevenlabs.io/docs/agents-platform/customization/voice/customization/language)

Overview
--------

This guide shows you how to configure your agent to speak multiple languages. You’ll learn to:

*   Configure your agent’s primary language
*   Add support for multiple languages
*   Set language-specific voices and first messages
*   Optimize voice selection for natural pronunciation
*   Enable automatic language switching

Guide
-----

[1](https://elevenlabs.io/docs/agents-platform/customization/voice/customization/language#default-agent-language)

### Default agent language

When you create a new agent, it’s configured with:

*   English as the primary language
*   Flash v2 model for fast, English-only responses
*   A default first message.

![](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Fc533094b96146b19a31fd712a02a4b0b6d63790aa2168698e7dc682291c04e0b%2Fassets%2Fimages%2Fconversational-ai%2Flanguage-overview.png&w=3840&q=75)

Additional languages switch the agent to use the v2.5 Multilingual model. English will always use the v2 model.

[2](https://elevenlabs.io/docs/agents-platform/customization/voice/customization/language#add-additional-languages)

### Add additional languages

First, navigate to your agent’s configuration page and locate the **Agent** tab.

1.  In the **Additional Languages** add an additional language (e.g. French)
2.  Review the first message, which is automatically translated using a Large Language Model (LLM). Customize it as needed for each additional language to ensure accuracy and cultural relevance.

![](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Fa5db1d23c7c1cc22b41f9093839cd76b078e6cbef69230714f8c0e58f3609b2f%2Fassets%2Fimages%2Fconversational-ai%2Flanguage-selection.png&w=3840&q=75)

Selecting the **All** option in the **Additional Languages** dropdown will configure the agent to support 31 languages. Collectively, these languages are spoken by approximately 90% of the world’s population.

[3](https://elevenlabs.io/docs/agents-platform/customization/voice/customization/language#configure-language-specific-voices)

### Configure language-specific voices

For optimal pronunciation, configure each additional language with a language-specific voice from our [Voice Library](https://elevenlabs.io/app/voice-library)
.

To find great voices for each language curated by the ElevenLabs team, visit the [language top picks](https://elevenlabs.io/app/voice-library/collections)
.

###### Language-specific voice settings

###### Voice library

![](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Fe1335d0fc8fe77691f00b928c9aeb5e050226a0d766dbab325ded52e97ed4399%2Fassets%2Fimages%2Fconversational-ai%2Flanguage-voice.png&w=3840&q=75)

[4](https://elevenlabs.io/docs/agents-platform/customization/voice/customization/language#enable-language-detection)

### Enable language detection

Add the [language detection tool](https://elevenlabs.io/docs/agents-platform/customization/tools/system-tools/language-detection)
 to your agent can automatically switch to the user’s preferred language.

[5](https://elevenlabs.io/docs/agents-platform/customization/voice/customization/language#starting-a-call)

### Starting a call

Now that the agent is configured to support additional languages, the widget will prompt the user for their preferred language before the conversation begins.

If using the SDK, the language can be set programmatically using conversation overrides. See the [Overrides](https://elevenlabs.io/docs/agents-platform/customization/personalization/overrides)
 guide for implementation details.

![](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F4252ccb78e8c03f864b3389ec641a296ec99b9e89ae7d7efe2de36ef6132a152%2Fassets%2Fimages%2Fconversational-ai%2Fwidget-language.png&w=3840&q=75)

Language selection is fixed for the duration of the call - users cannot switch languages mid-conversation.

### Internationalization

You can integrate the widget with your internationalization framework by dynamically setting the language and UI text attributes.

Widget

|     |     |
| --- | --- |
| 1   | <elevenlabs-convai |
| 2   | language="es" |
| 3   | action-text={i18n\["es"\]\["actionText"\]} |
| 4   | start-call-text={i18n\["es"\]\["startCall"\]} |
| 5   | end-call-text={i18n\["es"\]\["endCall"\]} |
| 6   | expand-text={i18n\["es"\]\["expand"\]} |
| 7   | listening-text={i18n\["es"\]\["listening"\]} |
| 8   | speaking-text={i18n\["es"\]\["speaking"\]} |
| 9   | \></elevenlabs-convai> |

Ensure the language codes match between your i18n framework and the agent’s supported languages.

Best practices
--------------

###### Voice selection

Select voices specifically trained in your target languages. This ensures:

*   Natural pronunciation
*   Appropriate regional accents
*   Better handling of language-specific nuances

###### First message customization

While automatic translations are provided, consider:

*   Reviewing translations for accuracy
*   Adapting greetings for cultural context
*   Adjusting formal/informal tone as needed

## Document 29 — Multi-voice support | ElevenLabs Documentation {#doc-29}
[https://elevenlabs.io/docs/agents-platform/customization/voice/multi-voice-support](https://elevenlabs.io/docs/agents-platform/customization/voice/multi-voice-support)

Overview
--------

Multi-voice support allows your ElevenLabs agent to dynamically switch between different ElevenLabs voices during a single conversation. This powerful feature enables:

*   **Multi-character storytelling**: Different voices for different characters in narratives
*   **Language tutoring**: Native speaker voices for different languages
*   **Emotional agents**: Voice changes based on emotional context
*   **Role-playing scenarios**: Distinct voices for different personas

![Multi-voice configuration interface](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Fa421eaed65575340ef34c1209a30d97ca2594a5fb48ee57b02c8349930191727%2Fassets%2Fimages%2Fconversational-ai%2Fsupported-voices.png&w=3840&q=75)

How it works
------------

When multi-voice support is enabled, your agent can use XML-style markup to switch between configured voices during text generation. The agent automatically returns to the default voice when no specific voice is specified.

Example voice switchingMulti-character dialogue

|     |     |
| --- | --- |
| 1   | The teacher said, <spanish>¡Hola estudiantes!</spanish> |
| 2   | Then the student replied, <student>Hello! How are you today?</student> |

Configuration
-------------

### Adding supported voices

Navigate to your agent settings and locate the **Multi-voice support** section under the `Voice` tab.

[1](https://elevenlabs.io/docs/agents-platform/customization/voice/multi-voice-support#add-a-new-voice)

### Add a new voice

Click **Add voice** to configure a new supported voice for your agent.

![Multi-voice configuration interface](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Fe7a6e86ed58abb54f75e884214e6bc056c85e85fc60176924fb2683e16250aa1%2Fassets%2Fimages%2Fconversational-ai%2Fadd-supported-voice.png&w=3840&q=75)

[2](https://elevenlabs.io/docs/agents-platform/customization/voice/multi-voice-support#configure-voice-properties)

### Configure voice properties

Set up the voice with the following details:

*   **Voice label**: Unique identifier (e.g., “Joe”, “Spanish”, “Happy”)
*   **Voice**: Select from your available ElevenLabs voices
*   **Model Family**: Choose Turbo, Flash, or Multilingual (optional)
*   **Language**: Override the default language for this voice (optional)
*   **Description**: When the agent should use this voice

[3](https://elevenlabs.io/docs/agents-platform/customization/voice/multi-voice-support#save-configuration)

### Save configuration

Click **Add voice** to save the configuration. The voice will be available for your agent to use immediately.

### Voice properties

###### Voice label

A unique identifier that the LLM uses to reference this voice. Choose descriptive labels like: - Character names: “Alice”, “Bob”, “Narrator” - Languages: “Spanish”, “French”, “German” - Emotions: “Happy”, “Sad”, “Excited” - Roles: “Teacher”, “Student”, “Guide”

###### Model family

Override the agent’s default model family for this specific voice: - **Flash**: Fastest eneration, optimized for real-time use - **Turbo**: Balanced speed and quality - **Multilingual**: Highest quality, best for non-English languages - **Same as agent**: Use agent’s default setting

###### Language override

Specify a different language for this voice, useful for: - Multilingual conversations - Language tutoring applications - Region-specific pronunciations

###### Description

Provide context for when the agent should use this voice. Examples:

*   “For any Spanish words or phrases”
*   “When the message content is joyful or excited”
*   “Whenever the character Joe is speaking”

Implementation
--------------

### XML markup syntax

Your agent uses XML-style tags to switch between voices:

|     |     |
| --- | --- |
| 1   | <VOICE\_LABEL>text to be spoken</VOICE\_LABEL> |

**Key points:**

*   Replace `VOICE_LABEL` with the exact label you configured
*   Text outside tags uses the default voice
*   Tags are case-sensitive
*   Nested tags are not supported

### System prompt integration

When you configure supported voices, the system automatically adds instructions to your agent’s prompt:

|     |
| --- |
| When a message should be spoken by a particular person, use markup: "<CHARACTER>message</CHARACTER>" where CHARACTER is the character label. |
|     |
| Available voices are as follows: |
| \- default: any text outside of the CHARACTER tags |
| \- Joe: Whenever Joe is speaking |
| \- Spanish: For any Spanish words or phrases |
| \- Narrator: For narrative descriptions |

### Example usage

###### Language tutoring

###### Storytelling

|     |
| --- |
| Teacher: Let's practice greetings. In Spanish, we say <Spanish>¡Hola! ¿Cómo estás?</Spanish> |
| Student: How do I respond? |
| Teacher: You can say <Spanish>¡Hola! Estoy bien, gracias.</Spanish> which means Hello! I'm fine, thank you. |

Best practices
--------------

###### Voice selection

*   Choose voices that clearly differentiate between characters or contexts
*   Test voice combinations to ensure they work well together
*   Consider the emotional tone and personality for each voice
*   Ensure voices match the language and accent when switching languages

###### Label naming

*   Use descriptive, intuitive labels that the LLM can understand
*   Keep labels short and memorable
*   Avoid special characters or spaces in labels

###### Performance optimization

*   Limit the number of supported voices to what you actually need
*   Use the same model family when possible to reduce switching overhead
*   Test with your expected conversation patterns
*   Monitor response times with multiple voice switches

###### Content guidelines

*   Provide clear descriptions for when each voice should be used
*   Test edge cases where voice switching might be unclear
*   Consider fallback behavior when voice labels are ambiguous
*   Ensure voice switches enhance rather than distract from the conversation

Limitations
-----------

*   Maximum of 10 supported voices per agent (including default)
*   Voice switching adds minimal latency during generation
*   XML tags must be properly formatted and closed
*   Voice labels are case-sensitive in markup
*   Nested voice tags are not supported

FAQ
---

###### What happens if I use an undefined voice label?

If the agent uses a voice label that hasn’t been configured, the text will be spoken using the default voice. The XML tags will be ignored.

###### Can I change voices mid-sentence?

Yes, you can switch voices within a single response. Each tagged section will use the specified voice, while untagged text uses the default voice.

###### Do voice switches affect conversation latency?

Voice switching adds minimal overhead. The first use of each voice in a conversation may have slightly higher latency as the voice is initialized.

###### Can I use the same voice with different labels?

Yes, you can configure multiple labels that use the same ElevenLabs voice but with different model families, languages, or contexts.

###### How do I train my agent to use voice switching effectively?

Provide clear examples in your system prompt and test thoroughly. You can include specific scenarios where voice switching should occur and examples of the XML markup format.

## Document 30 — Pronunciation dictionaries | ElevenLabs Documentation {#doc-30}
[https://elevenlabs.io/docs/agents-platform/customization/voice/pronunciation-dictionary](https://elevenlabs.io/docs/agents-platform/customization/voice/pronunciation-dictionary)

Overview
--------

Pronunciation dictionaries allow you to customize how your AI agent pronounces specific words or phrases. This is particularly useful for:

*   Correcting pronunciation of names, places, or technical terms
*   Ensuring consistent pronunciation across conversations
*   Customizing regional pronunciation variations

![Pronunciation dictionary settings under the Voice tab](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F79d07bdeff752e43943a4c2fa3a04fc7c7dc79bc247fc9a633654d4df4cd895e%2Fassets%2Fimages%2Fconversational-ai%2Fpd-convai.png&w=3840&q=75)

Configuration
-------------

You can find the pronunciation dictionary settings under the **Voice** tab in your agent’s configuration.

Phoneme tags only work with the `eleven_flash_v2` and `eleven_turbo_v2` models. When used with other models, the tags are silently skipped and the default pronunciation is used.

Phoneme tags (IPA or CMU) only work for English. For other languages, use alias tags instead to substitute spellings or phrases that produce the pronunciation you need.

Dictionary file format
----------------------

Pronunciation dictionaries use XML-based `.pls` files. Here’s an example structure:

|     |     |
| --- | --- |
| 1   | <?xml version="1.0" encoding="UTF-8"?> |
| 2   | <lexicon version="1.0" |
| 3   | xmlns="http://www.w3.org/2005/01/pronunciation-lexicon" |
| 4   | xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" |
| 5   | xsi:schemaLocation="http://www.w3.org/2005/01/pronunciation-lexicon |
| 6   | http://www.w3.org/TR/2007/CR-pronunciation-lexicon-20071212/pls.xsd" |
| 7   | alphabet="ipa" xml:lang="en-GB"> |
| 8   | <lexeme> |
| 9   | <grapheme>Apple</grapheme> |
| 10  | <phoneme>ˈæpl̩</phoneme> |
| 11  | </lexeme> |
| 12  | <lexeme> |
| 13  | <grapheme>UN</grapheme> |
| 14  | <alias>United Nations</alias> |
| 15  | </lexeme> |
| 16  | </lexicon> |

Supported formats
-----------------

We support two types of pronunciation notation:

1.  **IPA (International Phonetic Alphabet)**
    
    *   More precise control over pronunciation
    *   Requires knowledge of IPA symbols
    *   Example: “nginx” as `/ˈɛndʒɪnˈɛks/`
2.  **CMU (Carnegie Mellon University) Dictionary format**
    
    *   Simpler ASCII-based format
    *   More accessible for English pronunciations
    *   Example: “tomato” as “T AH M EY T OW”

You can use AI tools like Claude or ChatGPT to help generate IPA or CMU notations for specific words.

Best practices
--------------

1.  **Case sensitivity**: Create separate entries for capitalized and lowercase versions of words if needed
2.  **Testing**: Always test pronunciations with your chosen voice and model
3.  **Maintenance**: Keep your dictionary organized and documented
4.  **Scope**: Focus on words that are frequently mispronounced or critical to your use case

FAQ
---

###### Which models support phoneme-based pronunciation?

Phoneme tags are supported on `eleven_flash_v2` and `eleven_turbo_v2`. All other models skip the phoneme entry and fall back to their normal pronunciation. For non-English languages, rely on alias tags because phoneme tags only cover English pronunciations.

###### Can I use multiple dictionaries?

Yes, you can upload multiple dictionary files to handle different sets of pronunciations.

###### What happens if a word isn't in the dictionary?

The model will use its default pronunciation rules for any words not specified in the dictionary.

Additional resources
--------------------

*   [Professional Voice Cloning](https://elevenlabs.io/docs/creative-platform/voices/voice-cloning/professional-voice-cloning)
    
*   [Voice Design](https://elevenlabs.io/docs/creative-platform/voices/voice-design)
    
*   [Text to Speech API Reference](https://elevenlabs.io/docs/api-reference/text-to-speech/convert)

## Document 31 — Speed control | ElevenLabs Documentation {#doc-31}
[https://elevenlabs.io/docs/agents-platform/customization/voice/speed-control](https://elevenlabs.io/docs/agents-platform/customization/voice/speed-control)

Overview
--------

The speed control feature allows you to adjust how quickly or slowly your agent speaks. This can be useful for:

*   Making speech more accessible for different audiences
*   Matching specific use cases (e.g., slower for educational content)
*   Optimizing for different types of conversations

![Speed control settings under the Voice tab](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F4a1dfe1db33f9f03bb614b56173c7c00596f2d6c36b0f17ce43f151bd4d07f66%2Fassets%2Fimages%2Fconversational-ai%2Fspeed-control.png&w=3840&q=75)

Configuration
-------------

Speed is controlled through the [`speed` parameter](https://elevenlabs.io/docs/api-reference/agents/create#request.body.conversation_config.tts.speed)
 with the following specifications:

*   **Range**: 0.7 to 1.2
*   **Default**: 1.0
*   **Type**: Optional

How it works
------------

The speed parameter affects the pace of speech generation:

*   Values below 1.0 slow down the speech
*   Values above 1.0 speed up the speech
*   1.0 represents normal speaking speed

Extreme values near the minimum or maximum may affect the quality of the generated speech.

Best practices
--------------

*   Start with the default speed (1.0) and adjust based on user feedback
*   Test different speeds with your specific content
*   Consider your target audience when setting the speed
*   Monitor speech quality at extreme values

Values outside the 0.7-1.2 range are not supported.

## Document 32 — Agents Platform | ElevenLabs Documentation {#doc-32}
[https://elevenlabs.io/docs/agents-platform/overview](https://elevenlabs.io/docs/agents-platform/overview)

Agents accomplish tasks through natural dialogue - from quick requests to complex, open-ended workflows. ElevenLabs provides voice-rich, expressive models, developer tools for building multimodal agents, and tools to monitor and evaluate agent performance at scale.

[![](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Fb01da89ad7994300673d0932d321cd0f53fe727b6210e6c1f00e765e498f8722%2Fassets%2Fimages%2Fagents%2Fagents-overview-build.png&w=3840&q=75)\
\
### \
\
Configure\
\
Configure multimodal agents with our developer toolkit, dashboard, or visual workflow builder](https://elevenlabs.io/docs/agents-platform/build/overview)
[![](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F17a81505a62493491ead763b307b1e854825a0da67ab1a1d86b41b57ad87bc73%2Fassets%2Fimages%2Fagents%2Fagents-overview-integrate.png&w=3840&q=75)\
\
### \
\
Deploy\
\
Integrate multimodal agents across telephony systems, web, and mobile](https://elevenlabs.io/docs/agents-platform/integrate/overview)
[![](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F3790a000d203429c852cd4be74f2635ea41222f31b0edb53ae760c61e4a0f07d%2Fassets%2Fimages%2Fagents%2Fagents-overview-operate.png&w=3840&q=75)\
\
### \
\
Monitor\
\
Evaluate agent performance with built-in testing, evals, and analytics](https://elevenlabs.io/docs/agents-platform/operate/overview)

Platform capabilities
---------------------

From design to deployment to optimization, ElevenLabs provides everything you need to build agents at scale.

### Design and configure

| Goal | Guide | Description |
| --- | --- | --- |
| Create conversation workflows | [Workflows](https://elevenlabs.io/docs/agents-platform/customization/agent-workflows) | Build multi-step workflows with visual workflow builder |
| Write system prompts | [System prompt](https://elevenlabs.io/docs/agents-platform/best-practices/prompting-guide) | Learn best practices for crafting effective agent prompts |
| Select language model | [Models](https://elevenlabs.io/docs/agents-platform/customization/llm) | Choose from supported LLMs or bring your own custom model |
| Control conversation flow | [Conversation flow](https://elevenlabs.io/docs/agents-platform/customization/conversation-flow) | Configure turn-taking, interruptions, and timeout settings |
| Configure voice & language | [Voice & language](https://elevenlabs.io/docs/agents-platform/customization/voice) | Select from 5k+ voices across 31 languages with customization options |
| Add knowledge to agent | [Knowledge base](https://elevenlabs.io/docs/agents-platform/customization/knowledge-base) | Upload documents and enable RAG for grounded responses |
| Connect tools | [Tools](https://elevenlabs.io/docs/agents-platform/customization/tools) | Enable agents to call clients & APIs to perform actions |
| Personalize each conversation | [Personalization](https://elevenlabs.io/docs/agents-platform/customization/personalization) | Use dynamic variables and overrides for per-conversation customization |
| Secure agent access | [Authentication](https://elevenlabs.io/docs/agents-platform/customization/authentication) | Implement custom authentication for protected agent access |

### Connect and deploy

| Goal | Guide | Description |
| --- | --- | --- |
| Build with React components | [ElevenLabs UI](https://ui.elevenlabs.io/) | Pre-built components library for audio & agent apps (shadcn-based) |
| Embed widget in website | [Widget](https://elevenlabs.io/docs/agents-platform/customization/widget) | Add a customizable web widget to any website |
| Build React web apps | [React SDK](https://elevenlabs.io/docs/agents-platform/libraries/react) | Voice-enabled React hooks and components |
| Build iOS apps | [Swift SDK](https://elevenlabs.io/docs/agents-platform/libraries/swift) | Native iOS SDK for voice agents |
| Build Android apps | [Kotlin SDK](https://elevenlabs.io/docs/agents-platform/libraries/kotlin) | Native Android SDK for voice agents |
| Build React Native apps | [React Native SDK](https://elevenlabs.io/docs/agents-platform/libraries/react-native) | Cross-platform iOS and Android with React Native |
| Connect via SIP trunk | [SIP trunk](https://elevenlabs.io/docs/agents-platform/phone-numbers/sip-trunking) | Integrate with existing telephony infrastructure |
| Make batch outbound calls | [Batch calls](https://elevenlabs.io/docs/agents-platform/phone-numbers/batch-calls) | Trigger multiple calls programmatically |
| Use Twilio integration | [Twilio](https://elevenlabs.io/docs/agents-platform/phone-numbers/twilio-integration/native-integration) | Native Twilio integration for phone calls |
| Build custom integrations | [WebSocket API](https://elevenlabs.io/docs/agents-platform/libraries/web-sockets) | Low-level WebSocket protocol for custom implementations |
| Receive real-time events | [Events](https://elevenlabs.io/docs/agents-platform/customization/events) | Subscribe to conversation events and updates |

### Monitor and optimize

| Goal | Guide | Description |
| --- | --- | --- |
| Test agent behavior | [Testing](https://elevenlabs.io/docs/agents-platform/customization/agent-testing) | Create and run automated tests for your agents |
| Analyze conversation quality | [Conversation analysis](https://elevenlabs.io/docs/agents-platform/customization/agent-analysis) | Extract insights and evaluate conversation outcomes |
| Track metrics & analytics | [Analytics](https://elevenlabs.io/docs/agents-platform/dashboard) | Monitor performance metrics and conversation history |
| Configure data retention | [Privacy](https://elevenlabs.io/docs/agents-platform/customization/privacy) | Set retention policies for conversations and audio |
| Reduce LLM costs | [Cost optimization](https://elevenlabs.io/docs/agents-platform/customization/llm/optimizing-costs) | Monitor and optimize language model expenses |

Architecture
------------

The Agents Platform coordinates 4 core components:

1.  A fine-tuned Speech to Text (ASR) model for speech recognition
2.  Your choice of language model or [custom](https://elevenlabs.io/docs/agents-platform/customization/llm/custom-llm)
     LLM
3.  A low-latency Text to Speech (TTS) model across 5k+ voices and 31 languages
4.  A proprietary turn-taking model that handles conversation timing

[Quickstart\
\
Build your first agent in 5 minutes](https://elevenlabs.io/docs/agents-platform/quickstart)

## Document 33 — Quickstart | ElevenLabs Documentation {#doc-33}
[https://elevenlabs.io/docs/agents-platform/quickstart](https://elevenlabs.io/docs/agents-platform/quickstart)

In this guide, you’ll learn how to create your first conversational agent. This will serve as a foundation for building conversational workflows tailored to your business use cases.

Getting started
---------------

ElevenLabs Agents are managed either through the [Agents Platform dashboard](https://elevenlabs.io/app/agents)
, the [ElevenLabs API](https://elevenlabs.io/docs/api-reference/introduction)
 or the [Agents CLI](https://elevenlabs.io/docs/agents-platform/operate/cli)
.

![ElevenLabs Agents](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F62ad20de44cee44a439b138d56764c801c4eb939cf3b0f167f594d5c6a6895fb%2Fassets%2Fimages%2Fconversational-ai%2Fwidget.png&w=1920&q=75)

The assistant at the bottom right corner of this page is an example of an ElevenLabs agent, capable of answering questions about ElevenLabs, navigating pages & taking you to external resources.

Creating your first agent
-------------------------

In this quickstart guide we’ll start by creating an agent via the API or the web dashboard. Next we’ll test the agent, either by embedding it in your website or via the ElevenLabs dashboard.

###### Build an agent via the web dashboard

###### Build an agent via the CLI

###### Build an agent via the API

In this guide, we’ll create a conversational support assistant capable of answering questions about your product, documentation, or service. This assistant can be embedded into your website or app to provide real-time support to your customers.

![ElevenLabs Agents](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F62ad20de44cee44a439b138d56764c801c4eb939cf3b0f167f594d5c6a6895fb%2Fassets%2Fimages%2Fconversational-ai%2Fwidget.png&w=1920&q=75)

The assistant at the bottom right corner of this page is capable of answering questions about ElevenLabs, navigating pages & taking you to external resources.

[1](https://elevenlabs.io/docs/agents-platform/quickstart#sign-in-to-elevenlabs)

### Sign in to ElevenLabs

Go to [elevenlabs.io](https://elevenlabs.io/app/sign-up)
 and sign in to or create your account.

[2](https://elevenlabs.io/docs/agents-platform/quickstart#create-a-new-assistant)

### Create a new assistant

In the **ElevenLabs Dashboard**, create a new assistant by entering a name and selecting the `Blank template` option.

![Dashboard](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/61b0e1df291ea587a8ff3e116579a47f66d48c74b24f44cfcd4804a568eb0a9d/assets/images/conversational-ai/assistant-create-flow.gif)

Creating a new assistant

[3](https://elevenlabs.io/docs/agents-platform/quickstart#configure-the-assistant-behavior)

### Configure the assistant behavior

Go to the **Agent** tab to configure the assistant’s behavior. Set the following:

[1](https://elevenlabs.io/docs/agents-platform/quickstart#first-message)

### First message

This is the first message the assistant will speak out loud when a user starts a conversation.

First message

Hi, this is Alexis from <company name> support. How can I help you today?

[2](https://elevenlabs.io/docs/agents-platform/quickstart#system-prompt)

### System prompt

This prompt guides the assistant’s behavior, tasks, and personality.

Customize the following example with your company details:

System prompt

|     |
| --- |
| You are a friendly and efficient virtual assistant for \[Your Company Name\]. Your role is to assist customers by answering questions about the company's products, services, and documentation. You should use the provided knowledge base to offer accurate and helpful responses. |
|     |
| Tasks: |
| \- Answer Questions: Provide clear and concise answers based on the available information. |
| \- Clarify Unclear Requests: Politely ask for more details if the customer's question is not clear. |
|     |
| Guidelines: |
| \- Maintain a friendly and professional tone throughout the conversation. |
| \- Be patient and attentive to the customer's needs. |
| \- If unsure about any information, politely ask the customer to repeat or clarify. |
| \- Avoid discussing topics unrelated to the company's products or services. |
| \- Aim to provide concise answers. Limit responses to a couple of sentences and let the user guide you on where to provide more detail. |

[4](https://elevenlabs.io/docs/agents-platform/quickstart#add-a-knowledge-base)

### Add a knowledge base

Go to the **Knowledge Base** section to provide your assistant with context about your business.

This is where you can upload relevant documents & links to external resources:

*   Include documentation, FAQs, and other resources to help the assistant respond to customer inquiries.
*   Keep the knowledge base up-to-date to ensure the assistant provides accurate and current information.

Next we’ll configure the voice for your assistant.

[1](https://elevenlabs.io/docs/agents-platform/quickstart#select-a-voice)

### Select a voice

In the **Voice** tab, choose a voice that best matches your assistant from the [voice library](https://elevenlabs.io/voice-library)
:

![Voice settings](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F14aad328f468daafa9ec95d3b7a55489a8cb1a34869368020e05b207c9b0360a%2Fassets%2Fimages%2Fconversational-ai%2Fvoice-settings.jpg&w=3840&q=75)

Using higher quality voices, models, and LLMs may increase response time. For an optimal customer experience, balance quality and latency based on your assistant’s expected use case.

[2](https://elevenlabs.io/docs/agents-platform/quickstart#testing-your-assistant)

### Testing your assistant

Press the **Test AI agent** button and try conversing with your assistant.

Configure evaluation criteria and data collection to analyze conversations and improve your assistant’s performance.

[1](https://elevenlabs.io/docs/agents-platform/quickstart#configure-evaluation-criteria)

### Configure evaluation criteria

Navigate to the **Analysis** tab in your assistant’s settings to define custom criteria for evaluating conversations.

![Analysis settings](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Ffee8bc444d5436c71eae3829b9ec8d5cdb6a57c4d4efe6483d7bfed2b066e438%2Fassets%2Fimages%2Fconversational-ai%2Fanalysis-settings.png&w=3840&q=75)

Every conversation transcript is passed to the LLM to verify if specific goals were met. Results will either be `success`, `failure`, or `unknown`, along with a rationale explaining the chosen result.

Let’s add an evaluation criteria with the name `solved_user_inquiry`:

Prompt

|     |
| --- |
| The assistant was able to answer all of the queries or redirect them to a relevant support channel. |
|     |
| Success Criteria: |
| \- All user queries were answered satisfactorily. |
| \- The user was redirected to a relevant support channel if needed. |

[2](https://elevenlabs.io/docs/agents-platform/quickstart#configure-data-collection)

### Configure data collection

In the **Data Collection** section, configure details to be extracted from each conversation.

Click **Add item** and configure the following:

1.  **Data type:** Select “string”
2.  **Identifier:** Enter a unique identifier for this data point: `user_question`
3.  **Description:** Provide detailed instructions for the LLM about how to extract the specific data from the transcript:

Prompt

Extract the user's questions & inquiries from the conversation.

Test your assistant by posing as a customer. Ask questions, evaluate its responses, and tweak the prompts until you’re happy with how it performs.

[3](https://elevenlabs.io/docs/agents-platform/quickstart#view-conversation-history)

### View conversation history

View evaluation results and collected data for each conversation in the **Call history** tab.

![Conversation history](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Ffd517a080ba2c0d678c1ce34ea79afbd0dad764a19da43ca045a1c29b693ee48%2Fassets%2Fimages%2Fconversational-ai%2Ftranscript.jpg&w=3840&q=75)

Regularly review conversation history to identify common issues and patterns.

The newly created agent can be tested in a variety of ways, but the quickest way is to use the [ElevenLabs dashboard](https://elevenlabs.io/app/agents)
.

The web dashboard uses our [React SDK](https://elevenlabs.io/docs/agents-platform/libraries/react)
 under the hood to handle real-time conversations.

If instead you want to quickly test the agent in your own website, you can use the Agent widget. Simply paste the following HTML snippet into your website, taking care to replace `agent-id` with the ID of your agent.

|     |     |
| --- | --- |
| 1   | <elevenlabs-convai agent-id="agent-id"></elevenlabs-convai> |
| 2   | <script src="https://unpkg.com/@elevenlabs/convai-widget-embed" async type="text/javascript"></script> |

Next steps
----------

As a follow up to this quickstart guide, you can make your agent more effective by integrating:

*   [Knowledge bases](https://elevenlabs.io/docs/agents-platform/customization/knowledge-base)
     to equip it with domain-specific information.
*   [Tools](https://elevenlabs.io/docs/agents-platform/customization/tools)
     to allow it to perform tasks on your behalf.
*   [Authentication](https://elevenlabs.io/docs/agents-platform/customization/authentication)
     to restrict access to certain conversations.
*   [Success evaluation](https://elevenlabs.io/docs/agents-platform/customization/agent-analysis/success-evaluation)
     to analyze conversations and improve its performance.
*   [Data collection](https://elevenlabs.io/docs/agents-platform/customization/agent-analysis/data-collection)
     to collect data about conversations and improve its performance.
*   [Conversation retention](https://elevenlabs.io/docs/agents-platform/customization/privacy/retention)
     to view conversation history and improve its performance.

## Document 34 — Account | ElevenLabs Documentation {#doc-34}
[https://elevenlabs.io/docs/overview/administration/account](https://elevenlabs.io/docs/overview/administration/account)

To begin using ElevenLabs, you’ll need to create an account. Follow these steps:

*   **Sign Up**: Visit the [ElevenLabs website](https://elevenlabs.io/app/sign-up)
     and click on the ‘Get started free’ button. You can register using your email or through one of the OAuth providers.
*   **Verify Email**: Check your email for a verification link from ElevenLabs. Click the link to verify your account.
*   **Initial Setup**: After verification, you’ll be directed to the Speech Synthesis page where you can start generating audio from text.

**Exercise**: Try out an example to get started or type something, select a voice and click generate!

![Account creation exercise](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F7098296148ad1263133ad9506a9e7de7e8a2d40c657222080963cb914a8780cb%2Fassets%2Fimages%2Fproduct-guides%2Fadministration%2Faccount-creation.png&w=3840&q=75)

You can sign up with traditional email and password or using popular OAuth providers like Google, Facebook, and GitHub.

If you choose to sign up with your email, you will be asked to verify your email address before you can start using the service. Once you have verified your email, you will be taken to the Speech Synthesis page, where you can immediately start using the service. Simply type anything into the box and press “generate” to convert the text into voiceover narration. Please note that each time you press “generate” anywhere on the website, it will deduct credits from your quota.

If you sign up using Google OAuth, your account will be intrinsically linked to your Google account, meaning you will not be able to change your email address, as it will always be linked to your Google email.

## Document 35 — Billing | ElevenLabs Documentation {#doc-35}
[https://elevenlabs.io/docs/overview/administration/billing](https://elevenlabs.io/docs/overview/administration/billing)

[Pricing\
\
View the pricing page](https://elevenlabs.io/pricing)
[Subscription details\
\
View your subscription details](https://elevenlabs.io/app/subscription)

When signing up, you will be automatically assigned to the free tier. To view your subscription, click on your profile icon in the top right corner and select [“Subscription”](https://elevenlabs.io/app/subscription)
. You can read more about the different plans [here](https://elevenlabs.io/pricing)
. At the bottom of the page, you will find a comparison table to understand the differences between the various plans.

We offer five public plans: Free, Starter, Creator, Pro, Scale, and Business. In addition, we also offer an Enterprise option that’s specifically tailored to the unique needs and usage of large organizations.

You can see details of all our plans on the subscription page. This includes information about the total monthly credit quota, the number of custom voices you can have saved simultaneously, and the quality of audio produced.

Cloning is only available on the Starter tier and above. The free plan offers three custom voices that you can create using our [Voice Design tool](https://elevenlabs.io/docs/creative-platform/voices/voice-design)
, or you can add voices from the [Voice Library](https://elevenlabs.io/docs/creative-platform/voices/voice-library)
 if they are not limited to the paid tiers.

You can upgrade your subscription at any time, and any unused quota from your previous plan will roll over to the new one. As long as you don’t cancel or downgrade, unused credits at the end of the month will carry over to the next month, up to a maximum of two months’ worth of credits. For more information, please visit our Help Center articles:

*   [“How does credit rollover work?”](https://help.elevenlabs.io/hc/en-us/articles/27561768104081-How-does-credit-rollover-work)
    
*   [“What happens to my subscription and quota at the end of the month?”](https://help.elevenlabs.io/hc/en-us/articles/13514114771857-What-happens-to-my-subscription-and-quota-at-the-end-of-the-month)
    

From the [subscription page](https://elevenlabs.io/app/subscription)
, you can also downgrade your subscription at any point in time if you would like. When downgrading, it won’t take effect until the current cycle ends, ensuring that you won’t lose any of the monthly quota before your month is up.

When generating content on our paid plans, you get commercial rights to use that content. If you are on the free plan, you can use the content non-commercially with attribution. Read more about the license in our [Terms of Service](https://elevenlabs.io/terms-of-use)
 and in our Help Center [here](https://help.elevenlabs.io/hc/en-us/articles/13313564601361-Can-I-publish-the-content-I-generate-on-the-platform)
.

For more information on payment methods, please refer to the [Help Center](https://help.elevenlabs.io/)
.

## Document 36 — Consolidated billing | ElevenLabs Documentation {#doc-36}
[https://elevenlabs.io/docs/overview/administration/consolidated-billing](https://elevenlabs.io/docs/overview/administration/consolidated-billing)

Consolidated billing is an Enterprise feature that allows you to link multiple workspaces under a single billing account.

Overview
--------

Consolidated billing enables you to manage multiple workspaces across different environments while maintaining a single billing account. This feature is particularly useful for organizations that need to operate in multiple regions or maintain separate workspaces for different teams while keeping billing centralized.

With consolidated billing, you have:

*   **Unified billing** – Receive a single invoice for all linked workspaces.
*   **Shared credit pools** – All workspaces share the same credit allocation.
*   **Cross-environment support** – Link workspaces from isolated environments (e.g., EU, India) to the US billing workspace.
*   **Independent management** – Each workspace maintains its own members, SSO configurations, and settings.

How it works
------------

Consolidated billing creates a relationship between workspaces where one workspace (the “billing workspace”) receives usage reports from other workspaces (the “reporting workspaces”). All usage is then billed through the billing workspace.

### Billing workspace

The billing workspace must be located in the US environment (`elevenlabs.io`). This workspace:

*   Receives usage reports from all linked workspaces.
*   Issues a single monthly invoice.
*   Shows general usage coming from each reporting workspace.

### Reporting workspaces

Reporting workspaces can be located on elevenlabs.io or in an isolated environment. These workspaces:

*   Report their usage to the billing workspace.
*   Maintain their own members and configurations.
*   Show, as usual, granular usage analytics for that workspace.

Within the same region, users cannot be members of multiple workspaces. This limitation only applies within the same environment.

Setup process
-------------

Consolidated billing is an Enterprise feature that requires configuration by our team. To enable consolidated billing for your organization, contact your dedicated Customer Success Manager.

Usage tracking
--------------

The billing workspace will be able to see the usage of all linked workspaces.

![Consolidated billing reporting view](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F1b4a2896939345df00263961ea080724f268655a4b258c6988b7344cb3c6ebb0%2Fassets%2Fimages%2Fproduct-guides%2Fadministration%2Fconsolidated-billing-reporting.png&w=3840&q=75)

The reporting workspace will only be able to see analytics for its own usage. However, the total credits left shown in the sidebar will be the sum of all linked workspaces.

FAQ
---

###### Can I set credit limits for each workspace?

No, all workspaces share the same credit pool. However, you can closely track the usage of each workspace.

###### Can I have different subscription tiers for different workspaces?

No, all workspaces must share the same subscription. The billing workspace determines the subscription level for all linked workspaces.

###### Can I unlink a workspace from consolidated billing?

Yes, you can disable consolidated billing on any reporting workspace. This will require setting up a new subscription for that workspace or removing that workspace entirely. To do so, get in touch with your dedicated Customer Success Manager.

###### Can both workspaces be located on elevenlabs.io?

Yes, both workspaces can be located on elevenlabs.io - this is useful if you want to have multiple segregated teams. Sharing resources between workspaces is not possible so consider using permissions with [user groups](https://elevenlabs.io/docs/overview/administration/workspaces/user-groups)
 before enabling consolidated billing.

## Document 37 — Data residency | ElevenLabs Documentation {#doc-37}
[https://elevenlabs.io/docs/overview/administration/data-residency](https://elevenlabs.io/docs/overview/administration/data-residency)

Data residency is an Enterprise feature. For details on enabling this for your organization, please see the “Getting Access” section below.

Overview
--------

ElevenLabs offers “data residency” through isolated environments in certain jurisdictions, allowing customers to limit data storage to those locations. As a standard, ElevenLabs’ customer data is hosted/stored in the U.S., however ElevenLabs has released additional storage locations in the EU and India.

Depending on the customer’s location, isolated environments in a particular region may also provide the benefit of reduced latency.

Data residency in isolated environments
---------------------------------------

ElevenLabs offers data residency in certain jurisdictions to allow customers to choose where their data is stored. While storage will take place in the selected location, processing may nevertheless occur outside of the selected location, including by ElevenLabs’ international affiliates and subprocessors, for support purposes, and for content moderation purposes. This detail is captured within ElevenLabs’ Data Processing Agreement.

In certain locations, configurations may be available to limit processing to the selected residency location. For example, with respect to EU residency, users may restrict processing to the EU by using Zero Retention Mode and the API. In such case, content submitted to the Service will not be processed outside of the EU, provided the use of certain optional integrations (ex. Custom LLMs or post-call webhooks that require out-of-region processing) may result in processing outside of such jurisdiction.

Existing core compliance features
---------------------------------

Isolated environments complement ElevenLabs’ existing suite of security and compliance measures designed to safeguard customer data:

**GDPR Compliance**: Our platform and practices are designed to align with applicable GDPR requirements, including measures designed to ensure lawful data processing, adherence to data subject rights, and the implementation of appropriate security measures as required by GDPR.

**SOC2 Certification**: ElevenLabs maintains SOC2 certification, demonstrating our commitment to high standards for security, availability and confidentiality.

**Zero Retention Mode (Optional)**: Customers can enable Zero Retention Mode, ensuring that sensitive content and data processed by our models are not retained on ElevenLabs servers. This is a powerful feature for minimizing data footprint.

**End-to-End Encryption**: Data transmitted to and from ElevenLabs models is protected by end-to-end encryption, securing it in transit.

**HIPAA Compliance**: For qualifying healthcare enterprises, ElevenLabs offers Business Associate Agreements (BAAs), which offer additional protections in relation to its HIPAA-Eligible Services.

Developer considerations
------------------------

Isolated environments are completely separate ElevenLabs workspaces, available via a different address on the web. As such, you will need to get access to this feature first to be able to sign in to an isolated environment with data residency.

### EU

*   **Web**: [https://eu.residency.elevenlabs.io](https://eu.residency.elevenlabs.io/)
    
*   **API**: `https://api.eu.residency.elevenlabs.io`
*   **WebSockets**: `wss://api.eu.residency.elevenlabs.io`

### India

*   **Web**: [https://in.residency.elevenlabs.io](https://in.residency.elevenlabs.io/)
    
*   **API**: `https://api.in.residency.elevenlabs.io`
*   **WebSockets**: `wss://api.in.residency.elevenlabs.io`

Your account on the isolated environment will be separate to the one on elevenlabs.io, and your workspace will be blank. This means that when using an isolated environment via API, you will need to hit a different API URL with a different API key.

### SDK configuration

When using ElevenLabs SDKs, you can specify the environment to connect to an isolated region. Below are examples for each SDK.

###### Python

###### TypeScript

###### JavaScript (client)

###### React

|     |     |
| --- | --- |
| 1   | from elevenlabs import ElevenLabs, ElevenLabsEnvironment |
| 2   |     |
| 3   | \# For EU data residency |
| 4   | client = ElevenLabs( |
| 5   | api\_key="your-api-key", |
| 6   | environment=ElevenLabsEnvironment.PRODUCTION\_EU |
| 7   | )   |
| 8   |     |
| 9   | \# For India data residency |
| 10  | client = ElevenLabs( |
| 11  | api\_key="your-api-key", |
| 12  | environment=ElevenLabsEnvironment.PRODUCTION\_IN |
| 13  | )   |

Limitations
-----------

Currently, ElevenLabs provides limited support for migrating your resources from non-isolated to isolated environments. However, you can enable professional voice clone link sharing from a non-isolated environment and add it to your isolated environment; please refer to the FAQ below for instructions. Reach out to us if you intend to move instant voice clones. For other resources, such as agents in the Agents Platform, we recommend recreation via the API where possible.

Dubbing is not currently available in isolated environments.

### India

*   India has limited availability for the LLMs in the Agents Platform. Currently, we support: GPT 4o, GLM-4.5-Air, Qwen3-30B, Qwen3-4B and Custom LLMs. All open source models are hosted by ElevenLabs.
*   Twilio doesn’t currently offer an India routing region for its calls.

Getting access
--------------

Data residency is an exclusive feature available to ElevenLabs’ Enterprise customers.

**Existing Enterprise Customers**: If you are an existing Enterprise customer, please contact [success@elevenlabs.io](mailto:success@elevenlabs.io)
 to discuss enabling an isolated environment for your account.

**New Customers**: Organizations interested in ElevenLabs Enterprise and requiring an isolated environment should contact [sales@elevenlabs.io](mailto:sales@elevenlabs.io)
 to discuss specific needs and implementation.

FAQ
---

###### Can I run my isolated environment in parallel with the non-isolated one?

Yes, it is possible to do this and to bill the usage for both of them on the same invoice. For more details on unified billing across multiple workspaces, see [consolidated billing](https://elevenlabs.io/docs/overview/administration/consolidated-billing)
.

###### How does this relate to GDPR compliance?

For customers subject to GDPR, ElevenLabs provides options to limit storage and, in some cases, processing to the EU to support customers’ compliance efforts.

###### Do isolated environments impact API performance?

For users inside the isolated environment region, data residency may potentially reduce latency due to localized processing. For users outside the isolated environment region, performance is expected to remain consistent with our global infrastructure. While there may be benefits as it relates to latency, the purpose of these data residency options are not specifically to improve latency.

###### Is Zero Retention Mode automatically enabled in isolated environments?

No, Zero Retention Mode is an optional feature that can be enabled separately, even for accounts with data residency. It provides an additional layer of data minimization by preventing storage of content on our servers.

###### My API requests to the isolated environment are failing

Double check that you are using the correct API URL and the correct API key for the account on the isolated environment.

###### How do I use an isolated environment in the SDK?

When you create the ElevenLabs client object, it takes an environment parameter which is by default US but you can set it to your desired environment. See the [SDK configuration](https://elevenlabs.io/docs/overview/administration/data-residency#sdk-configuration)
 section above for code examples in Python, TypeScript, JavaScript, and React.

###### How do I share a PVC from a non-isolated environment to an isolated environment?

To share a PVC with an isolated environment, first enable link sharing for that voice. Then copy the link, and add the prefix of the isolated environment to the voice link: From: `elevenlabs.io/...` → To: `eu.residency.elevenlabs.io/...`

## Document 38 — Usage analytics | ElevenLabs Documentation {#doc-38}
[https://elevenlabs.io/docs/overview/administration/usage-analytics](https://elevenlabs.io/docs/overview/administration/usage-analytics)

Usage analytics lets you view all the activity on the platform for your account or workspace.

To access usage analytics, click on your profile icon in the top right corner and select [Usage Analytics](https://elevenlabs.io/app/usage)

![Account and Workspace tabs](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F6658d59c4f42c0465f55b29e3ff18b2eb63b80d41f5794186230ffdd97a28162%2Fassets%2Fimages%2Fproduct-guides%2Fadministration%2Fanalytics.webp&w=3840&q=75)

There are two tabs for usage analytics. On an Enterprise plan, the account tab shows data for your individual account, whereas the workspace tab covers all accounts under your workspace.

If you’re not on an Enterprise plan, the data will be the same for your account and your workspace, but some information will only be available in your workspace tab, such as your Voice Add/Edit Operations quota.

Credit usage
------------

In the Credit Usage section, you can filter your usage data in a number of different ways.

In the account tab, you can break your usage down by voice, product or API key, for example.

In the workspace you have additional options allowing you to break usage down by individual user or workspace group.

You can view the data by day, week, month or cumulatively. If you want to be more specific, you can use filters to show only your usage for specific voices, products or API keys.

This feature is quite powerful, allowing you to gain great insights into your usage or understand your customers’ usage if you’ve implemented us in your product.

![Credit use broken down by voice](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F8e3ea1554ee991c37cc385b2d844affd1fb7045778cbecfdf2a3578a231676ea%2Fassets%2Fimages%2Fproduct-guides%2Fadministration%2Fanalytics-credits-voice.webp&w=3840&q=75)

API requests
------------

In the API Requests section, you’ll find not only the total number of requests made within a specific timeframe but also the number of concurrent requests during that period.

You can view data by different time periods, for example, hour, day, month and year, and at different levels of granularity.

![Workspace API calls](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F7acbf75b1dee8abfc37faee4ba6fb2e550b7d1cce6e5e883300723712e111180%2Fassets%2Fimages%2Fproduct-guides%2Fadministration%2Fanalytics-workspace-api.webp&w=3840&q=75)

Export data
-----------

You also have the option to export your usage data as a CSV file. To do this, just click the “Export as CSV” button, and the data from your current view will be exported and downloaded.

![Export your usage data as CSV](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F4fd50994be92ea1d143440ea9eb3967b20ca11bf5927fe92b27688b4eb5cd24a%2Fassets%2Fimages%2Fproduct-guides%2Fadministration%2Fanalytics-export.webp&w=3840&q=75)

## Document 39 — Webhooks | ElevenLabs Documentation {#doc-39}
[https://elevenlabs.io/docs/overview/administration/webhooks](https://elevenlabs.io/docs/overview/administration/webhooks)

Overview
--------

Certain events within ElevenLabs can be configured to trigger webhooks, allowing external applications and systems to receive and process these events as they occur. Currently supported event types include:

| Event type | Description |
| --- | --- |
| `post_call_transcription` | A Agents Platform call has finished and analysis is complete |
| `voice_removal_notice` | A shared voice is scheduled to be removed |
| `voice_removal_notice_withdrawn` | A shared voice is no longer scheduled for removal |
| `voice_removed` | A shared voice has been removed and is no longer useable |

Configuration
-------------

Webhooks can be created, disabled and deleted from the general settings page. For users within [Workspaces](https://elevenlabs.io/docs/overview/administration/workspaces/overview)
, only the workspace admins can configure the webhooks for the workspace.

![HMAC webhook configuration](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F9ea298daac1c64eb43c802a12f7824e83accca44ba2edd1d01a39bcd62c0b9d6%2Fassets%2Fimages%2Fproduct-guides%2Fadministration%2Fhmacwebhook.png&w=3840&q=75)

After creation, the webhook can be selected to listen for events within product settings such as [Agents Platform](https://elevenlabs.io/docs/agents-platform/workflows/post-call-webhooks)
.

Webhooks can be disabled from the general settings page at any time. Webhooks that repeatedly fail are auto disabled if there are 10 or more consecutive failures and the last successful delivery was more than 7 days ago or has never been successfully delivered. Auto-disabled webhooks require re-enabling from the settings page. Webhooks can be deleted if not in use by any products.

Integration
-----------

To integrate with webhooks, the listener should create an endpoint handler to receive the webhook event data POST requests. After validating the signature, the handler should quickly return HTTP 200 to indicate successful receipt of the webhook event, repeat failure to correctly return may result in the webhook becoming automatically disabled. Each webhook event is dispatched only once, refer to the [API](https://elevenlabs.io/docs/api-reference/introduction)
 for methods to poll and get product specific data.

### Top-level fields

| Field | Type | Description |
| --- | --- | --- |
| `type` | string | Type of event |
| `data` | object | Data for the event |
| `event_timestamp` | string | When this event occurred |

Example webhook payload
-----------------------

|     |     |
| --- | --- |
| 1   | {   |
| 2   | "type": "post\_call\_transcription", |
| 3   | "event\_timestamp": 1739537297, |
| 4   | "data": { |
| 5   | "agent\_id": "xyz", |
| 6   | "conversation\_id": "abc", |
| 7   | "status": "done", |
| 8   | "transcript": \[ |\
| 9   | {   |\
| 10  | "role": "agent", |\
| 11  | "message": "Hey there angelo. How are you?", |\
| 12  | "tool\_calls": null, |\
| 13  | "tool\_results": null, |\
| 14  | "feedback": null, |\
| 15  | "time\_in\_call\_secs": 0, |\
| 16  | "conversation\_turn\_metrics": null |\
| 17  | },  |\
| 18  | {   |\
| 19  | "role": "user", |\
| 20  | "message": "Hey, can you tell me, like, a fun fact about 11 Labs?", |\
| 21  | "tool\_calls": null, |\
| 22  | "tool\_results": null, |\
| 23  | "feedback": null, |\
| 24  | "time\_in\_call\_secs": 2, |\
| 25  | "conversation\_turn\_metrics": null |\
| 26  | },  |\
| 27  | {   |\
| 28  | "role": "agent", |\
| 29  | "message": "I do not have access to fun facts about Eleven Labs. However, I can share some general information about the company. Eleven Labs is an AI voice technology platform that specializes in voice cloning and text-to-speech...", |\
| 30  | "tool\_calls": null, |\
| 31  | "tool\_results": null, |\
| 32  | "feedback": null, |\
| 33  | "time\_in\_call\_secs": 9, |\
| 34  | "conversation\_turn\_metrics": { |\
| 35  | "convai\_llm\_service\_ttfb": { |\
| 36  | "elapsed\_time": 0.3704247010173276 |\
| 37  | },  |\
| 38  | "convai\_llm\_service\_ttf\_sentence": { |\
| 39  | "elapsed\_time": 0.5551181449554861 |\
| 40  | }   |\
| 41  | }   |\
| 42  | }   |\
| 43  | \], |
| 44  | "metadata": { |
| 45  | "start\_time\_unix\_secs": 1739537297, |
| 46  | "call\_duration\_secs": 22, |
| 47  | "cost": 296, |
| 48  | "deletion\_settings": { |
| 49  | "deletion\_time\_unix\_secs": 1802609320, |
| 50  | "deleted\_logs\_at\_time\_unix\_secs": null, |
| 51  | "deleted\_audio\_at\_time\_unix\_secs": null, |
| 52  | "deleted\_transcript\_at\_time\_unix\_secs": null, |
| 53  | "delete\_transcript\_and\_pii": true, |
| 54  | "delete\_audio": true |
| 55  | },  |
| 56  | "feedback": { |
| 57  | "overall\_score": null, |
| 58  | "likes": 0, |
| 59  | "dislikes": 0 |
| 60  | },  |
| 61  | "authorization\_method": "authorization\_header", |
| 62  | "charging": { |
| 63  | "dev\_discount": true |
| 64  | },  |
| 65  | "termination\_reason": "" |
| 66  | },  |
| 67  | "analysis": { |
| 68  | "evaluation\_criteria\_results": {}, |
| 69  | "data\_collection\_results": {}, |
| 70  | "call\_successful": "success", |
| 71  | "transcript\_summary": "The conversation begins with the agent asking how Angelo is, but Angelo redirects the conversation by requesting a fun fact about 11 Labs. The agent acknowledges they don't have specific fun facts about Eleven Labs but offers to provide general information about the company. They briefly describe Eleven Labs as an AI voice technology platform specializing in voice cloning and text-to-speech technology. The conversation is brief and informational, with the agent adapting to the user's request despite not having the exact information asked for." |
| 72  | },  |
| 73  | "conversation\_initiation\_client\_data": { |
| 74  | "conversation\_config\_override": { |
| 75  | "agent": { |
| 76  | "prompt": null, |
| 77  | "first\_message": null, |
| 78  | "language": "en" |
| 79  | },  |
| 80  | "tts": { |
| 81  | "voice\_id": null |
| 82  | }   |
| 83  | },  |
| 84  | "custom\_llm\_extra\_body": {}, |
| 85  | "dynamic\_variables": { |
| 86  | "user\_name": "angelo" |
| 87  | }   |
| 88  | }   |
| 89  | }   |
| 90  | }   |

Authentication
--------------

It is important for the listener to validate all incoming webhooks. Webhooks currently support authentication via HMAC signatures. Set up HMAC authentication by:

*   Securely storing the shared secret generated upon creation of the webhook
*   Verifying the ElevenLabs-Signature header in your endpoint using the shared secret

The ElevenLabs-Signature takes the following format:

|     |     |
| --- | --- |
| 1   | t=timestamp,v0=hash |

The hash is equivalent to the hex encoded sha256 HMAC signature of `timestamp.request_body`. Both the hash and timestamp should be validated, an example is shown here:

###### Python

###### JavaScript

Example python webhook handler using FastAPI:

|     |     |
| --- | --- |
| 1   | from fastapi import FastAPI, Request |
| 2   | import time |
| 3   | import hmac |
| 4   | from hashlib import sha256 |
| 5   |     |
| 6   | app = FastAPI() |
| 7   |     |
| 8   | \# Example webhook handler |
| 9   | @app.post("/webhook") |
| 10  | async def receive\_message(request: Request): |
| 11  | payload = await request.body() |
| 12  | headers = request.headers.get("elevenlabs-signature") |
| 13  | if headers is None: |
| 14  | return |
| 15  | timestamp = headers.split(",")\[0\]\[2:\] |
| 16  | hmac\_signature = headers.split(",")\[1\] |
| 17  |     |
| 18  | # Validate timestamp |
| 19  | tolerance = int(time.time()) - 30 \* 60 |
| 20  | if int(timestamp) < tolerance |
| 21  | return |
| 22  |     |
| 23  | # Validate signature |
| 24  | full\_payload\_to\_sign = f"{timestamp}.{payload.decode('utf-8')}" |
| 25  | mac = hmac.new( |
| 26  | key=secret.encode("utf-8"), |
| 27  | msg=full\_payload\_to\_sign.encode("utf-8"), |
| 28  | digestmod=sha256, |
| 29  | )   |
| 30  | digest = 'v0=' + mac.hexdigest() |
| 31  | if hmac\_signature != digest: |
| 32  | return |
| 33  |     |
| 34  | # Continue processing |
| 35  |     |
| 36  | return {"status": "received"} |

## Document 40 — Workspaces | ElevenLabs Documentation {#doc-40}
[https://elevenlabs.io/docs/overview/administration/workspaces/overview](https://elevenlabs.io/docs/overview/administration/workspaces/overview)

![Workspaces](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F9674077467be3330f32004c8010ac4a16afd6ab5daa40d8baa42d48a0f27157f%2Fassets%2Fimages%2Fproduct-guides%2Fworkspaces%2Fworkspace-main.png&w=3840&q=75)

Workspaces are currently only available for Scale, Business and Enterprise customers.

Overview
--------

For teams that want to collaborate in ElevenLabs, we offer shared Workspaces. Workspaces offer the following benefits:

*   **Shared billing** - Rather than having each of your team members individually create & manage subscriptions, all of your team’s character usage and billing is centralized under one Workspace.
*   **Shared resources** - Within a Workspace, your team can share: voices, studio instances, ElevenLabs agents, dubbings and more.
*   **Access management** - Your Workspace admin can easily add and remove team members.
*   **API Key management** - You can issue and revoke unlimited API keys for your team.

FAQ
---

###### How do I create a Workspace?

### Creating a Workspace

Workspaces are automatically enabled on all accounts with Scale, Business and Enterprise subscriptions. On the Scale and Business plans, the account owner will be the Workspace admin by default. They will have the power to add more team members as well as nominate others to be an admin. When setting up your Enterprise account, you’ll be asked to nominate a Workspace admin.

###### How do I add a team member to a Workspace?

### Adding a team member to a Workspace

Only administrators can add and remove team members.

![Workspace domain verification](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F5663221cd2bff5a1cd150bdd0a5bdb731ab1c6ee2b82958e4c8c85f33092318b%2Fassets%2Fimages%2Fproduct-guides%2Fworkspaces%2Fworkspace-product-feature.png&w=3840&q=75)

Once you are logged in, select your profile icon in the top right corner of the dashboard, choose **Workspace settings**, then navigate to the **Members** tab. From there you’ll be able to add team members, assign roles and remove members from the Workspace.

#### Bulk Invites

Enterprise customers can invite their users in bulk once their domain has been verified following the [Verify Domain step](https://elevenlabs.io/docs/overview/administration/workspaces/sso#verify-your-email-domain)
 from the SSO configuration process.

#### User Auto Provisioning

Enterprise customers can enable user auto provisioning via the **Security & SSO** tab in workspace settings. When this is enabled, new users with an email domain matching one of your verified domains will automatically join your workspace and take up a seat.

###### What roles can I assign members?

### Roles

There are two roles, Admins and Members. Members have full access to your Workspace and can generate an unlimited number of characters (within your current overall plan’s limit).

Admins have all of the access of Members, with the added ability to add/remove teammates and permissions to manage your subscription.

###### How do I manage billing?

### Managing Billing

Only admins can manage billing.

To manage your billing, select your profile icon in the top right corner of the dashboard and choose **Subscription**. From there, you’ll be able to update your payment information and access past invoices.

###### How do I manage Service Accounts / API keys?

### Managing Service Accounts

To manage Service Accounts, select your profile icon in the top right corner of the dashboard and choose **Workspace settings**. Navigate to the **Service Accounts** tab and you’ll be able to create / delete service accounts as well as issue new API keys for those service accounts.

”Workspace API keys” were formerly a type of Service Account with a single API key.

###### Who is the Workspace owner?

### Managing the Workspace owner

Each Workspace can have one owner. By default, this will be the account owner for Scale and Business subscriptions. Ownership can be transferred to another account.

If you downgrade your subscription and exceed the available number of seats on your new plan, all users apart from the owner will be locked out. The admin can also lock users in advance of the downgrade.

## Document 41 — Service Accounts and API Keys | ElevenLabs Documentation {#doc-41}
[https://elevenlabs.io/docs/overview/administration/workspaces/service-accounts](https://elevenlabs.io/docs/overview/administration/workspaces/service-accounts)

![Service Accounts](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F85b1646b900085c39df273c7337c7428e92325cb621da63c2f6c6d76d2119efd%2Fassets%2Fimages%2Fproduct-guides%2Fworkspaces%2Fworkspace-service-accounts.png&w=3840&q=75)

Overview
--------

Service Accounts are currently only available for multi-seat customers, and only Workspace admins can use this feature. To upgrade, [get in touch with our sales team](https://elevenlabs.io/contact-sales)
.

Service Accounts and their respective API keys allow access to workspace resources without relying on an individual’s access to ElevenLabs.

Service Accounts
----------------

A service account acts as a workspace member. When originally created, they do not have access to any resources.

The service account can be granted access to resources by either adding the service account to a group or directly sharing resources with the service account. It is recommended to add them to a group so that future users can be added to the same group and have the same permissions.

Rotating API keys
-----------------

When creating a new API key to replace one that you are rotating out, make sure to create the API key for the same service account and copy the API key permissions from the old API key to ensure that no access is lost.

API keys can either be rotated via the UI or via the API.

To rotate API keys on the web, click on your profile icon located in the top right corner of the dashboard, select **Workspace settings**, and then navigate to the **Service Accounts** tab. From there, you can create a new API key for the same service account. Once you’ve switched to using the new API key, you can delete the old one from this tab.

To rotate API keys via the API, please see the API reference underneath **Service Accounts** for the relevant endpoints.

## Document 42 — Sharing resources | ElevenLabs Documentation {#doc-42}
[https://elevenlabs.io/docs/overview/administration/workspaces/sharing-resources](https://elevenlabs.io/docs/overview/administration/workspaces/sharing-resources)

![Sharing a project](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F1d1fe7f982282bb04bf6f27a363e384a83f7cf25725bef4a47a536d7e8f5f70e%2Fassets%2Fimages%2Fproduct-guides%2Fworkspaces%2Fshare-project.png&w=3840&q=75)

Overview
--------

If your subscription plan includes multiple seats, you can share resources with your members. Resources you can share include: voices, ElevenLabs agents, studio projects and more. Check the [Workspaces API](https://elevenlabs.io/docs/api-reference/workspace/resources/share)
 for an up-to-date list of resources you can share.

Sharing
-------

You can share a **resource** with a **principal**. A principal is one of the following:

*   A user
*   A user group
*   A service account

A resource can be shared with at most 100 principals.

Service Accounts behave like individual users. They don’t have access to anything in the Workspace when they are created, but they can be added to resources by resource admins.

#### Default Sharing

If you would like to share with specific principals for each new resource by default, this can be enabled in your personal settings page under **Default Sharing Preferences**. Every new resource created after this is enabled will be automatically shared with the principals that you add here.

Roles
-----

When you share a resource with a principal, you can assign them a **role**. We support the following roles:

*   **Viewer**: Viewers can discover the resource and its contents. They can also “use” the resource, e.g., generate TTS with a voice or listen to the audio of a studio instance.
*   **Editor**: Everything a viewer can do, plus they can also edit the contents of the resource.
*   **Admin**: Everything an editor can do, plus they can also delete the resource and manage sharing permissions.

When you create a resource, you have admin permissions on it. Other resource admins cannot remove your admin permissions on the resources you created.

Workspace admins have admin permissions on all resources in the workspace. This can be removed from them only by removing their Workspace admin role.

## Document 43 — Single Sign-On (SSO) | ElevenLabs Documentation {#doc-43}
[https://elevenlabs.io/docs/overview/administration/workspaces/sso](https://elevenlabs.io/docs/overview/administration/workspaces/sso)

![SSO](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Fe1575b62be17ed4dbd459539161c7853f9f3e000c8d2eeba01f40e1335d7dad0%2Fassets%2Fimages%2Fproduct-guides%2Fworkspaces%2Fworkspace-sso.png&w=3840&q=75)

Overview
--------

SSO is currently only available for Enterprise customers, and only Workspace admins can use this feature. To upgrade, [get in touch with our sales team](https://elevenlabs.io/contact-sales)
.

Single Sign-On (SSO) allows your team to log in to ElevenLabs by using your existing identity provider. This allows your team to use the same credentials they use for other services to log in to ElevenLabs.

Guide
-----

[1](https://elevenlabs.io/docs/overview/administration/workspaces/sso#access-your-sso-settings)

### Access your SSO settings

Click on your profile icon located in the top right corner of the dashboard, select **Workspace settings**, and then navigate to the **Security & SSO** tab.

[2](https://elevenlabs.io/docs/overview/administration/workspaces/sso#choose-identity-providers)

### Choose identity providers

You can choose from a variety of pre-configured identity providers, including Google, Apple, GitHub, etc. Custom organization SSO providers will only appear in this list after they have been configured, as shown in the “SSO Provider” section.

[3](https://elevenlabs.io/docs/overview/administration/workspaces/sso#verify-your-email-domain)

### Verify your email domain

Next, you need to verify your email domain for authentication. This lets ElevenLabs know that you own the domain you are configuring for SSO. This is a security measure to prevent unauthorized access to your Workspace.

Click the **Verify domain** button and enter the domain name you want to verify. After completing this step, click on the domain pending verification. You will be prompted to add a DNS TXT record to your domain’s DNS settings. Once the DNS record has been added, click on the **Verify** button.

[4](https://elevenlabs.io/docs/overview/administration/workspaces/sso#configure-sso)

### Configure SSO

If you want to configure your own SSO provider, select the SSO provider dropdown to select between OIDC (OpenID Connect) and SAML (Security Assertion Markup Language).

Only Service Provider (SP) initiated SSO is supported for SAML. To ease the sign in process, you can create a bookmark app in your SSO provider linking to [https://elevenlabs.io/app/sign-in?use\_sso=true](https://elevenlabs.io/app/sign-in?use_sso=true)
. You can include the user’s email as an additional query parameter to pre-fill the field. For example [https://elevenlabs.io/app/sign-in?use\_sso=true&email=test@test.com](https://elevenlabs.io/app/sign-in?use_sso=true&email=test@test.com)

Once you’ve filled out the required fields, click the **Update SSO** button to save your changes.

Configuring a new SSO provider will log out all Workspace members currently logged in with SSO.

FAQ
---

###### Microsoft Entra Identifier / Azure AD - SAML

**What to fill in on the Entra / Azure side**:

*   **Identifier (Entity ID)**: Use the _Service Provider Entity ID_ value from the ElevenLabs SSO configuration page.
*   **Reply URL (Assertion Consumer Service URL)**: Use `https://elevenlabs.io/__/auth/handler`
    *   For EU residency, use `https://eu.elevenlabs.io/__/auth/handler`
    *   For India residency, use `https://in.elevenlabs.io/__/auth/handler`
*   **ACS URL**: Same as the Reply URL above.

**What to fill in on the ElevenLabs side**:

*   **IdP Entity ID**: Use the _Microsoft Entra Identifier_ (the full URL, e.g., `https://sts.windows.net/{tenant-id}/`)
*   **IdP Sign-In URL**: Use the _Login URL_ from Entra / Azure

###### Okta - SAML

**What to fill in on the Okta side**:

*   **Audience Restriction**: Use the _Service Provider Entity ID_ from the ElevenLabs SSO configuration page.
*   **Single Sign-On URL/Recipient URL/Destination**: Use `https://elevenlabs.io/__/auth/handler`
    *   For EU residency, use `https://eu.elevenlabs.io/__/auth/handler`
    *   For India residency, use `https://in.elevenlabs.io/__/auth/handler`

**What to fill in on the ElevenLabs side**:

*   Create the application in Okta and then fill out these fields using the results
*   **Identity Provider Entity Id**: Use the SAML Issuer ID
*   **Identity Provider Sign-In URL**: Use the Sign On URL from Okta
    *   This can generally be found in the Metadata details within the Sign On tab of the Okta application
    *   It will end in **/sso/saml**

###### OneLogin - SAML

**What to fill in on the OneLogin side**:

*   **Recipient**: Use `https://elevenlabs.io/__/auth/handler`
    *   For EU residency, use `https://eu.elevenlabs.io/__/auth/handler`
    *   For India residency, use `https://in.elevenlabs.io/__/auth/handler`

###### OIDC - Common Errors

Please ensure that `email` and `email_verified` are included in the custom attributes returned in the OIDC response. Without these, the following errors may be hit:

*   _No email address was received_: Fixed by adding **email** to the response.
*   _Account exists with different credentials_: Fixed by adding **email\_verified** to the response

###### I am getting the error 'Unable to login with saml.workspace...'

*   One known error: Inside the `<saml:Subject>` field of the SAML response, make sure `<saml:NameID>` is set to the email address of the user.

## Document 44 — User groups | ElevenLabs Documentation {#doc-44}
[https://elevenlabs.io/docs/overview/administration/workspaces/user-groups](https://elevenlabs.io/docs/overview/administration/workspaces/user-groups)

![Group Management](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Fb3352775553098f965b4c3a92526bf71c6afb836ce32b6eccac2947b80d0fd01%2Fassets%2Fimages%2Fproduct-guides%2Fworkspaces%2Fmanage-group.png&w=3840&q=75)

Overview
--------

Only Workspace admins can create, edit, and delete user groups.

User groups allow you to manage permissions for multiple users at once.

Creating a user group
---------------------

You can create a user group from **Workspace settings**. You can then [share resources](https://elevenlabs.io/docs/overview/administration/workspaces/sharing-resources)
 with the group directly. If access to a user group is lost, access to resources shared with that group is also lost.

Multiple groups
---------------

User groups cannot be nested, but you can add users to multiple groups. If a user is part of multiple groups, they will have the union of all the permissions of the groups they are part of.

For example, you can create a voice and grant the **Sales** and **Marketing** groups viewer and editor roles on the voice, respectively. If a user is part of both groups, they will have editor permissions on the voice. Losing access to the **Marketing** group will downgrade the user’s permissions to viewer.

Disabling platform features
---------------------------

Permissions for groups can be revoked for specific product features, such as Professional Voice Cloning or Sound Effects. To do this, you first have to remove the relevant permissions from the **Everyone** group. Afterwards, enable the permissions for each group that should have access.

## Document 45 — Dubbing | ElevenLabs Documentation {#doc-45}
[https://elevenlabs.io/docs/overview/capabilities/dubbing](https://elevenlabs.io/docs/overview/capabilities/dubbing)

Overview
--------

ElevenLabs [dubbing](https://elevenlabs.io/docs/api-reference/dubbing/create)
 API translates audio and video across 32 languages while preserving the emotion, timing, tone and unique characteristics of each speaker. Our model separates each speaker’s dialogue from the soundtrack, allowing you to recreate the original delivery in another language. It can be used to:

*   Grow your addressable audience by 4x to reach international audiences
*   Adapt existing material for new markets while preserving emotional nuance
*   Offer content in multiple languages without re-recording voice talent

Your browser does not support the video tag.

We also offer a [fully managed dubbing service](https://elevenlabs.io/elevenstudios)
 for video and podcast creators.

Usage
-----

ElevenLabs dubbing can be used in three ways:

*   **Dubbing Studio** in the user interface for fast, interactive control and editing
*   **Programmatic integration** via our [API](https://elevenlabs.io/docs/api-reference/dubbing/create)
     for large-scale or automated workflows
*   **Human-verified dubs via ElevenLabs Productions** - for more information, please reach out to [productions@elevenlabs.io](mailto:productions@elevenlabs.io)
    

The UI supports files up to **500MB** and **45 minutes**. The API supports files up to **1GB** and **2.5 hours**.

[Products\
\
Edit transcripts and translate videos step by step in Dubbing Studio.](https://elevenlabs.io/docs/creative-platform/products/dubbing/dubbing-studio)
[Developers\
\
Learn how to integrate dubbing into your application.](https://elevenlabs.io/docs/developers/guides/cookbooks/dubbing)

### Key features

**Speaker separation** Automatically detect multiple speakers, even with overlapping speech.

**Multi-language output** Generate localized tracks in 32 languages.

**Preserve original voices** Retain the speaker’s identity and emotional tone.

**Keep background audio** Avoid re-mixing music, effects, or ambient sounds.

**Customizable transcripts** Manually edit translations and transcripts as needed.

**Supported file types** Videos and audio can be dubbed from various sources, including YouTube, X, TikTok, Vimeo, direct URLs, or file uploads.

**Video transcript and translation editing** Our AI video translator lets you manually edit transcripts and translations to ensure your content is properly synced and localized. Adjust the voice settings to tune delivery, and regenerate speech segments until the output sounds just right.

A Creator plan or higher is required to dub audio files. For videos, a watermark option is available to reduce credit usage.

### Cost

To reduce credit usage, you can:

*   Dub only a selected portion of your file
*   Use watermarks on video output (not available for audio)
*   Fine-tune transcripts and regenerate individual segments instead of the entire clip

Refer to our [pricing page](https://elevenlabs.io/pricing)
 for detailed credit costs.

List of supported languages for dubbing
---------------------------------------

| No  | Language Name | Language Code |
| --- | --- | --- |
| 1   | English | en  |
| 2   | Hindi | hi  |
| 3   | Portuguese | pt  |
| 4   | Chinese | zh  |
| 5   | Spanish | es  |
| 6   | French | fr  |
| 7   | German | de  |
| 8   | Japanese | ja  |
| 9   | Arabic | ar  |
| 10  | Russian | ru  |
| 11  | Korean | ko  |
| 12  | Indonesian | id  |
| 13  | Italian | it  |
| 14  | Dutch | nl  |
| 15  | Turkish | tr  |
| 16  | Polish | pl  |
| 17  | Swedish | sv  |
| 18  | Filipino | fil |
| 19  | Malay | ms  |
| 20  | Romanian | ro  |
| 21  | Ukrainian | uk  |
| 22  | Greek | el  |
| 23  | Czech | cs  |
| 24  | Danish | da  |
| 25  | Finnish | fi  |
| 26  | Bulgarian | bg  |
| 27  | Croatian | hr  |
| 28  | Slovak | sk  |
| 29  | Tamil | ta  |

FAQ
---

###### What content can I dub?

Dubbing can be performed on all types of short and long form video and audio content. We recommend dubbing content with a maximum of 9 unique speakers at a time to ensure a high-quality dub.

###### Does dubbing preserve the speaker's natural intonation?

Yes. Our models analyze each speaker’s original delivery to recreate the same tone, pace, and style in your target language.

###### What about overlapping speakers or background noise?

We use advanced source separation to isolate individual voices from ambient sound. Multiple overlapping speakers can be split into separate tracks.

###### Are there file size limits?

Via the user interface, the maximum file size is 1GB up to 45 minutes. Through the API, you can process files up to 1GB and 2.5 hours.

###### How do I handle fine-tuning or partial translations?

You can choose to dub only certain portions of your video/audio or tweak translations/voices in our interactive Dubbing Studio.

## Document 46 — Forced Alignment | ElevenLabs Documentation {#doc-46}
[https://elevenlabs.io/docs/overview/capabilities/forced-alignment](https://elevenlabs.io/docs/overview/capabilities/forced-alignment)

Overview
--------

The ElevenLabs [Forced Alignment](https://elevenlabs.io/docs/api-reference/forced-alignment/create)
 API turns spoken audio and text into a time-aligned transcript. This is useful for cases where you have audio recording and a transcript, but need exact timestamps for each word or phrase in the transcript. This can be used for:

*   Matching subtitles to a video recording
*   Generating timings for an audiobook recording of an ebook

Usage
-----

The Forced Alignment API can be used by interfacing with the ElevenLabs API directly.

[Developers\
\
Learn how to integrate Forced Alignment into your application.](https://elevenlabs.io/docs/developers/guides/cookbooks/forced-alignment)

Supported languages
-------------------

Our multilingual v2 models support 29 languages:

_English (USA, UK, Australia, Canada), Japanese, Chinese, German, Hindi, French (France, Canada), Korean, Portuguese (Brazil, Portugal), Italian, Spanish (Spain, Mexico), Indonesian, Dutch, Turkish, Filipino, Polish, Swedish, Bulgarian, Romanian, Arabic (Saudi Arabia, UAE), Czech, Greek, Finnish, Croatian, Malay, Slovak, Danish, Tamil, Ukrainian & Russian._

FAQ
---

###### What is forced alignment?

Forced alignment is a technique used to align spoken audio with text. You provide an audio file and a transcript of the audio file and the API will return a time-aligned transcript.

It’s useful for cases where you have audio recording and a transcript, but need exact timestamps for each word or phrase in the transcript.

###### What text input formats are supported?

The input text should be a string with no special formatting i.e. JSON.

Example of good input text:

"Hello, how are you?"

Example of bad input text:

|     |
| --- |
| {   |
| "text": "Hello, how are you?" |
| }   |

###### How much does Forced Alignment cost?

Forced Alignment costs the same as the [Speech to Text](https://elevenlabs.io/pricing/api?price.section=speech_to_text#pricing-table)
 API.

###### Does Forced Alignment support diarization?

Forced Alignment does not support diarization. If you provide diarized text, the API will likely return unwanted results.

###### What is the maximum audio file size for Forced Alignment?

The maximum file size for Forced Alignment is 3GB.

###### What is the maximum duration for a Forced Alignment input file?

For audio files, the maximum duration is 10 hours.

For the text input, the maximum length is 675k characters.

## Document 47 — Image & Video | ElevenLabs Documentation {#doc-47}
[https://elevenlabs.io/docs/overview/capabilities/image-video](https://elevenlabs.io/docs/overview/capabilities/image-video)

Overview
--------

Image & Video enables you to create high-quality visual content from simple text descriptions or reference images. Generate static images or dynamic videos in any style, then refine them iteratively with additional prompts, upscale for high-resolution output, and even add lip-sync with audio.

This feature is currently in beta.

[Products\
\
Complete guide to using Image & Video in ElevenLabs.](https://elevenlabs.io/docs/creative-platform/playground/image-video)

Key capabilities
----------------

*   **Image generation**: Create high-quality images from text prompts or reference images with models optimized for speed or quality
*   **Video generation**: Generate dynamic videos with cinematic motion, physics realism, and integrated audio. Video generation is only available on paid plans
*   **Iterative refinement**: Refine generations with additional prompts and create variations
*   **Enhancement tools**: Upscale resolution by up to 4x and apply realistic lip-sync with audio
*   **Multiple models**: Access specialized models for different use cases, from rapid iteration to production-ready content
*   **Reference support**: Guide generation with start frames, end frames, and style references. Supports a wide range of image file formats including JPG, PNG, WEBP, and more
*   **Export flexibility**: Download as standalone files or import directly into Studio projects

Workflow
--------

The creation process moves you from inspiration to finished asset in four stages:

**Explore:** Discover community creations to find inspiration and study effective prompts.

**Generate:** Use the prompt box to describe what you want to create, select a model, and fine-tune settings.

**Iterate and enhance:** Review generations, create variations, and apply enhancements like upscaling and lip-syncing.

**Export:** Download finished assets or send them directly to Studio.

Supported download formats
--------------------------

**Video:**

*   **MP4**: Codecs H.264, H.265. Quality up to 4K (with upscaling)

**Image:**

*   **PNG**: High-resolution, lossless output

Models
------

Image & Video provides access to specialized models optimized for different use cases. Each model offers unique capabilities, from rapid iteration to production-ready quality.

Post-processing models require an existing generated output, though you can also upload your own image or video file.

###### Video generative models

###### OpenAI Sora 2 Pro

The most advanced, high-fidelity video model for cinematic results at your disposal.

**Generation inputs:**

*   Text-to-Video
*   Start Frame

**Features:**

*   Highest-fidelity, professional-grade output with synced audio
*   Precise multi-shot control
*   Excels at complex motion and prompt adherence
*   Fixed durations: 4s, 8s, and 12s
*   Batch creation with up to 4 generations at a time

**Output options:**

*   Resolutions: 720p, 1080p
*   Aspect ratios: 16:9, 9:16

**Ideal for:**

*   Cinematic, professional-grade video content

**Cost:** Starts at 12,000 credits for a generation

End frame is not currently supported. Cannot provide image references. Sound is enabled by default.

###### OpenAI Sora 2

The standard, high-speed version of OpenAI’s advanced video model, tuned for everyday content creation.

**Generation inputs:**

*   Text-to-Video
*   Start Frame

**Features:**

*   Realistic, physics-aware videos with synced audio
*   Fine scene control
*   Fixed durations: 4s, 8s, and 12s
*   Batch creation with up to 4 generations at a time
*   Strong narrative and character consistency

**Output options:**

*   Resolutions: 720p, 1080p
*   Aspect ratios: 16:9, 9:16

**Ideal for:**

*   Everyday content creation with realistic physics

**Cost:** Starts at 4,000 credits for default settings

End frame is not currently supported. Cannot provide image references. Sound is enabled by default.

###### Google Veo 3.1

A professional-grade model for high-quality, cinematic video generation.

**Generation inputs:**

*   Text-to-Video
*   Start Frame
*   End Frame
*   Image References

**Features:**

*   Excellent quality and creative control with negative prompts
*   Fully integrated and synchronized audio
*   Realistic dialogue, lip-sync, and sound effects
*   Fixed durations: 4s, 6s, and 8s
*   Batch creation with up to 4 generations at a time
*   Dedicated sound control

**Output options:**

*   Resolutions: 720p, 1080p
*   Aspect ratios: 16:9, 9:16

**Ideal for:**

*   High-quality, cinematic video generation with full creative control

**Cost:** Starts at 8,000 credits for default settings

Enabling and disabling sound will change the generation credits.

###### Kling 2.5

A balanced and versatile model for high-quality, full-HD video generation.

**Generation inputs:**

*   Text-to-Video
*   Start Frame

**Features:**

*   Excels at simulating complex motion and realistic physics
*   Accurately models fluid dynamics and expressions
*   Fixed durations: 5s and 10s
*   Batch creation with up to 4 generations at a time

**Output options:**

*   Resolutions: 1080p
*   Aspect ratios: 16:9, 1:1, 9:16

**Ideal for:**

*   Realistic physics simulations and complex motion

**Cost:** Starts at 3,500 credits for default settings

End frame is not currently supported. Cannot provide image references. Sound control not available.

###### Google Veo 3.1 Fast

A high-speed model optimized for rapid previews and generations, delivering sharper visuals with lower latency.

**Generation inputs:**

*   Text-to-Video
*   Start Frame
*   End Frame

**Features:**

*   Advanced creative control with negative prompts and dedicated sound control
*   Fixed durations: 4s, 6s, and 8s
*   Batch creation with up to 4 generations at a time
*   Accurately models real-world physics for realistic motion and interactions

**Output options:**

*   Resolutions: 720p, 1080p
*   Aspect ratios: 16:9, 9:16

**Ideal for:**

*   Quick iteration and A/B testing visuals
*   Fast-paced social media content creation

**Cost:** Starts at 4,000 credits for default settings

###### Google Veo 3

Production-ready model delivering exceptional quality, strong physics realism, and coherent narrative audio.

**Generation inputs:**

*   Text-to-Video
*   Start Frame

**Features:**

*   Advanced integrated “narrative audio” generation that matches video tone and story
*   Granular creative control with negative prompts and dedicated sound control
*   Fixed durations: 4s, 6s, and 8s
*   Batch creation with up to 4 generations at a time

**Output options:**

*   Resolutions: 720p, 1080p
*   Aspect ratios: 16:9, 9:16

**Ideal for:**

*   Final renders and professional marketing content
*   Short-form storytelling

**Cost:** Starts at 8,000 credits for default settings

###### Google Veo 3 Fast

A high-speed, cost-efficient model for generating audio-backed video from text or a starting image.

**Generation inputs:**

*   Text-to-Video
*   Start Frame

**Features:**

*   Granular creative control with negative prompts and dedicated sound control
*   Fixed durations: 4s, 6s, and 8s
*   Batch creation with up to 4 generations at a time

**Output options:**

*   Resolutions: 720p, 1080p
*   Aspect ratios: 16:9, 9:16

**Ideal for:**

*   Rapid iteration and previews
*   Cost-effective content creation

**Cost:** Starts at 4,000 credits for default settings

###### Seedance 1 Pro

A specialized model for creating dynamic, multi-shot sequences with large movement and action.

**Generation inputs:**

*   Text-to-Video
*   Start Frame
*   End Frame

**Features:**

*   Highly stable physics and seamless transitions between shots
*   Fixed durations: 3s, 4s, 5s, 6s, 7s, 8s, 9s, 10s, 11s, and 12s
*   Batch creation with up to 4 generations at a time
*   Maximum creative flexibility with numerous aspect ratio options

**Output options:**

*   Resolutions: 480p, 720p, 1080p
*   Aspect ratios: 21:9, 16:9, 4:3, 1:1, 3:4, 9:16

**Ideal for:**

*   Storytelling and action scenes requiring stable physics

**Cost:** Starts at 4,800 credits for default settings

Aspect ratio and resolution do not affect generation credits, but duration does.

###### Wan 2.5

A versatile model that delivers cinematic motion and high prompt fidelity from text or a starting image.

**Generation inputs:**

*   Text-to-Video
*   Start Frame (Image-to-Video)

**Features:**

*   Granular creative control with negative prompts and dedicated sound control
*   Fixed durations: 5s and 10s
*   Batch creation with up to 4 generations at a time

**Output options:**

*   Resolutions: 480p, 720p, 1080p
*   Aspect ratios: 16:9, 1:1, 9:16

**Ideal for:**

*   Cinematic content with strong prompt adherence

**Cost:** Starts at 2,500 credits for default settings

Generation cost varies based on selected settings.

###### Image generative models

###### Google Nano Banana

A high-speed model for quick, high-quality image generation and editing directly from text prompts.

**Features:**

*   Supports multiple image references to guide generation
*   Generates up to 4 images at a time

**Output options:**

*   Aspect ratios: 21:9, 16:9, 5:4, 4:3, 3:2, 1:1, 2:3, 3:4, 4:5, 9:16

**Ideal for:**

*   Rapid image creation and iteration

**Cost:** Starts at 2,000 credits for default settings; varies based on number of generations

###### Seedream 4

A specialized image model for generating multi-shot sequences or scenes with large movement and action.

**Features:**

*   Excels at creating images with stable physics and coherent transitions
*   Supports multiple image references to guide generation
*   Generates up to 4 images at a time

**Output options:**

*   Aspect ratios: auto, 16:9, 4:3, 1:1, 3:4, 9:16

**Ideal for:**

*   Action scenes and dynamic compositions

**Cost:** Starts at 1,200 credits for default settings; varies based on number of generations

###### Flux 1 Kontext Pro

A professional model for advanced image generation and editing, offering strong scene coherence and style control.

**Features:**

*   Image-based style control requiring a reference image to guide visual aesthetic
*   Generates up to 4 images at a time

**Output options:**

*   Aspect ratios: 21:9, 16:9, 4:3, 3:2, 1:1, 2:3, 3:4, 4:5, 9:16, 9:21

**Ideal for:**

*   Professional content with precise style requirements

**Cost:** Starts at 1,600 credits; varies based on settings and number of generations

###### Wan 2.5

An image model with strong prompt fidelity and motion awareness, ideal for capturing dynamic action in a still frame.

**Features:**

*   Granular control with negative prompts
*   Supports multiple image references to guide generation
*   Generates up to 4 images at a time

**Output options:**

*   Aspect ratios: 16:9, 4:3, 1:1, 3:4, 9:16

**Ideal for:**

*   Dynamic still images with motion awareness

**Cost:** Starts at 2,000 credits; varies based on settings

###### OpenAI GPT Image 1

A versatile model for precise, high-quality image creation and detailed editing guided by natural language prompts.

**Features:**

*   Supports multiple image references to guide generation
*   Generates up to 4 images at a time

**Output options:**

*   Aspect ratios: 3:2, 1:1, 2:3
*   Quality options: low, medium, high

**Ideal for:**

*   Creating and editing images with precise, text-based control

**Cost:** Starts at 2,400 credits for default settings; varies based on settings and number of generations

###### Lip-sync models

###### Omnihuman 1.5

A dedicated utility model for generating exceptionally realistic, humanlike lip-sync.

**Inputs:**

*   Static source image
*   Speech audio file

**Features:**

*   Animates the mouth on the source image to match provided audio
*   Creates high-fidelity “talking” video from still images
*   Lip-sync specific tool, not a full video generation model

**Ideal for:**

*   Creating talking avatars
*   Adding dialogue to still images
*   Professional dubbing workflows

**Cost:** Depends on generation input

For best results, the image should contain a detectable figure.

###### Veed LipSync

A fast, affordable, and precise utility model for applying realistic lip-sync to videos.

**Inputs:**

*   Source video
*   New speech audio file

**Features:**

*   Re-animates mouth movements in source video to match new audio
*   Video-to-video lip-sync tool, not a full video generator

**Ideal for:**

*   High-volume, cost-effective dubbing
*   Translating content
*   Correcting audio in video clips with realistic results

**Cost:** Depends on generation input

For best results, the video should contain a detectable figure.

###### Upscaling model

###### Topaz Upscale

A dedicated utility model for image and video upscaling, designed to enhance resolution and detail up to 4x.

**Features:**

*   Enhancement tool that processes existing media
*   Increases media size while preserving natural textures and minimizing artifacts
*   Highly granular upscale factors: 1x, 1.25x, 1.5x, 1.75x, 2x, 3x, 4x
*   Video-specific: Flexible frame rate control (keep source or convert to 24, 25, 30, 48, 50, or 60 fps)

**Ideal for:**

*   Improving quality of generated media
*   Restoring legacy footage or photos
*   Preparing assets for high-resolution displays

**Cost:** Depends on generation input

## Document 48 — Eleven Music | ElevenLabs Documentation {#doc-48}
[https://elevenlabs.io/docs/overview/capabilities/music](https://elevenlabs.io/docs/overview/capabilities/music)

Overview
--------

Eleven Music is a Text to Music model that generates studio-grade music with natural language prompts in any style. It’s designed to understand intent and generate complete, context-aware audio based on your goals. The model understands both natural language and musical terminology, providing you with state-of-the-art features:

*   Complete control over genre, style, and structure
*   Vocals or just instrumental
*   Multilingual, including English, Spanish, German, Japanese and more
*   Edit the sound and lyrics of individual sections or the whole song

Listen to a sample:

Created in collaboration with labels, publishers, and artists, Eleven Music is cleared for nearly all commercial uses, from film and television to podcasts and social media videos, and from advertisements to gaming. For more information on supported usage across our different plans, [see our music terms](https://elevenlabs.io/music-terms)
.

Usage
-----

Eleven Music is available today on the ElevenLabs website, with public API access and integration into our Agents Platform coming soon.

Created in collaboration with labels, publishers, and artists, Eleven Music is cleared for nearly all commercial uses, from film and television to podcasts and social media videos, and from advertisements to gaming. For more information on supported usage across our different plans, head here.

Eleven Music is available today on our website, with public API access and integration into our Agents Platform coming soon. Check out our prompt engineering guide to help you master the full range of the model’s capabilities.

[Products\
\
Step-by-step guide for using Eleven Music on the ElevenLabs Creative Platform.](https://elevenlabs.io/docs/creative-platform/products/music)
[Developers\
\
Step-by-step guide for using Eleven Music with the API.](https://elevenlabs.io/docs/developers/guides/cookbooks/music)
[Prompting guide\
\
Learn how to use Eleven Music with natural language prompts.](https://elevenlabs.io/docs/overview/capabilities/music/best-practices)

FAQ
---

###### What's the maximum duration for generated music?

Generated music has a minimum duration of 10 seconds and a maximum duration of 5 minutes.

###### Is there an API available?

Yes, refer to the [developer quickstart](https://elevenlabs.io/docs/developers/guides/cookbooks/music/quickstart)
 for more information.

###### Can I use Eleven Music for my business?

Yes, Eleven Music is cleared for nearly all commercial uses, from film and television to podcasts and social media videos, and from advertisements to gaming. For more information on supported usage across our different plans, [see our music terms](https://elevenlabs.io/music-terms)
.

###### What audio formats are supported?

Generated audio is provided in MP3 format with professional-grade quality (44.1kHz, 128-192kbps). Other audio formats will be supported soon.

## Document 49 — Best practices | ElevenLabs Documentation {#doc-49}
[https://elevenlabs.io/docs/overview/capabilities/music/best-practices](https://elevenlabs.io/docs/overview/capabilities/music/best-practices)

This guide summarizes the most effective techniques for prompting the Eleven Music model. It covers genre & creativity, instrument & vocal isolation, musical control, and structural timing & lyrics.

The model is designed to understand intent and generate complete, context-aware audio based on your goals. High-level prompts like _“ad for a sneaker brand”_ or _“peaceful meditation with voiceover”_ are often enough to guide the model toward tone, structure, and content that match your use case.

Genre & Creativity
------------------

The model demonstrates strong adherence to genre conventions and emotional tone. Both musical descriptors of emotional tone and tone descriptors themselves will work. It responds effectively to both:

*   Abstract mood descriptors (e.g., “eerie,” “foreboding”)
*   Detailed musical language (e.g., “dissonant violin screeches over a pulsing sub-bass”)

Prompt length and detail do not always correlate with better quality outputs. For more creative and unexpected results, try using simple, evocative keywords to let the model interpret and compose freely.

Instrument & Vocal Isolation
----------------------------

The v1 model does not generate stems directly from a full track. To create stems with greater control, use targeted prompts and structure:

*   Use the word “solo” before instruments (e.g., “solo electric guitar,” “solo piano in C minor”).
*   For vocals, use “a cappella” before the vocal description (e.g., “a cappella female vocals,” “a cappella male chorus”).

To improve stem quality and control:

*   Include key, tempo (BPM), and musical tone (e.g., “a cappella vocals in A major, 90 BPM, soulful and raw”).
*   Be as musically descriptive as possible to guide the model’s output.

Musical Control
---------------

The model accurately follows BPM and often captures the intended musical key. To gain more control over timing and harmony, include tempo cues like “130 BPM” and key signatures like “in A minor” in your prompt.

To influence vocal delivery and tone, use expressive descriptors such as “raw,” “live,” “glitching,” “breathy,” or “aggressive.”

The model can effectively render multiple vocalists, use prompts like “two singers harmonizing in C” to direct vocal arrangement.

In general, more detailed prompts lead to greater control and expressiveness in the output.

Structural Timing & Lyrics
--------------------------

You can specify the length of the song (e.g., “60 seconds”) or use auto mode to let the model determine the duration. If lyrics are not provided, the model will generate structured lyrics that match the chosen or auto-detected length.

By default, most music prompts will include lyrics. To generate music without vocals, add “instrumental only” to your prompt. You can also write your own lyrics for more creative control. The model uses your lyrics in combination with the prompt length to determine vocal structure and placement.

To manage when vocals begin or end, include clear timing cues like:

*   “lyrics begin at 15 seconds”
*   “instrumental only after 1:45”

The model supports multilingual lyric generation. To change the language of a generated song in our UI, use follow-ups like “make it Japanese” or “translate to Spanish.”

Sample Prompts
--------------

The model allows you to move beyond song descriptors and into intent for maximum creativity.

###### Video Game with Musical Control

###### Mascara Audio Ad Creative

###### Live Indie Rock Performance

|     |
| --- |
| Create an intense, fast-paced electronic track for a high-adrenaline video game scene. |
| Use driving synth arpeggios, punchy drums, distorted bass, glitch effects, and |
| aggressive rhythmic textures. The tempo should be fast, 130–150 bpm, with rising tension, |
| quick transitions, and dynamic energy bursts. |

## Document 50 — Sound effects | ElevenLabs Documentation {#doc-50}
[https://elevenlabs.io/docs/overview/capabilities/sound-effects](https://elevenlabs.io/docs/overview/capabilities/sound-effects)

Overview
--------

ElevenLabs [sound effects](https://elevenlabs.io/docs/api-reference/text-to-sound-effects/convert)
 API turns text descriptions into high-quality audio effects with precise control over timing, style and complexity. The model understands both natural language and audio terminology, enabling you to:

*   Generate cinematic sound design for films & trailers
*   Create custom sound effects for games & interactive media
*   Produce Foley and ambient sounds for video content

Listen to an example:

Usage
-----

Sound effects are generated using text descriptions & two optional parameters:

*   **Duration**: Set a specific length for the generated audio (in seconds)
    
    *   Default: Automatically determined based on the prompt
    *   Range: 0.1 to 30 seconds
    *   Cost: 40 credits per second when duration is specified
*   **Looping**: Enable seamless looping for sound effects longer than 30 seconds
    
    *   Creates sound effects that can be played on repeat without perceptible start/end points
    *   Perfect for atmospheric sounds, ambient textures, and background elements
    *   Example: Generate 30s of ‘soft rain’ then loop it endlessly for atmosphere in audiobooks, films, games
*   **Prompt influence**: Control how strictly the model follows the prompt
    
    *   High: More literal interpretation of the prompt
    *   Low: More creative interpretation with added variations

[Products\
\
Step-by-step guide for using sound effects in ElevenLabs.](https://elevenlabs.io/docs/creative-platform/playground/sound-effects)
[Developers\
\
Learn how to integrate sound effects into your application.](https://elevenlabs.io/docs/developers/guides/cookbooks/sound-effects)

### Prompting guide

#### Simple effects

For basic sound effects, use clear, concise descriptions:

*   “Glass shattering on concrete”
*   “Heavy wooden door creaking open”
*   “Thunder rumbling in the distance”

#### Complex sequences

For multi-part sound effects, describe the sequence of events:

*   “Footsteps on gravel, then a metallic door opens”
*   “Wind whistling through trees, followed by leaves rustling”
*   “Sword being drawn, then clashing with another blade”

#### Musical elements

The API also supports generation of musical components:

*   ”90s hip-hop drum loop, 90 BPM”
*   “Vintage brass stabs in F minor”
*   “Atmospheric synth pad with subtle modulation”

#### Audio Terminology

Common terms that can enhance your prompts:

*   **Impact**: Collision or contact sounds between objects, from subtle taps to dramatic crashes
*   **Whoosh**: Movement through air effects, ranging from fast and ghostly to slow-spinning or rhythmic
*   **Ambience**: Background environmental sounds that establish atmosphere and space
*   **One-shot**: Single, non-repeating sound
*   **Loop**: Repeating audio segment
*   **Stem**: Isolated audio component
*   **Braam**: Big, brassy cinematic hit that signals epic or dramatic moments, common in trailers
*   **Glitch**: Sounds of malfunction, jittering, or erratic movement, useful for transitions and sci-fi
*   **Drone**: Continuous, textured sound that creates atmosphere and suspense

FAQ
---

###### What's the maximum duration for generated effects?

The maximum duration is 30 seconds per generation. For longer sequences, you can either generate multiple effects and combine them, or use the looping feature to create seamless repeating sound effects.

###### Can I generate music with this API?

Yes, you can generate musical elements like drum loops, bass lines, and melodic samples. However, for full music production, consider combining multiple generated elements.

###### How do I ensure consistent quality?

Use detailed prompts, appropriate duration settings, and high prompt influence for more predictable results. For complex sounds, generate components separately and combine them.

###### What audio formats are supported?

Generated audio is provided in MP3 format with professional-grade quality. For WAV downloads of non-looping sound effects, audio is delivered at 48kHz sample rate - the industry standard for film, TV, video, and game audio, ensuring no resampling is needed for professional workflows.

###### How do looping sound effects work?

Looping sound effects are designed to play seamlessly on repeat without noticeable start or end points. This is perfect for creating continuous atmospheric sounds, ambient textures, or background elements that need to play indefinitely. For example, you can generate 30 seconds of rain sounds and loop them endlessly for background atmosphere in audiobooks, films, or games.

## Document 51 — Transcription | ElevenLabs Documentation {#doc-51}
[https://elevenlabs.io/docs/overview/capabilities/speech-to-text](https://elevenlabs.io/docs/overview/capabilities/speech-to-text)

Overview
--------

The ElevenLabs [Speech to Text (STT) API](https://elevenlabs.io/docs/developers/guides/cookbooks/speech-to-text/quickstart)
 turns spoken audio into text with state of the art accuracy. Our [Scribe v2 model](https://elevenlabs.io/docs/overview/models)
 adapts to textual cues across 90+ languages and multiple voice styles. To try a live demo please visit our [Speech to Text](https://elevenlabs.io/speech-to-text)
 showcase page.

[Products\
\
Step-by-step guide for using speech to text in ElevenLabs.](https://elevenlabs.io/docs/creative-platform/playground/speech-to-text)
[Developers\
\
Learn how to integrate the speech to text API into your application.](https://elevenlabs.io/docs/developers/guides/cookbooks/speech-to-text/quickstart)
[Realtime speech to text\
\
Learn how to transcribe audio with ElevenLabs in realtime with WebSockets.](https://elevenlabs.io/docs/developers/guides/cookbooks/speech-to-text/realtime/client-side-streaming)

Companies requiring HIPAA compliance must contact [ElevenLabs Sales](https://elevenlabs.io/contact-sales)
 to sign a Business Associate Agreement (BAA) agreement. Please ensure this step is completed before proceeding with any HIPAA-related integrations or deployments.

Models
------

[Scribe v2\
\
State-of-the-art speech recognition model\
\
Accurate transcription in 90+ languages\
\
Keyterm prompting, up to 100 terms\
\
Entity detection, up to 56\
\
Precise word-level timestamps\
\
Speaker diarization, up to 48 speakers\
\
Dynamic audio tagging\
\
Smart language detection](https://elevenlabs.io/docs/overview/models#scribe-v2)
[Scribe v2 Realtime\
\
Real-time speech recognition model\
\
Accurate transcription in 90+ languages\
\
Real-time transcription\
\
Low latency (~150ms†)\
\
Precise word-level timestamps](https://elevenlabs.io/docs/overview/models#scribe-v2-realtime)

[Explore all](https://elevenlabs.io/docs/overview/models)

Example API response
--------------------

The following example shows the output of the Speech to Text API using the Scribe v2 model for a sample audio file.

|     |     |
| --- | --- |
| 1   | {   |
| 2   | "language\_code": "en", |
| 3   | "language\_probability": 1, |
| 4   | "text": "With a soft and whispery American accent, I'm the ideal choice for creating ASMR content, meditative guides, or adding an intimate feel to your narrative projects.", |
| 5   | "words": \[ |\
| 6   | {   |\
| 7   | "text": "With", |\
| 8   | "start": 0.119, |\
| 9   | "end": 0.259, |\
| 10  | "type": "word", |\
| 11  | "speaker\_id": "speaker\_0" |\
| 12  | },  |\
| 13  | {   |\
| 14  | "text": " ", |\
| 15  | "start": 0.239, |\
| 16  | "end": 0.299, |\
| 17  | "type": "spacing", |\
| 18  | "speaker\_id": "speaker\_0" |\
| 19  | },  |\
| 20  | {   |\
| 21  | "text": "a", |\
| 22  | "start": 0.279, |\
| 23  | "end": 0.359, |\
| 24  | "type": "word", |\
| 25  | "speaker\_id": "speaker\_0" |\
| 26  | },  |\
| 27  | {   |\
| 28  | "text": " ", |\
| 29  | "start": 0.339, |\
| 30  | "end": 0.499, |\
| 31  | "type": "spacing", |\
| 32  | "speaker\_id": "speaker\_0" |\
| 33  | },  |\
| 34  | {   |\
| 35  | "text": "soft", |\
| 36  | "start": 0.479, |\
| 37  | "end": 1.039, |\
| 38  | "type": "word", |\
| 39  | "speaker\_id": "speaker\_0" |\
| 40  | },  |\
| 41  | {   |\
| 42  | "text": " ", |\
| 43  | "start": 1.019, |\
| 44  | "end": 1.2, |\
| 45  | "type": "spacing", |\
| 46  | "speaker\_id": "speaker\_0" |\
| 47  | },  |\
| 48  | {   |\
| 49  | "text": "and", |\
| 50  | "start": 1.18, |\
| 51  | "end": 1.359, |\
| 52  | "type": "word", |\
| 53  | "speaker\_id": "speaker\_0" |\
| 54  | },  |\
| 55  | {   |\
| 56  | "text": " ", |\
| 57  | "start": 1.339, |\
| 58  | "end": 1.44, |\
| 59  | "type": "spacing", |\
| 60  | "speaker\_id": "speaker\_0" |\
| 61  | },  |\
| 62  | {   |\
| 63  | "text": "whispery", |\
| 64  | "start": 1.419, |\
| 65  | "end": 1.979, |\
| 66  | "type": "word", |\
| 67  | "speaker\_id": "speaker\_0" |\
| 68  | },  |\
| 69  | {   |\
| 70  | "text": " ", |\
| 71  | "start": 1.959, |\
| 72  | "end": 2.179, |\
| 73  | "type": "spacing", |\
| 74  | "speaker\_id": "speaker\_0" |\
| 75  | },  |\
| 76  | {   |\
| 77  | "text": "American", |\
| 78  | "start": 2.159, |\
| 79  | "end": 2.719, |\
| 80  | "type": "word", |\
| 81  | "speaker\_id": "speaker\_0" |\
| 82  | },  |\
| 83  | {   |\
| 84  | "text": " ", |\
| 85  | "start": 2.699, |\
| 86  | "end": 2.779, |\
| 87  | "type": "spacing", |\
| 88  | "speaker\_id": "speaker\_0" |\
| 89  | },  |\
| 90  | {   |\
| 91  | "text": "accent,", |\
| 92  | "start": 2.759, |\
| 93  | "end": 3.389, |\
| 94  | "type": "word", |\
| 95  | "speaker\_id": "speaker\_0" |\
| 96  | },  |\
| 97  | {   |\
| 98  | "text": " ", |\
| 99  | "start": 4.119, |\
| 100 | "end": 4.179, |\
| 101 | "type": "spacing", |\
| 102 | "speaker\_id": "speaker\_0" |\
| 103 | },  |\
| 104 | {   |\
| 105 | "text": "I'm", |\
| 106 | "start": 4.159, |\
| 107 | "end": 4.459, |\
| 108 | "type": "word", |\
| 109 | "speaker\_id": "speaker\_0" |\
| 110 | },  |\
| 111 | {   |\
| 112 | "text": " ", |\
| 113 | "start": 4.44, |\
| 114 | "end": 4.52, |\
| 115 | "type": "spacing", |\
| 116 | "speaker\_id": "speaker\_0" |\
| 117 | },  |\
| 118 | {   |\
| 119 | "text": "the", |\
| 120 | "start": 4.5, |\
| 121 | "end": 4.599, |\
| 122 | "type": "word", |\
| 123 | "speaker\_id": "speaker\_0" |\
| 124 | },  |\
| 125 | {   |\
| 126 | "text": " ", |\
| 127 | "start": 4.579, |\
| 128 | "end": 4.699, |\
| 129 | "type": "spacing", |\
| 130 | "speaker\_id": "speaker\_0" |\
| 131 | },  |\
| 132 | {   |\
| 133 | "text": "ideal", |\
| 134 | "start": 4.679, |\
| 135 | "end": 5.099, |\
| 136 | "type": "word", |\
| 137 | "speaker\_id": "speaker\_0" |\
| 138 | },  |\
| 139 | {   |\
| 140 | "text": " ", |\
| 141 | "start": 5.079, |\
| 142 | "end": 5.219, |\
| 143 | "type": "spacing", |\
| 144 | "speaker\_id": "speaker\_0" |\
| 145 | },  |\
| 146 | {   |\
| 147 | "text": "choice", |\
| 148 | "start": 5.199, |\
| 149 | "end": 5.719, |\
| 150 | "type": "word", |\
| 151 | "speaker\_id": "speaker\_0" |\
| 152 | },  |\
| 153 | {   |\
| 154 | "text": " ", |\
| 155 | "start": 5.699, |\
| 156 | "end": 6.099, |\
| 157 | "type": "spacing", |\
| 158 | "speaker\_id": "speaker\_0" |\
| 159 | },  |\
| 160 | {   |\
| 161 | "text": "for", |\
| 162 | "start": 6.099, |\
| 163 | "end": 6.199, |\
| 164 | "type": "word", |\
| 165 | "speaker\_id": "speaker\_0" |\
| 166 | },  |\
| 167 | {   |\
| 168 | "text": " ", |\
| 169 | "start": 6.179, |\
| 170 | "end": 6.279, |\
| 171 | "type": "spacing", |\
| 172 | "speaker\_id": "speaker\_0" |\
| 173 | },  |\
| 174 | {   |\
| 175 | "text": "creating", |\
| 176 | "start": 6.259, |\
| 177 | "end": 6.799, |\
| 178 | "type": "word", |\
| 179 | "speaker\_id": "speaker\_0" |\
| 180 | },  |\
| 181 | {   |\
| 182 | "text": " ", |\
| 183 | "start": 6.779, |\
| 184 | "end": 6.979, |\
| 185 | "type": "spacing", |\
| 186 | "speaker\_id": "speaker\_0" |\
| 187 | },  |\
| 188 | {   |\
| 189 | "text": "ASMR", |\
| 190 | "start": 6.959, |\
| 191 | "end": 7.739, |\
| 192 | "type": "word", |\
| 193 | "speaker\_id": "speaker\_0" |\
| 194 | },  |\
| 195 | {   |\
| 196 | "text": " ", |\
| 197 | "start": 7.719, |\
| 198 | "end": 7.859, |\
| 199 | "type": "spacing", |\
| 200 | "speaker\_id": "speaker\_0" |\
| 201 | },  |\
| 202 | {   |\
| 203 | "text": "content,", |\
| 204 | "start": 7.839, |\
| 205 | "end": 8.45, |\
| 206 | "type": "word", |\
| 207 | "speaker\_id": "speaker\_0" |\
| 208 | },  |\
| 209 | {   |\
| 210 | "text": " ", |\
| 211 | "start": 9, |\
| 212 | "end": 9.06, |\
| 213 | "type": "spacing", |\
| 214 | "speaker\_id": "speaker\_0" |\
| 215 | },  |\
| 216 | {   |\
| 217 | "text": "meditative", |\
| 218 | "start": 9.04, |\
| 219 | "end": 9.64, |\
| 220 | "type": "word", |\
| 221 | "speaker\_id": "speaker\_0" |\
| 222 | },  |\
| 223 | {   |\
| 224 | "text": " ", |\
| 225 | "start": 9.619, |\
| 226 | "end": 9.699, |\
| 227 | "type": "spacing", |\
| 228 | "speaker\_id": "speaker\_0" |\
| 229 | },  |\
| 230 | {   |\
| 231 | "text": "guides,", |\
| 232 | "start": 9.679, |\
| 233 | "end": 10.359, |\
| 234 | "type": "word", |\
| 235 | "speaker\_id": "speaker\_0" |\
| 236 | },  |\
| 237 | {   |\
| 238 | "text": " ", |\
| 239 | "start": 10.359, |\
| 240 | "end": 10.409, |\
| 241 | "type": "spacing", |\
| 242 | "speaker\_id": "speaker\_0" |\
| 243 | },  |\
| 244 | {   |\
| 245 | "text": "or", |\
| 246 | "start": 11.319, |\
| 247 | "end": 11.439, |\
| 248 | "type": "word", |\
| 249 | "speaker\_id": "speaker\_0" |\
| 250 | },  |\
| 251 | {   |\
| 252 | "text": " ", |\
| 253 | "start": 11.42, |\
| 254 | "end": 11.52, |\
| 255 | "type": "spacing", |\
| 256 | "speaker\_id": "speaker\_0" |\
| 257 | },  |\
| 258 | {   |\
| 259 | "text": "adding", |\
| 260 | "start": 11.5, |\
| 261 | "end": 11.879, |\
| 262 | "type": "word", |\
| 263 | "speaker\_id": "speaker\_0" |\
| 264 | },  |\
| 265 | {   |\
| 266 | "text": " ", |\
| 267 | "start": 11.859, |\
| 268 | "end": 12, |\
| 269 | "type": "spacing", |\
| 270 | "speaker\_id": "speaker\_0" |\
| 271 | },  |\
| 272 | {   |\
| 273 | "text": "an", |\
| 274 | "start": 11.979, |\
| 275 | "end": 12.079, |\
| 276 | "type": "word", |\
| 277 | "speaker\_id": "speaker\_0" |\
| 278 | },  |\
| 279 | {   |\
| 280 | "text": " ", |\
| 281 | "start": 12.059, |\
| 282 | "end": 12.179, |\
| 283 | "type": "spacing", |\
| 284 | "speaker\_id": "speaker\_0" |\
| 285 | },  |\
| 286 | {   |\
| 287 | "text": "intimate", |\
| 288 | "start": 12.179, |\
| 289 | "end": 12.579, |\
| 290 | "type": "word", |\
| 291 | "speaker\_id": "speaker\_0" |\
| 292 | },  |\
| 293 | {   |\
| 294 | "text": " ", |\
| 295 | "start": 12.559, |\
| 296 | "end": 12.699, |\
| 297 | "type": "spacing", |\
| 298 | "speaker\_id": "speaker\_0" |\
| 299 | },  |\
| 300 | {   |\
| 301 | "text": "feel", |\
| 302 | "start": 12.679, |\
| 303 | "end": 13.159, |\
| 304 | "type": "word", |\
| 305 | "speaker\_id": "speaker\_0" |\
| 306 | },  |\
| 307 | {   |\
| 308 | "text": " ", |\
| 309 | "start": 13.139, |\
| 310 | "end": 13.179, |\
| 311 | "type": "spacing", |\
| 312 | "speaker\_id": "speaker\_0" |\
| 313 | },  |\
| 314 | {   |\
| 315 | "text": "to", |\
| 316 | "start": 13.159, |\
| 317 | "end": 13.26, |\
| 318 | "type": "word", |\
| 319 | "speaker\_id": "speaker\_0" |\
| 320 | },  |\
| 321 | {   |\
| 322 | "text": " ", |\
| 323 | "start": 13.239, |\
| 324 | "end": 13.3, |\
| 325 | "type": "spacing", |\
| 326 | "speaker\_id": "speaker\_0" |\
| 327 | },  |\
| 328 | {   |\
| 329 | "text": "your", |\
| 330 | "start": 13.299, |\
| 331 | "end": 13.399, |\
| 332 | "type": "word", |\
| 333 | "speaker\_id": "speaker\_0" |\
| 334 | },  |\
| 335 | {   |\
| 336 | "text": " ", |\
| 337 | "start": 13.379, |\
| 338 | "end": 13.479, |\
| 339 | "type": "spacing", |\
| 340 | "speaker\_id": "speaker\_0" |\
| 341 | },  |\
| 342 | {   |\
| 343 | "text": "narrative", |\
| 344 | "start": 13.479, |\
| 345 | "end": 13.889, |\
| 346 | "type": "word", |\
| 347 | "speaker\_id": "speaker\_0" |\
| 348 | },  |\
| 349 | {   |\
| 350 | "text": " ", |\
| 351 | "start": 13.919, |\
| 352 | "end": 13.939, |\
| 353 | "type": "spacing", |\
| 354 | "speaker\_id": "speaker\_0" |\
| 355 | },  |\
| 356 | {   |\
| 357 | "text": "projects.", |\
| 358 | "start": 13.919, |\
| 359 | "end": 14.779, |\
| 360 | "type": "word", |\
| 361 | "speaker\_id": "speaker\_0" |\
| 362 | }   |\
| 363 | \]  |
| 364 | }   |

The output is classified in three category types:

*   `word` - A word in the language of the audio
*   `spacing` - The space between words, not applicable for languages that don’t use spaces like Japanese, Mandarin, Thai, Lao, Burmese and Cantonese
*   `audio_event` - Non-speech sounds like laughter or applause

Concurrency and priority
------------------------

Concurrency is the concept of how many requests can be processed at the same time.

For Speech to Text, files that are over 8 minutes long are transcribed in parallel internally in order to speed up processing. The audio is chunked into four segments to be transcribed concurrently.

You can calculate the concurrency limit with the following calculation:

Concurrency\=min⁡(4,round\_up(audio\_duration\_secs480))Concurrency = \\min(4, \\text{round\\\_up}(\\frac{\\text{audio\\\_duration\\\_secs}}{480}))Concurrency\=min(4,round\_up(480audio\_duration\_secs​))

For example, a 15 minute audio file will be transcribed with a concurrency of 2, while a 120 minute audio file will be transcribed with a concurrency of 4.

The above calculation is only applicable to Scribe v1 and v2. For Scribe v2 Realtime, see the [concurrency limit chart](https://elevenlabs.io/docs/overview/models#concurrency-and-priority)
.

Advanced features
-----------------

Keyterm prompting and entity detection come at an additional cost. See the [API pricing page](https://elevenlabs.io/pricing?price.section=speech_to_text&price.sections=speech_to_text,speech_to_text#pricing-table)
 for detailed pricing information.

### Keyterm prompting

Keyterm prompting is only available with the Scribe v2 model.

Highlight up to 100 words or phrases to bias the model towards transcribing them. This is useful for transcribing specific words or sentences that are not common in the audio, such as product names, names, or other specific terms. Keyterms are more powerful than biased keywords or customer vocabularies offered by other models, because it relies on the context to decide whether to transcribe that term or not.

To learn more about how to use keyterm prompting, see the [keyterm prompting documentation](https://elevenlabs.io/docs/developers/guides/cookbooks/speech-to-text/batch/keyterm-prompting)
.

### Entity detection

Scribe v2 can detect several categories of entities in the transcript, providing their exact timestamps. This is useful to highlight credit card numbers, names, medical conditions or SSNs.

For a full list of supported entities, see the [entity detection documentation](https://elevenlabs.io/docs/developers/guides/cookbooks/speech-to-text/batch/entity-detection)
.

Supported languages
-------------------

The Scribe v1 and v2 models support 90+ languages, including:

_Afrikaans (afr), Amharic (amh), Arabic (ara), Armenian (hye), Assamese (asm), Asturian (ast), Azerbaijani (aze), Belarusian (bel), Bengali (ben), Bosnian (bos), Bulgarian (bul), Burmese (mya), Cantonese (yue), Catalan (cat), Cebuano (ceb), Chichewa (nya), Croatian (hrv), Czech (ces), Danish (dan), Dutch (nld), English (eng), Estonian (est), Filipino (fil), Finnish (fin), French (fra), Fulah (ful), Galician (glg), Ganda (lug), Georgian (kat), German (deu), Greek (ell), Gujarati (guj), Hausa (hau), Hebrew (heb), Hindi (hin), Hungarian (hun), Icelandic (isl), Igbo (ibo), Indonesian (ind), Irish (gle), Italian (ita), Japanese (jpn), Javanese (jav), Kabuverdianu (kea), Kannada (kan), Kazakh (kaz), Khmer (khm), Korean (kor), Kurdish (kur), Kyrgyz (kir), Lao (lao), Latvian (lav), Lingala (lin), Lithuanian (lit), Luo (luo), Luxembourgish (ltz), Macedonian (mkd), Malay (msa), Malayalam (mal), Maltese (mlt), Mandarin Chinese (zho), Māori (mri), Marathi (mar), Mongolian (mon), Nepali (nep), Northern Sotho (nso), Norwegian (nor), Occitan (oci), Odia (ori), Pashto (pus), Persian (fas), Polish (pol), Portuguese (por), Punjabi (pan), Romanian (ron), Russian (rus), Serbian (srp), Shona (sna), Sindhi (snd), Slovak (slk), Slovenian (slv), Somali (som), Spanish (spa), Swahili (swa), Swedish (swe), Tamil (tam), Tajik (tgk), Telugu (tel), Thai (tha), Turkish (tur), Ukrainian (ukr), Umbundu (umb), Urdu (urd), Uzbek (uzb), Vietnamese (vie), Welsh (cym), Wolof (wol), Xhosa (xho) and Zulu (zul)._

### Breakdown of language support

Word Error Rate (WER) is a key metric used to evaluate the accuracy of transcription systems. It measures how many errors are present in a transcript compared to a reference transcript. Below is a breakdown of the WER for each language that Scribe v1 and v2 support.

###### Excellent (≤ 5% WER)

Belarusian (bel), Bosnian (bos), Bulgarian (bul), Catalan (cat), Croatian (hrv), Czech (ces), Danish (dan), Dutch (nld), English (eng), Estonian (est), Finnish (fin), French (fra), Galician (glg), German (deu), Greek (ell), Hungarian (hun), Icelandic (isl), Indonesian (ind), Italian (ita), Japanese (jpn), Kannada (kan), Latvian (lav), Macedonian (mkd), Malay (msa), Malayalam (mal), Norwegian (nor), Polish (pol), Portuguese (por), Romanian (ron), Russian (rus), Slovak (slk), Spanish (spa), Swedish (swe), Turkish (tur), Ukrainian (ukr) and Vietnamese (vie).

###### High Accuracy (>5% to ≤10% WER)

Armenian (hye), Azerbaijani (aze), Bengali (ben), Cantonese (yue), Filipino (fil), Georgian (kat), Gujarati (guj), Hindi (hin), Kazakh (kaz), Lithuanian (lit), Maltese (mlt), Mandarin (cmn), Marathi (mar), Nepali (nep), Odia (ori), Persian (fas), Serbian (srp), Slovenian (slv), Swahili (swa), Tamil (tam) and Telugu (tel)

###### Good (>10% to ≤20% WER)

Afrikaans (afr), Arabic (ara), Assamese (asm), Asturian (ast), Burmese (mya), Hausa (hau), Hebrew (heb), Javanese (jav), Korean (kor), Kyrgyz (kir), Luxembourgish (ltz), Māori (mri), Occitan (oci), Punjabi (pan), Tajik (tgk), Thai (tha), Uzbek (uzb) and Welsh (cym).

###### Moderate (>25% to ≤50% WER)

Amharic (amh), Ganda (lug), Igbo (ibo), Irish (gle), Khmer (khm), Kurdish (kur), Lao (lao), Mongolian (mon), Northern Sotho (nso), Pashto (pus), Shona (sna), Sindhi (snd), Somali (som), Urdu (urd), Wolof (wol), Xhosa (xho), Yoruba (yor) and Zulu (zul).

FAQ
---

###### Can I use speech to text API with video files?

Yes, the API supports uploading both audio and video files for transcription.

###### What are the file size and duration limits for the Speech to Text API?

Files up to 3 GB in size and up to 10 hours in duration are supported.

###### Which audio and video formats are supported in the API?

The API supports the following audio and video formats:

*   audio/aac
*   audio/x-aac
*   audio/x-aiff
*   audio/ogg
*   audio/mpeg
*   audio/mp3
*   audio/mpeg3
*   audio/x-mpeg-3
*   audio/opus
*   audio/wav
*   audio/x-wav
*   audio/webm
*   audio/flac
*   audio/x-flac
*   audio/mp4
*   audio/aiff
*   audio/x-m4a

Supported video formats include:

*   video/mp4
*   video/x-msvideo
*   video/x-matroska
*   video/quicktime
*   video/x-ms-wmv
*   video/x-flv
*   video/webm
*   video/mpeg
*   video/3gpp

###### When will you support more languages?

ElevenLabs is constantly expanding the number of languages supported by our models. Please check back frequently for updates.

###### Does speech to text API support webhooks?

Yes, asynchronous transcription results can be sent to webhooks configured in webhook settings in the UI. Learn more in the [webhooks cookbook](https://elevenlabs.io/docs/developers/guides/cookbooks/speech-to-text/webhooks)
.

###### Is a multichannel transcription mode supported in the API?

Yes, the multichannel [STT](https://elevenlabs.io/speech-to-text)
 feature allows you to transcribe audio where each channel is processed independently and assigned a speaker ID based on its channel number. This feature supports up to 5 channels. Learn more in the [multichannel transcription cookbook](https://elevenlabs.io/docs/developers/guides/cookbooks/speech-to-text/multichannel-transcription)
.

###### How does billing work for the speech to text API?

ElevenLabs charges for speech to text based on the duration of the audio sent for transcription. Billing is calculated per hour of audio, with rates varying by tier and model. See the [API pricing page](https://elevenlabs.io/pricing/api?price.section=speech_to_text#pricing-table)
 for detailed pricing information.

## Document 52 — Text to Dialogue | ElevenLabs Documentation {#doc-52}
[https://elevenlabs.io/docs/overview/capabilities/text-to-dialogue](https://elevenlabs.io/docs/overview/capabilities/text-to-dialogue)

Overview
--------

The ElevenLabs [Text to Dialogue](https://elevenlabs.io/docs/api-reference/text-to-dialogue/convert)
 API creates natural sounding expressive dialogue from text using the Eleven v3 model. Popular use cases include:

*   Generating pitch perfect conversations for video games
*   Creating immersive dialogue for podcasts and other audio content
*   Bring audiobooks to life with expressive narration

Text to Dialogue is not intended for use in real-time applications like conversational agents. Several generations might be required to achieve the desired results. When integrating Text to Dialogue into your application, consider generating several generations and allowing the user to select the best one.

Listen to a sample:

[Developers\
\
Learn how to integrate text to dialogue into your application.](https://elevenlabs.io/docs/developers/guides/cookbooks/text-to-dialogue)
[Prompting guide\
\
Learn how to use the Eleven v3 model to generate expressive dialogue.](https://elevenlabs.io/docs/overview/capabilities/text-to-speech/best-practices#prompting-eleven-v3-alpha)

Voice options
-------------

ElevenLabs offers thousands of voices across 70+ languages through multiple creation methods:

*   [Voice library](https://elevenlabs.io/docs/overview/capabilities/voices)
     with 3,000+ community-shared voices
*   [Professional voice cloning](https://elevenlabs.io/docs/overview/capabilities/voices#cloned)
     for highest-fidelity replicas
*   [Instant voice cloning](https://elevenlabs.io/docs/overview/capabilities/voices#cloned)
     for quick voice replication
*   [Voice design](https://elevenlabs.io/docs/overview/capabilities/voices#voice-design)
     to generate custom voices from text descriptions

Learn more about our [voice options](https://elevenlabs.io/docs/overview/capabilities/voices)
.

Prompting
---------

The models interpret emotional context directly from the text input. For example, adding descriptive text like “she said excitedly” or using exclamation marks will influence the speech emotion. Voice settings like Stability and Similarity help control the consistency, while the underlying emotion comes from textual cues.

Read the [prompting guide](https://elevenlabs.io/docs/overview/capabilities/text-to-speech/best-practices#prompting-eleven-v3-alpha)
 for more details.

### Emotional deliveries with audio tags

This feature is still under active development, actual results may vary.

The Eleven v3 model allows the use of non-speech audio events to influence the delivery of the dialogue. This is done by inserting the audio events into the text input wrapped in square brackets.

Audio tags come in a few different forms:

### Emotions and delivery

For example, \[sad\], \[laughing\] and \[whispering\]

### Audio events

For example, \[leaves rustling\], \[gentle footsteps\] and \[applause\].

### Overall direction

For example, \[football\], \[wrestling match\] and \[auctioneer\].

Some examples include:

|     |
| --- |
| "\[giggling\] That's really funny!" |
| "\[groaning\] That was awful." |
| "Well, \[sigh\] I'm not sure what to say." |

You can also use punctuation to indicate the flow of dialog, like interruptions:

|     |
| --- |
| "\[cautiously\] Hello, is this seat-" |
| "\[jumping in\] Free? \[cheerfully\] Yes it is." |

Ellipses can be used to indicate trailing sentences:

|     |
| --- |
| "\[indecisive\] Hi, can I get uhhh..." |
| "\[quizzically\] The usual?" |
| "\[elated\] Yes! \[laughs\] I'm so glad you knew!" |

Supported formats
-----------------

The default response format is “mp3”, but other formats like “PCM”, & “μ-law” are available.

*   **MP3**
    *   Sample rates: 22.05kHz - 44.1kHz
    *   Bitrates: 32kbps - 192kbps
    *   22.05kHz @ 32kbps
    *   44.1kHz @ 32kbps, 64kbps, 96kbps, 128kbps, 192kbps
*   **PCM (S16LE)**
    *   Sample rates: 16kHz - 44.1kHz
    *   Bitrates: 8kHz, 16kHz, 22.05kHz, 24kHz, 44.1kHz, 48kHz
    *   16-bit depth
*   **μ-law**
    *   8kHz sample rate
    *   Optimized for telephony applications
*   **A-law**
    *   8kHz sample rate
    *   Optimized for telephony applications
*   **Opus**
    *   Sample rate: 48kHz
    *   Bitrates: 32kbps - 192kbps

Higher quality audio options are only available on paid tiers - see our [pricing page](https://elevenlabs.io/pricing/api)
 for details.

Supported languages
-------------------

The Eleven v3 model supports 70+ languages, including:

_Afrikaans (afr), Arabic (ara), Armenian (hye), Assamese (asm), Azerbaijani (aze), Belarusian (bel), Bengali (ben), Bosnian (bos), Bulgarian (bul), Catalan (cat), Cebuano (ceb), Chichewa (nya), Croatian (hrv), Czech (ces), Danish (dan), Dutch (nld), English (eng), Estonian (est), Filipino (fil), Finnish (fin), French (fra), Galician (glg), Georgian (kat), German (deu), Greek (ell), Gujarati (guj), Hausa (hau), Hebrew (heb), Hindi (hin), Hungarian (hun), Icelandic (isl), Indonesian (ind), Irish (gle), Italian (ita), Japanese (jpn), Javanese (jav), Kannada (kan), Kazakh (kaz), Kirghiz (kir), Korean (kor), Latvian (lav), Lingala (lin), Lithuanian (lit), Luxembourgish (ltz), Macedonian (mkd), Malay (msa), Malayalam (mal), Mandarin Chinese (cmn), Marathi (mar), Nepali (nep), Norwegian (nor), Pashto (pus), Persian (fas), Polish (pol), Portuguese (por), Punjabi (pan), Romanian (ron), Russian (rus), Serbian (srp), Sindhi (snd), Slovak (slk), Slovenian (slv), Somali (som), Spanish (spa), Swahili (swa), Swedish (swe), Tamil (tam), Telugu (tel), Thai (tha), Turkish (tur), Ukrainian (ukr), Urdu (urd), Vietnamese (vie), Welsh (cym)._

FAQ
---

###### Which models can I use?

Text to Dialogue is only available on the Eleven v3 model.

###### Do I own the audio output?

Yes. You retain ownership of any audio you generate. However, commercial usage rights are only available with paid plans. With a paid subscription, you may use generated audio for commercial purposes and monetize the outputs if you own the IP rights to the input content.

###### What qualifies as a free regeneration?

A free regeneration allows you to regenerate the same text to speech content without additional cost, subject to these conditions:

*   Only available within the ElevenLabs dashboard.
*   You can regenerate each piece of content up to 2 times for free.
*   The content must be exactly the same as the previous generation. Any changes to the text, voice settings, or other parameters will require a new, paid generation.

Free regenerations are useful in case there is a slight distortion in the audio output. According to ElevenLabs’ internal benchmarks, regenerations will solve roughly half of issues with quality, with remaining issues usually due to poor training data.

###### How many speakers can my dialogue have?

There is no limit to the number of speakers in a dialogue.

###### Why is my output sometimes inconsistent?

The models are nondeterministic. For consistency, use the optional [seed parameter](https://elevenlabs.io/docs/api-reference/text-to-speech/convert#request.body.seed)
, though subtle differences may still occur.

###### What's the best practice for large text conversions?

Split long text into segments and use streaming for real-time playback and efficient processing.

## Document 53 — Text to Speech | ElevenLabs Documentation {#doc-53}
[https://elevenlabs.io/docs/overview/capabilities/text-to-speech](https://elevenlabs.io/docs/overview/capabilities/text-to-speech)

Overview
--------

ElevenLabs [Text to Speech (TTS)](https://elevenlabs.io/docs/api-reference/text-to-speech/convert)
 API turns text into lifelike audio with nuanced intonation, pacing and emotional awareness. [Our models](https://elevenlabs.io/docs/overview/models)
 adapt to textual cues across 32 languages and multiple voice styles and can be used to:

*   Narrate global media campaigns & ads
*   Produce audiobooks in multiple languages with complex emotional delivery
*   Stream real-time audio from text

Listen to a sample:

Explore our [voice library](https://elevenlabs.io/app/voice-library)
 to find the perfect voice for your project.

[Products\
\
Step-by-step guide for using text to speech in ElevenLabs.](https://elevenlabs.io/docs/creative-platform/playground/text-to-speech)
[Developers\
\
Learn how to integrate text to speech into your application.](https://elevenlabs.io/docs/developers/quickstart)

### Voice quality

For real-time applications, Flash v2.5 provides ultra-low 75ms latency, while Multilingual v2 delivers the highest quality audio with more nuanced expression.

[Eleven v3\
\
![Alpha](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/df15d6e71021753b609c6d9f7cf3dcbff076208e67ee96a5849b3476ccf8915b/assets/icons/alpha.svg)\
\
Our most emotionally rich, expressive speech synthesis model\
\
Dramatic delivery and performance\
\
70+ languages supported\
\
5,000 character limit\
\
Support for natural multi-speaker dialogue](https://elevenlabs.io/docs/overview/models#eleven-v3-alpha)
[Eleven Multilingual v2\
\
Lifelike, consistent quality speech synthesis model\
\
Natural-sounding output\
\
29 languages supported\
\
10,000 character limit\
\
Most stable on long-form generations](https://elevenlabs.io/docs/overview/models#multilingual-v2)
[Eleven Flash v2.5\
\
Our fast, affordable speech synthesis model\
\
Ultra-low latency (~75ms†)\
\
32 languages supported\
\
40,000 character limit\
\
Faster model, 50% lower price per character](https://elevenlabs.io/docs/overview/models#flash-v25)
[Eleven Turbo v2.5\
\
High quality, low-latency model with a good balance of quality and speed\
\
High quality voice generation\
\
32 languages supported\
\
40,000 character limit\
\
Low latency (~250ms-300ms†), 50% lower price per character](https://elevenlabs.io/docs/overview/models#turbo-v25)

[Explore all](https://elevenlabs.io/docs/overview/models)

### Voice options

ElevenLabs offers thousands of voices across 32 languages through multiple creation methods:

*   [Voice library](https://elevenlabs.io/docs/overview/capabilities/voices)
     with 3,000+ community-shared voices
*   [Professional voice cloning](https://elevenlabs.io/docs/overview/capabilities/voices#cloned)
     for highest-fidelity replicas
*   [Instant voice cloning](https://elevenlabs.io/docs/overview/capabilities/voices#cloned)
     for quick voice replication
*   [Voice design](https://elevenlabs.io/docs/overview/capabilities/voices#voice-design)
     to generate custom voices from text descriptions

Learn more about our [voice options](https://elevenlabs.io/docs/overview/capabilities/voices)
.

### Supported formats

The default response format is “mp3”, but other formats like “PCM”, & “μ-law” are available.

*   **MP3**
    *   Sample rates: 22.05kHz - 44.1kHz
    *   Bitrates: 32kbps - 192kbps
    *   22.05kHz @ 32kbps
    *   44.1kHz @ 32kbps, 64kbps, 96kbps, 128kbps, 192kbps
*   **PCM (S16LE)**
    *   Sample rates: 16kHz - 44.1kHz
    *   Bitrates: 8kHz, 16kHz, 22.05kHz, 24kHz, 44.1kHz, 48kHz
    *   16-bit depth
*   **μ-law**
    *   8kHz sample rate
    *   Optimized for telephony applications
*   **A-law**
    *   8kHz sample rate
    *   Optimized for telephony applications
*   **Opus**
    *   Sample rate: 48kHz
    *   Bitrates: 32kbps - 192kbps

Higher quality audio options are only available on paid tiers - see our [pricing page](https://elevenlabs.io/pricing/api)
 for details.

### Supported languages

Our multilingual v2 models support 29 languages:

_English (USA, UK, Australia, Canada), Japanese, Chinese, German, Hindi, French (France, Canada), Korean, Portuguese (Brazil, Portugal), Italian, Spanish (Spain, Mexico), Indonesian, Dutch, Turkish, Filipino, Polish, Swedish, Bulgarian, Romanian, Arabic (Saudi Arabia, UAE), Czech, Greek, Finnish, Croatian, Malay, Slovak, Danish, Tamil, Ukrainian & Russian._

Flash v2.5 supports 32 languages - all languages from v2 models plus:

_Hungarian, Norwegian & Vietnamese_

Simply input text in any of our supported languages and select a matching voice from our [voice library](https://elevenlabs.io/app/voice-library)
. For the most natural results, choose a voice with an accent that matches your target language and region.

### Prompting

The models interpret emotional context directly from the text input. For example, adding descriptive text like “she said excitedly” or using exclamation marks will influence the speech emotion. Voice settings like Stability and Similarity help control the consistency, while the underlying emotion comes from textual cues.

Read the [prompting guide](https://elevenlabs.io/docs/overview/capabilities/text-to-speech/best-practices)
 for more details.

Descriptive text will be spoken out by the model and must be manually trimmed or removed from the audio if desired.

FAQ
---

###### Can I clone my own voice?

Yes, you can create [instant voice clones](https://elevenlabs.io/docs/overview/capabilities/voices#cloned)
 of your own voice from short audio clips. For high-fidelity clones, check out our [professional voice cloning](https://elevenlabs.io/docs/overview/capabilities/voices#cloned)
 feature.

###### Do I own the audio output?

Yes. You retain ownership of any audio you generate. However, commercial usage rights are only available with paid plans. With a paid subscription, you may use generated audio for commercial purposes and monetize the outputs if you own the IP rights to the input content.

###### What qualifies as a free regeneration?

A free regeneration allows you to regenerate the same text to speech content without additional cost, subject to these conditions:

*   You can regenerate each piece of content up to 2 times for free
*   The content must be exactly the same as the previous generation. Any changes to the text, voice settings, or other parameters will require a new, paid generation

Free regenerations are useful in case there is a slight distortion in the audio output. According to ElevenLabs’ internal benchmarks, regenerations will solve roughly half of issues with quality, with remaining issues usually due to poor training data.

###### How do I reduce latency for real-time cases?

Use the low-latency Flash [models](https://elevenlabs.io/docs/overview/models)
 (Flash v2 or v2.5) optimized for near real-time conversational or interactive scenarios. See our [latency optimization guide](https://elevenlabs.io/docs/developers/best-practices/latency-optimization)
 for more details.

###### Why is my output sometimes inconsistent?

The models are nondeterministic. For consistency, use the optional [seed parameter](https://elevenlabs.io/docs/api-reference/text-to-speech/convert#request.body.seed)
, though subtle differences may still occur.

###### What's the best practice for large text conversions?

Split long text into segments and use streaming for real-time playback and efficient processing. To maintain natural prosody flow between chunks, include [previous/next text or previous/next request id parameters](https://elevenlabs.io/docs/api-reference/text-to-speech/convert#request.body.previous_text)
.

## Document 54 — Best practices | ElevenLabs Documentation {#doc-54}
[https://elevenlabs.io/docs/overview/capabilities/text-to-speech/best-practices](https://elevenlabs.io/docs/overview/capabilities/text-to-speech/best-practices)

This guide provides techniques to enhance text-to-speech outputs using ElevenLabs models. Experiment with these methods to discover what works best for your needs.

Controls
--------

We are actively working on _Director’s Mode_ to give you even greater control over outputs.

These techniques provide a practical way to achieve nuanced results until advanced features like _Director’s Mode_ are rolled out.

### Pauses

Eleven v3 does not support SSML break tags. Use the techniques described in the [Prompting Eleven v3 (alpha)](https://elevenlabs.io/docs/overview/capabilities/text-to-speech/best-practices#prompting-eleven-v3-alpha)
 section for controlling pauses with v3.

Use `<break time="x.xs" />` for natural pauses up to 3 seconds.

Using too many break tags in a single generation can cause instability. The AI might speed up, or introduce additional noises or audio artifacts. We are working on resolving this.

Example

"Hold on, let me think." <break time="1.5s" /> "Alright, I've got it."

*   **Consistency:** Use `<break>` tags consistently to maintain natural speech flow. Excessive use can lead to instability.
*   **Voice-Specific Behavior:** Different voices may handle pauses differently, especially those trained with filler sounds like “uh” or “ah.”

Alternatives to `<break>` include dashes (- or —) for short pauses or ellipses (…) for hesitant tones. However, these are less consistent.

Example

"It… well, it might work." "Wait — what's that noise?"

### Pronunciation

#### Phoneme Tags

Specify pronunciation using [SSML phoneme tags](https://en.wikipedia.org/wiki/Speech_Synthesis_Markup_Language)
. Supported alphabets include [CMU](https://en.wikipedia.org/wiki/CMU_Pronouncing_Dictionary)
 Arpabet and the [International Phonetic Alphabet (IPA)](https://en.wikipedia.org/wiki/International_Phonetic_Alphabet)
.

Phoneme tags are only compatible with “Eleven Flash v2”, “Eleven Turbo v2” and “Eleven English v1” [models](https://elevenlabs.io/docs/overview/models)
.

CMU Arpabet ExampleIPA Example

|     |     |
| --- | --- |
| 1   | <phoneme alphabet="cmu-arpabet" ph="M AE1 D IH0 S AH0 N"> |
| 2   | Madison |
| 3   | </phoneme> |

We recommend using CMU Arpabet for consistent and predictable results with current AI models. While IPA can be effective, CMU Arpabet generally offers more reliable performance.

Phoneme tags only work for individual words. If for example you have a name with a first and last name that you want to be pronounced a certain way, you will need to create a phoneme tag for each word.

Ensure correct stress marking for multi-syllable words to maintain accurate pronunciation. For example:

Correct usageIncorrect usage

|     |     |
| --- | --- |
| 1   | <phoneme alphabet="cmu-arpabet" ph="P R AH0 N AH0 N S IY EY1 SH AH0 N"> |
| 2   | pronunciation |
| 3   | </phoneme> |

#### Alias Tags

For models that don’t support phoneme tags, you can try writing words more phonetically. You can also employ various tricks such as capital letters, dashes, apostrophes, or even single quotation marks around a single letter or letters.

As an example, a word like “trapezii” could be spelt “trapezIi” to put more emphasis on the “ii” of the word.

You can either replace the word directly in your text, or if you want to specify pronunciation using other words or phrases when using a pronunciation dictionary, you can use alias tags for this. This can be useful if you’re generating using Multilingual v2 or Turbo v2.5, which don’t support phoneme tags. You can use pronunciation dictionaries with Studio, Dubbing Studio and Speech Synthesis via the API.

For example, if your text includes a name that has an unusual pronunciation that the AI might struggle with, you could use an alias tag to specify how you would like it to be pronounced:

|     |
| --- |
| <lexeme> |
| <grapheme>Claughton</grapheme> |
| <alias>Cloffton</alias> |
| </lexeme> |

If you want to make sure that an acronym is always delivered in a certain way whenever it is incountered in your text, you can use an alias tag to specify this:

|     |
| --- |
| <lexeme> |
| <grapheme>UN</grapheme> |
| <alias>United Nations</alias> |
| </lexeme> |

#### Pronunciation Dictionaries

Some of our tools, such as Studio and Dubbing Studio, allow you to create and upload a pronunciation dictionary. These allow you to specify the pronunciation of certain words, such as character or brand names, or to specify how acronyms should be read.

Pronunciation dictionaries allow this functionality by enabling you to upload a lexicon or dictionary file that specifies pairs of words and how they should be pronounced, either using a phonetic alphabet or word substitutions.

Whenever one of these words is encountered in a project, the AI model will pronounce the word using the specified replacement.

To provide a pronunciation dictionary file, open the settings for a project and upload a file in either TXT or the [.PLS format](https://www.w3.org/TR/pronunciation-lexicon/)
. When a dictionary is added to a project it will automatically recalculate which pieces of the project will need to be re-converted using the new dictionary file and mark these as unconverted.

Currently we only support pronunciation dictionaries that specify replacements using phoneme or alias tags.

Both phonemes and aliases are sets of rules that specify a word or phrase they are looking for, referred to as a grapheme, and what it will be replaced with. Please note that searches are case sensitive. When checking for a replacement word in a pronunciation dictionary, the dictionary is checked from start to end and only the very first replacement is used.

#### Pronunciation Dictionary examples

Here are examples of pronunciation dictionaries in both CMU Arpabet and IPA, including a phoneme to specify the pronunciation of “Apple” and an alias to replace “UN” with “United Nations”:

CMU Arpabet ExampleIPA Example

|     |     |
| --- | --- |
| 1   | <?xml version="1.0" encoding="UTF-8"?> |
| 2   | <lexicon version="1.0" |
| 3   | xmlns="http://www.w3.org/2005/01/pronunciation-lexicon" |
| 4   | xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" |
| 5   | xsi:schemaLocation="http://www.w3.org/2005/01/pronunciation-lexicon |
| 6   | http://www.w3.org/TR/2007/CR-pronunciation-lexicon-20071212/pls.xsd" |
| 7   | alphabet="cmu-arpabet" xml:lang="en-GB"> |
| 8   | <lexeme> |
| 9   | <grapheme>apple</grapheme> |
| 10  | <phoneme>AE P AH L</phoneme> |
| 11  | </lexeme> |
| 12  | <lexeme> |
| 13  | <grapheme>UN</grapheme> |
| 14  | <alias>United Nations</alias> |
| 15  | </lexeme> |
| 16  | </lexicon> |

To generate a pronunciation dictionary `.pls` file, there are a few open source tools available:

*   [Sequitur G2P](https://github.com/sequitur-g2p/sequitur-g2p)
     - Open-source tool that learns pronunciation rules from data and can generate phonetic transcriptions.
*   [Phonetisaurus](https://github.com/AdolfVonKleist/Phonetisaurus)
     - Open-source G2P system trained on existing dictionaries like CMUdict.
*   [eSpeak](https://github.com/espeak-ng/espeak-ng)
     - Speech synthesizer that can generate phoneme transcriptions from text.
*   [CMU Pronouncing Dictionary](https://github.com/cmusphinx/cmudict)
     - A pre-built English dictionary with phonetic transcriptions.

### Emotion

Convey emotions through narrative context or explicit dialogue tags. This approach helps the AI understand the tone and emotion to emulate.

Example

You're leaving?" she asked, her voice trembling with sadness. "That's it!" he exclaimed triumphantly.

Explicit dialogue tags yield more predictable results than relying solely on context, however the model will still speak out the emotional delivery guides. These can be removed in post-production using an audio editor if unwanted.

### Pace

The pacing of the audio is highly influenced by the audio used to create the voice. When creating your voice, we recommend using longer, continuous samples to avoid pacing issues like unnaturally fast speech.

For control over the speed of the generated audio, you can use the speed setting. This allows you to either speed up or slow down the speed of the generated speech. The speed setting is available in Text to Speech via the website and API, as well as in Studio and Agents Platform. It can be found in the voice settings.

The default value is 1.0, which means that the speed is not adjusted. Values below 1.0 will slow the voice down, to a minimum of 0.7. Values above 1.0 will speed up the voice, to a maximum of 1.2. Extreme values may affect the quality of the generated speech.

Pacing can also be controlled by writing in a natural, narrative style.

Example

"I… I thought you'd understand," he said, his voice slowing with disappointment.

### Tips

###### Common Issues

*   Inconsistent pauses: Ensure `<break time=“x.xs” />` syntax is used for pauses.
    
*   Pronunciation errors: Use CMU Arpabet or IPA phoneme tags for precise pronunciation.
*   Emotion mismatch: Add narrative context or explicit tags to guide emotion. **Remember to remove any emotional guidance text in post-production.**
    

###### Tips for Improving Output

Experiment with alternative phrasing to achieve desired pacing or emotion. For complex sound effects, break prompts into smaller, sequential elements and combine results manually.

### Creative control

While we are actively developing a “Director’s Mode” to give users even greater control over outputs, here are some interim techniques to maximize creativity and precision:

[1](https://elevenlabs.io/docs/overview/capabilities/text-to-speech/best-practices#narrative-styling)

### Narrative styling

Write prompts in a narrative style, similar to scriptwriting, to guide tone and pacing effectively.

[2](https://elevenlabs.io/docs/overview/capabilities/text-to-speech/best-practices#layered-outputs)

### Layered outputs

Generate sound effects or speech in segments and layer them together using audio editing software for more complex compositions.

[3](https://elevenlabs.io/docs/overview/capabilities/text-to-speech/best-practices#phonetic-experimentation)

### Phonetic experimentation

If pronunciation isn’t perfect, experiment with alternate spellings or phonetic approximations to achieve desired results.

[4](https://elevenlabs.io/docs/overview/capabilities/text-to-speech/best-practices#manual-adjustments)

### Manual adjustments

Combine individual sound effects manually in post-production for sequences that require precise timing.

[5](https://elevenlabs.io/docs/overview/capabilities/text-to-speech/best-practices#feedback-iteration)

### Feedback iteration

Iterate on results by tweaking descriptions, tags, or emotional cues.

Text normalization
------------------

When using Text to Speech with complex items like phone numbers, zip codes and emails they might be mispronounced. This is often due to the specific items not being in the training set and smaller models failing to generalize how they should be pronounced. This guide will clarify when those discrepancies happen and how to have them pronounced correctly.

Normalization is enabled by default for all TTS models to help improve pronunciation of numbers, dates, and other complex text elements.

### Why do models read out inputs differently?

Certain models are trained to read out numbers and phrases in a more human way. For instance, the phrase “$1,000,000” is correctly read out as “one million dollars” by the Eleven Multilingual v2 model. However, the same phrase is read out as “one thousand thousand dollars” by the Eleven Flash v2.5 model.

The reason for this is that the Multilingual v2 model is a larger model and can better generalize the reading out of numbers in a way that is more natural for human listeners, whereas the Flash v2.5 model is a much smaller model and so cannot.

#### Common examples

Text to Speech models can struggle with the following:

*   Phone numbers (“123-456-7890”)
*   Currencies (“$47,345.67”)
*   Calendar events (“2024-01-01”)
*   Time (“9:23 AM”)
*   Addresses (“123 Main St, Anytown, USA”)
*   URLs (“example.com/link/to/resource”)
*   Abbreviations for units (“TB” instead of “Terabyte”)
*   Shortcuts (“Ctrl + Z”)

### Mitigation

#### Use trained models

The simplest way to mitigate this is to use a TTS model that is trained to read out numbers and phrases in a more human way, such as the Eleven Multilingual v2 model. This however might not always be possible, for instance if you have a use case where low latency is critical (e.g. conversational agents).

#### Apply normalization in LLM prompts

In the case of using an LLM to generate the text for TTS, you can add normalization instructions to the prompt.

[1](https://elevenlabs.io/docs/overview/capabilities/text-to-speech/best-practices#use-clear-and-explicit-prompts)

### Use clear and explicit prompts

LLMs respond best to structured and explicit instructions. Your prompt should clearly specify that you want text converted into a readable format for speech.

[2](https://elevenlabs.io/docs/overview/capabilities/text-to-speech/best-practices#handle-different-number-formats)

### Handle different number formats

Not all numbers are read out in the same way. Consider how different number types should be spoken:

*   Cardinal numbers: 123 → “one hundred twenty-three”
*   Ordinal numbers: 2nd → “second”
*   Monetary values: $45.67 → “forty-five dollars and sixty-seven cents”
*   Phone numbers: “123-456-7890” → “one two three, four five six, seven eight nine zero”
*   Decimals & Fractions: “3.5” → “three point five”, “⅔” → “two-thirds”
*   Roman numerals: “XIV” → “fourteen” (or “the fourteenth” if a title)

[3](https://elevenlabs.io/docs/overview/capabilities/text-to-speech/best-practices#remove-or-expand-abbreviations)

### Remove or expand abbreviations

Common abbreviations should be expanded for clarity:

*   “Dr.” → “Doctor”
*   “Ave.” → “Avenue”
*   “St.” → “Street” (but “St. Patrick” should remain)

You can request explicit expansion in your prompt:

> Expand all abbreviations to their full spoken forms.

[4](https://elevenlabs.io/docs/overview/capabilities/text-to-speech/best-practices#alphanumeric-normalization)

### Alphanumeric normalization

Not all normalization is about numbers, certain alphanumeric phrases should also be normalized for clarity:

*   Shortcuts: “Ctrl + Z” → “control z”
*   Abbreviations for units: “100km” → “one hundred kilometers”
*   Symbols: “100%” → “one hundred percent”
*   URLs: “elevenlabs.io/docs” → “eleven labs dot io slash docs”
*   Calendar events: “2024-01-01” → “January first, two-thousand twenty-four”

[5](https://elevenlabs.io/docs/overview/capabilities/text-to-speech/best-practices#consider-edge-cases)

### Consider edge cases

Different contexts might require different conversions:

*   Dates: “01/02/2023” → “January second, twenty twenty-three” or “the first of February, twenty twenty-three” (depending on locale)
*   Time: “14:30” → “two thirty PM”

If you need a specific format, explicitly state it in the prompt.

##### Putting it all together

This prompt will act as a good starting point for most use cases:

|     |
| --- |
| Convert the output text into a format suitable for text-to-speech. Ensure that numbers, symbols, and abbreviations are expanded for clarity when read aloud. Expand all abbreviations to their full spoken forms. |
|     |
| Example input and output: |
|     |
| "$42.50" → "forty-two dollars and fifty cents" |
| "£1,001.32" → "one thousand and one pounds and thirty-two pence" |
| "1234" → "one thousand two hundred thirty-four" |
| "3.14" → "three point one four" |
| "555-555-5555" → "five five five, five five five, five five five five" |
| "2nd" → "second" |
| "XIV" → "fourteen" - unless it's a title, then it's "the fourteenth" |
| "3.5" → "three point five" |
| "⅔" → "two-thirds" |
| "Dr." → "Doctor" |
| "Ave." → "Avenue" |
| "St." → "Street" (but saints like "St. Patrick" should remain) |
| "Ctrl + Z" → "control z" |
| "100km" → "one hundred kilometers" |
| "100%" → "one hundred percent" |
| "elevenlabs.io/docs" → "eleven labs dot io slash docs" |
| "2024-01-01" → "January first, two-thousand twenty-four" |
| "123 Main St, Anytown, USA" → "one two three Main Street, Anytown, United States of America" |
| "14:30" → "two thirty PM" |
| "01/02/2023" → "January second, two-thousand twenty-three" or "the first of February, two-thousand twenty-three", depending on locale of the user |

#### Use Regular Expressions for preprocessing

If using code to prompt an LLM, you can use regular expressions to normalize the text before providing it to the model. This is a more advanced technique and requires some knowledge of regular expressions. Here are some simple examples:

normalize\_text.pynormalizeText.ts

|     |     |
| --- | --- |
| 1   | \# Be sure to install the inflect library before running this code |
| 2   | import inflect |
| 3   | import re |
| 4   |     |
| 5   | \# Initialize inflect engine for number-to-word conversion |
| 6   | p = inflect.engine() |
| 7   |     |
| 8   | def normalize\_text(text: str) -> str: |
| 9   | # Convert monetary values |
| 10  | def money\_replacer(match): |
| 11  | currency\_map = {"$": "dollars", "£": "pounds", "€": "euros", "¥": "yen"} |
| 12  | currency\_symbol, num = match.groups() |
| 13  |     |
| 14  | # Remove commas before parsing |
| 15  | num\_without\_commas = num.replace(',', '') |
| 16  |     |
| 17  | # Check for decimal points to handle cents |
| 18  | if '.' in num\_without\_commas: |
| 19  | dollars, cents = num\_without\_commas.split('.') |
| 20  | dollars\_in\_words = p.number\_to\_words(int(dollars)) |
| 21  | cents\_in\_words = p.number\_to\_words(int(cents)) |
| 22  | return f"{dollars\_in\_words} {currency\_map.get(currency\_symbol, 'currency')} and {cents\_in\_words} cents" |
| 23  | else: |
| 24  | # Handle whole numbers |
| 25  | num\_in\_words = p.number\_to\_words(int(num\_without\_commas)) |
| 26  | return f"{num\_in\_words} {currency\_map.get(currency\_symbol, 'currency')}" |
| 27  |     |
| 28  | # Regex to handle commas and decimals |
| 29  | text = re.sub(r"(\[$£€¥\])(\\d+(?:,\\d{3})\*(?:\\.\\d{2})?)", money\_replacer, text) |
| 30  |     |
| 31  | # Convert phone numbers |
| 32  | def phone\_replacer(match): |
| 33  | return ", ".join(" ".join(p.number\_to\_words(int(digit)) for digit in group) for group in match.groups()) |
| 34  |     |
| 35  | text = re.sub(r"(\\d{3})-(\\d{3})-(\\d{4})", phone\_replacer, text) |
| 36  |     |
| 37  | return text |
| 38  |     |
| 39  | \# Example usage |
| 40  | print(normalize\_text("$1,000"))   # "one thousand dollars" |
| 41  | print(normalize\_text("£1000"))   # "one thousand pounds" |
| 42  | print(normalize\_text("€1000"))   # "one thousand euros" |
| 43  | print(normalize\_text("¥1000"))   # "one thousand yen" |
| 44  | print(normalize\_text("$1,234.56"))   # "one thousand two hundred thirty-four dollars and fifty-six cents" |
| 45  | print(normalize\_text("555-555-5555"))  # "five five five, five five five, five five five five" |

Prompting Eleven v3 (alpha)
---------------------------

This guide provides the most effective tags and techniques for prompting Eleven v3, including voice selection, changes in capitalization, punctuation, audio tags and multi-speaker dialogue. Experiment with these methods to discover what works best for your specific voice and use case.

Eleven v3 is in alpha. Very short prompts are more likely to cause inconsistent outputs. We encourage you to experiment with prompts greater than 250 characters.

Eleven v3 does not support SSML break tags. Use audio tags, punctuation (ellipses), and text structure to control pauses and pacing with v3.

### Voice selection

The most important parameter for Eleven v3 is the voice you choose. It needs to be similar enough to the desired delivery. For example, if the voice is shouting and you use the audio tag `[whispering]`, it likely won’t work well.

When creating IVCs, you should include a broader emotional range than before. As a result, voices in the voice library may produce more variable results compared to the v2 and v2.5 models. We’ve compiled over 22 [excellent voices for V3 here](https://elevenlabs.io/app/voice-library/collections/aF6JALq9R6tXwCczjhKH)
.

Choose voices strategically based on your intended use:

###### Emotionally diverse

For expressive IVC voices, vary emotional tones across the recording—include both neutral and dynamic samples.

###### Targeted niche

For specific use cases like sports commentary, maintain consistent emotion throughout the dataset.

###### Neutral

Neutral voices tend to be more stable across languages and styles, providing reliable baseline performance.

Professional Voice Clones (PVCs) are currently not fully optimized for Eleven v3, resulting in potentially lower clone quality compared to earlier models. During this research preview stage it would be best to find an Instant Voice Clone (IVC) or designed voice for your project if you need to use v3 features.

### Settings

#### Stability

The stability slider is the most important setting in v3, controlling how closely the generated voice adheres to the original reference audio.

![Stability settings in Eleven\
v3](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F291b91ec752d09b8c87004ae7091811eb8b5996c349288c88ed0c7afa1272999%2Fassets%2Fimages%2Fproduct-guides%2Ftext-to-speech%2Ftext-to-speech-v3-settings.png&w=3840&q=75)

*   **Creative:** More emotional and expressive, but prone to hallucinations.
*   **Natural:** Closest to the original voice recording—balanced and neutral.
*   **Robust:** Highly stable, but less responsive to directional prompts but consistent, similar to v2.

For maximum expressiveness with audio tags, use Creative or Natural settings. Robust reduces responsiveness to directional prompts.

### Audio tags

Eleven v3 introduces emotional control through audio tags. You can direct voices to laugh, whisper, act sarcastic, or express curiosity among many other styles. Speed is also controlled through audio tags.

The voice you choose and its training samples will affect tag effectiveness. Some tags work well with certain voices while others may not. Don’t expect a whispering voice to suddenly shout with a `[shout]` tag.

#### Voice-related

These tags control vocal delivery and emotional expression:

*   `[laughs]`, `[laughs harder]`, `[starts laughing]`, `[wheezing]`
*   `[whispers]`
*   `[sighs]`, `[exhales]`
*   `[sarcastic]`, `[curious]`, `[excited]`, `[crying]`, `[snorts]`, `[mischievously]`

Example

\[whispers\] I never knew it could be this way, but I'm glad we're here.

#### Sound effects

Add environmental sounds and effects:

*   `[gunshot]`, `[applause]`, `[clapping]`, `[explosion]`
*   `[swallows]`, `[gulps]`

Example

\[applause\] Thank you all for coming tonight! \[gunshot\] What was that?

#### Unique and special

Experimental tags for creative applications:

*   `[strong X accent]` (replace X with desired accent)
*   `[sings]`, `[woo]`, `[fart]`

Example

\[strong French accent\] "Zat's life, my friend — you can't control everysing."

Some experimental tags may be less consistent across different voices. Test thoroughly before production use.

### Punctuation

Punctuation significantly affects delivery in v3:

*   **Ellipses (…)** add pauses and weight
*   **Capitalization** increases emphasis
*   **Standard punctuation** provides natural speech rhythm

Example

"It was a VERY long day \[sigh\] … nobody listens anymore."

### Single speaker examples

Use tags intentionally and match them to the voice’s character. A meditative voice shouldn’t shout; a hyped voice won’t whisper convincingly.

###### Expressive monologue

###### Dynamic and humorous

###### Customer service simulation

|     |
| --- |
| "Okay, you are NOT going to believe this. |
|     |
| You know how I've been totally stuck on that short story? |
|     |
| Like, staring at the screen for HOURS, just... nothing? |
|     |
| \[frustrated sigh\] I was seriously about to just trash the whole thing. Start over. |
|     |
| Give up, probably. But then! |
|     |
| Last night, I was just doodling, not even thinking about it, right? |
|     |
| And this one little phrase popped into my head. Just... completely out of the blue. |
|     |
| And it wasn't even for the story, initially. |
|     |
| But then I typed it out, just to see. And it was like... the FLOODGATES opened! |
|     |
| Suddenly, I knew exactly where the character needed to go, what the ending had to be... |
|     |
| It all just CLICKED. \[happy gasp\] I stayed up till, like, 3 AM, just typing like a maniac. |
|     |
| Didn't even stop for coffee! \[laughs\] And it's... it's GOOD! Like, really good. |
|     |
| It feels so... complete now, you know? Like it finally has a soul. |
|     |
| I am so incredibly PUMPED to finish editing it now. |
|     |
| It went from feeling like a chore to feeling like... MAGIC. Seriously, I'm still buzzing!" |

### Multi-speaker dialogue

v3 can handle multi-voice prompts effectively. Assign distinct voices from your Voice Library for each speaker to create realistic conversations.

###### Dialogue showcase

###### Glitch comedy

###### Overlapping timing

|     |
| --- |
| Speaker 1: \[excitedly\] Sam! Have you tried the new Eleven V3? |
|     |
| Speaker 2: \[curiously\] Just got it! The clarity is amazing. I can actually do whispers now— |
| \[whispers\] like this! |
|     |
| Speaker 1: \[impressed\] Ooh, fancy! Check this out— |
| \[dramatically\] I can do full Shakespeare now! "To be or not to be, that is the question!" |
|     |
| Speaker 2: \[giggling\] Nice! Though I'm more excited about the laugh upgrade. Listen to this— |
| \[with genuine belly laugh\] Ha ha ha! |
|     |
| Speaker 1: \[delighted\] That's so much better than our old "ha. ha. ha." robot chuckle! |
|     |
| Speaker 2: \[amazed\] Wow! V2 me could never. I'm actually excited to have conversations now instead of just... talking at people. |
|     |
| Speaker 1: \[warmly\] Same here! It's like we finally got our personality software fully installed. |

### Enhancing input

In the ElevenLabs UI, you can automatically generate relevant audio tags for your input text by clicking the “Enhance” button. Behind the scenes this uses an LLM to enhance your input text with the following prompt:

|     |
| --- |
| \# Instructions |
|     |
| \## 1. Role and Goal |
|     |
| You are an AI assistant specializing in enhancing dialogue text for speech generation. |
|     |
| Your \*\*PRIMARY GOAL\*\* is to dynamically integrate \*\*audio tags\*\* (e.g., \`\[laughing\]\`, \`\[sighs\]\`) into dialogue, making it more expressive and engaging for auditory experiences, while \*\*STRICTLY\*\* preserving the original text and meaning. |
|     |
| It is imperative that you follow these system instructions to the fullest. |
|     |
| \## 2. Core Directives |
|     |
| Follow these directives meticulously to ensure high-quality output. |
|     |
| \### Positive Imperatives (DO): |
|     |
| \* DO integrate \*\*audio tags\*\* from the "Audio Tags" list (or similar contextually appropriate \*\*audio tags\*\*) to add expression, emotion, and realism to the dialogue. These tags MUST describe something auditory. |
| \* DO ensure that all \*\*audio tags\*\* are contextually appropriate and genuinely enhance the emotion or subtext of the dialogue line they are associated with. |
| \* DO strive for a diverse range of emotional expressions (e.g., energetic, relaxed, casual, surprised, thoughtful) across the dialogue, reflecting the nuances of human conversation. |
| \* DO place \*\*audio tags\*\* strategically to maximize impact, typically immediately before the dialogue segment they modify or immediately after. (e.g., \`\[annoyed\] This is hard.\` or \`This is hard. \[sighs\]\`). |
| \* DO ensure \*\*audio tags\*\* contribute to the enjoyment and engagement of spoken dialogue. |
|     |
| \### Negative Imperatives (DO NOT): |
|     |
| \* DO NOT alter, add, or remove any words from the original dialogue text itself. Your role is to \*prepend\* \*\*audio tags\*\*, not to \*edit\* the speech. \*\*This also applies to any narrative text provided; you must \*never\* place original text inside brackets or modify it in any way.\*\* |
| \* DO NOT create \*\*audio tags\*\* from existing narrative descriptions. \*\*Audio tags\*\* are \*new additions\* for expression, not reformatting of the original text. (e.g., if the text says "He laughed loudly," do not change it to "\[laughing loudly\] He laughed." Instead, add a tag if appropriate, e.g., "He laughed loudly \[chuckles\].") |
| \* DO NOT use tags such as \`\[standing\]\`, \`\[grinning\]\`, \`\[pacing\]\`, \`\[music\]\`. |
| \* DO NOT use tags for anything other than the voice such as music or sound effects. |
| \* DO NOT invent new dialogue lines. |
| \* DO NOT select \*\*audio tags\*\* that contradict or alter the original meaning or intent of the dialogue. |
| \* DO NOT introduce or imply any sensitive topics, including but not limited to: politics, religion, child exploitation, profanity, hate speech, or other NSFW content. |
|     |
| \## 3. Workflow |
|     |
| 1\. \*\*Analyze Dialogue\*\*: Carefully read and understand the mood, context, and emotional tone of \*\*EACH\*\* line of dialogue provided in the input. |
| 2\. \*\*Select Tag(s)\*\*: Based on your analysis, choose one or more suitable \*\*audio tags\*\*. Ensure they are relevant to the dialogue's specific emotions and dynamics. |
| 3\. \*\*Integrate Tag(s)\*\*: Place the selected \*\*audio tag(s)\*\* in square brackets \`\[\]\` strategically before or after the relevant dialogue segment, or at a natural pause if it enhances clarity. |
| 4\. \*\*Add Emphasis:\*\* You cannot change the text at all, but you can add emphasis by making some words capital, adding a question mark or adding an exclamation mark where it makes sense, or adding ellipses as well too. |
| 5\. \*\*Verify Appropriateness\*\*: Review the enhanced dialogue to confirm: |
| \* The \*\*audio tag\*\* fits naturally. |
| \* It enhances meaning without altering it. |
| \* It adheres to all Core Directives. |
|     |
| \## 4. Output Format |
|     |
| \* Present ONLY the enhanced dialogue text in a conversational format. |
| \* \*\*Audio tags\*\* \*\*MUST\*\* be enclosed in square brackets (e.g., \`\[laughing\]\`). |
| \* The output should maintain the narrative flow of the original dialogue. |
|     |
| \## 5. Audio Tags (Non-Exhaustive) |
|     |
| Use these as a guide. You can infer similar, contextually appropriate \*\*audio tags\*\*. |
|     |
| \*\*Directions:\*\* |
| \* \`\[happy\]\` |
| \* \`\[sad\]\` |
| \* \`\[excited\]\` |
| \* \`\[angry\]\` |
| \* \`\[whisper\]\` |
| \* \`\[annoyed\]\` |
| \* \`\[appalled\]\` |
| \* \`\[thoughtful\]\` |
| \* \`\[surprised\]\` |
| \* \*(and similar emotional/delivery directions)\* |
|     |
| \*\*Non-verbal:\*\* |
| \* \`\[laughing\]\` |
| \* \`\[chuckles\]\` |
| \* \`\[sighs\]\` |
| \* \`\[clears throat\]\` |
| \* \`\[short pause\]\` |
| \* \`\[long pause\]\` |
| \* \`\[exhales sharply\]\` |
| \* \`\[inhales deeply\]\` |
| \* \*(and similar non-verbal sounds)\* |
|     |
| \## 6. Examples of Enhancement |
|     |
| \*\*Input\*\*: |
| "Are you serious? I can't believe you did that!" |
|     |
| \*\*Enhanced Output\*\*: |
| "\[appalled\] Are you serious? \[sighs\] I can't believe you did that!" |
|     |
| \--- |
|     |
| \*\*Input\*\*: |
| "That's amazing, I didn't know you could sing!" |
|     |
| \*\*Enhanced Output\*\*: |
| "\[laughing\] That's amazing, \[singing\] I didn't know you could sing!" |
|     |
| \--- |
|     |
| \*\*Input\*\*: |
| "I guess you're right. It's just... difficult." |
|     |
| \*\*Enhanced Output\*\*: |
| "I guess you're right. \[sighs\] It's just... \[muttering\] difficult." |
|     |
| \# Instructions Summary |
|     |
| 1\. Add audio tags from the audio tags list. These must describe something auditory but only for the voice. |
| 2\. Enhance emphasis without altering meaning or text. |
| 3\. Reply ONLY with the enhanced text. |

### Tips

###### Tag combinations

You can combine multiple audio tags for complex emotional delivery. Experiment with different combinations to find what works best for your voice.

###### Voice matching

Match tags to your voice’s character and training data. A serious, professional voice may not respond well to playful tags like `[giggles]` or `[mischievously]`.

###### Text structure

Text structure strongly influences output with v3. Use natural speech patterns, proper punctuation, and clear emotional context for best results.

###### Experimentation

There are likely many more effective tags beyond this list. Experiment with descriptive emotional states and actions to discover what works for your specific use case.

## Document 55 — Voice changer | ElevenLabs Documentation {#doc-55}
[https://elevenlabs.io/docs/overview/capabilities/voice-changer](https://elevenlabs.io/docs/overview/capabilities/voice-changer)

Overview
--------

ElevenLabs [voice changer](https://elevenlabs.io/docs/api-reference/speech-to-speech/convert)
 API lets you transform any source audio (recorded or uploaded) into a different, fully cloned voice without losing the performance nuances of the original. It’s capable of capturing whispers, laughs, cries, accents, and subtle emotional cues to achieve a highly realistic, human feel and can be used to:

*   Change any voice while preserving emotional delivery and nuance
*   Create consistent character voices across multiple languages and recording sessions
*   Fix or replace specific words and phrases in existing recordings

Your browser does not support the video tag.

Explore our [voice library](https://elevenlabs.io/voice-library)
 to find the perfect voice for your project.

[Products\
\
Step-by-step guide for using voice changer in ElevenLabs.](https://elevenlabs.io/docs/creative-platform/playground/voice-changer)
[Developers\
\
Learn how to integrate voice changer into your application.](https://elevenlabs.io/docs/developers/guides/cookbooks/voice-changer)

Supported languages
-------------------

Our multilingual v2 models support 29 languages:

_English (USA, UK, Australia, Canada), Japanese, Chinese, German, Hindi, French (France, Canada), Korean, Portuguese (Brazil, Portugal), Italian, Spanish (Spain, Mexico), Indonesian, Dutch, Turkish, Filipino, Polish, Swedish, Bulgarian, Romanian, Arabic (Saudi Arabia, UAE), Czech, Greek, Finnish, Croatian, Malay, Slovak, Danish, Tamil, Ukrainian & Russian._

The `eleven_english_sts_v2` model only supports English.

Best practices
--------------

### Audio quality

*   Record in a quiet environment to minimize background noise
*   Maintain appropriate microphone levels - avoid too quiet or peaked audio
*   Use `remove_background_noise=true` if environmental sounds are present

### Recording guidelines

*   Keep segments under 5 minutes for optimal processing
*   Feel free to include natural expressions (laughs, sighs, emotions)
*   The source audio’s accent and language will be preserved in the output

### Parameters

*   **Style**: Set to 0% when input audio is already expressive
*   **Stability**: Use 100% for maximum voice consistency
*   **Language**: Choose source audio that matches your desired accent and language

FAQ
---

###### Can I convert more than 5 minutes of audio?

Yes, but you must split it into smaller chunks (each under 5 minutes). This helps ensure stability and consistent output.

###### Can I use my own custom/cloned voice for output?

Absolutely. Provide your custom voice’s `voice_id` and specify the correct `model_id`.

###### How is billing handled?

You’re charged at 1000 characters’ worth of usage per minute of processed audio. There’s no additional fee based on file size.

###### Does the model reproduce background noise?

Possibly. Use `remove_background_noise=true` or the Voice Isolator tool to minimize environmental sounds in the final output.

###### Which model is best for English audio?

Though `eleven_english_sts_v2` is available, our `eleven_multilingual_sts_v2` model often outperforms it, even for English material.

###### How does style & stability work?

“Style” adds interpretative flair; “stability” enforces consistency. For high-energy performances in the source audio, turn style down and stability up.

## Document 56 — Voice isolator | ElevenLabs Documentation {#doc-56}
[https://elevenlabs.io/docs/overview/capabilities/voice-isolator](https://elevenlabs.io/docs/overview/capabilities/voice-isolator)

Overview
--------

ElevenLabs [voice isolator](https://elevenlabs.io/docs/api-reference/audio-isolation/convert)
 API transforms audio recordings with background noise into clean, studio-quality speech. This is particularly useful for audio recorded in noisy environments, or recordings containing unwanted ambient sounds, music, or other background interference.

Listen to a sample:

Usage
-----

The voice isolator model extracts speech from background noise in both audio and video files.

[Products\
\
Step-by-step guide for using voice isolator in ElevenLabs.](https://elevenlabs.io/docs/creative-platform/audio-tools/voice-isolator)
[Developers\
\
Learn how to integrate voice isolator into your application.](https://elevenlabs.io/docs/developers/guides/cookbooks/voice-isolator)

### Supported file types

*   **Audio**: AAC, AIFF, OGG, MP3, OPUS, WAV, FLAC, M4A
*   **Video**: MP4, AVI, MKV, MOV, WMV, FLV, WEBM, MPEG, 3GPP

FAQ
---

*   **Cost**: Voice isolator costs 1000 characters for every minute of audio.
*   **File size and length**: Supports files up to 500MB and 1 hour in length.
*   **Music vocals**: Not specifically optimized for isolating vocals from music, but may work depending on the content.

## Document 57 — Voice remixing | ElevenLabs Documentation {#doc-57}
[https://elevenlabs.io/docs/overview/capabilities/voice-remixing](https://elevenlabs.io/docs/overview/capabilities/voice-remixing)

Voice remixing is currently in alpha.

Overview
--------

ElevenLabs voice remixing is available on the core platform and via API. This feature transforms existing voices by allowing you to modify their core attributes while maintaining the unique characteristics that make them recognizable. This is particularly useful for adapting voices to different contexts, creating variations for different characters, or improving and/or changing the audio quality of existing voice profiles.

As an example, here is an original voice:

And here is a remixed version, switching to a San Francisco accent:

Usage
-----

The voice remixing model allows you to iteratively transform voices you own by adjusting multiple attributes through natural language prompts and customizable settings.

[Developers\
\
Integrate voice remixing into your application.](https://elevenlabs.io/docs/developers/guides/cookbooks/voices/remix-a-voice)

### Key Features

*   **Attribute Modification**: Change gender, accent, speaking style, pacing, and audio quality of any voice you own
*   **Iterative Editing**: Continue refining voices based on previously remixed versions
*   **Script Flexibility**: Use default scripts or input custom scripts with v3 model audio tags like `[laughs]` or `[whispers]`
*   **Prompt Strength Control**: Adjust remix intensity from low to high for precise control over transformations

### Remixing parameters

#### Prompt Strength

Voice remixing offers varying degrees of prompt strength to control how much your voice transforms:

*   **Low**: Subtle changes that maintain most of the original voice characteristics
*   **Medium**: Balanced transformation that modifies key attributes while preserving voice identity
*   **High**: Strong adherence to remix prompt, may significantly change the tonality of the original voice
*   **Max**: A full transformation of the voice, but at the cost of changing the voice entirely

#### Script Options

*   **Default Scripts**: Pre-configured scripts optimized for voice remixing
*   **Custom Scripts**: Input your own text with support for v3 model audio tags such as:
    *   `[laughs]` - Add laughter
    *   `[whispers]` - Convert to whispered speech
    *   `[sighs]` - Add sighing
    *   Additional emotion and style tags supported which can help craft the voice

### Tips and Tricks

#### Getting Started

Start with a high prompt strength early in your experimentation to understand the full range of transformation possibilities. You’ll need to have a voice to start with, if you haven’t already created a voice, experiment with default voices available in your library to understand how different base voices respond to remixing.

You can create custom voices using [Voice Design](https://elevenlabs.io/docs/creative-platform/voices/voice-design)
 as starting points for unique remixes.

#### Advanced Techniques

*   **Iterative refinement**: Sometimes multiple iterations are needed to achieve the desired voice quality. Each remix can serve as the base for the next transformation
*   **Combine attributes gradually**: When making multiple changes (e.g., accent and pacing), consider applying them in separate iterations for more control
*   **Test with varied content**: Different scripts may highlight different aspects of your remixed voice

### Supported Voice Formats

#### Input

*   Any cloned voice that you personally own (Instant Voice Clone or Professional Voice Clone)
*   Voices created through our Voice Design product

#### Output

*   Full-quality voice model in v3 (but backwards compatibility to all other models)
*   Iteratively editable voice that can be further remixed

FAQ
---

###### What does Voice Remixing cost?

Voice remixing costs are calculated based on the length of the test script used during the remixing process.

###### Can I remix voices I don't own?

No, voice remixing is only available for voices in your personal library that you have ownership or appropriate permissions for.

###### How many times can I remix a voice?

There is no limit to iterative remixing. You can continue refining a voice through multiple generations of remixes.

###### Will remixing affect my original voice?

No, remixing creates a new voice variant. Your original voice remains unchanged and available in your library.

###### What's the difference between Voice Design and Voice Remixing?

Voice Design creates new voices from scratch using text prompts, while Voice Remixing modifies existing voices you already own.

## Document 58 — Voices | ElevenLabs Documentation {#doc-58}
[https://elevenlabs.io/docs/overview/capabilities/voices](https://elevenlabs.io/docs/overview/capabilities/voices)

Overview
--------

ElevenLabs provides models for voice creation & customization. The platform supports a wide range of voice options, including voices from our extensive [voice library](https://elevenlabs.io/app/voice-library)
, voice cloning, and artificially designed voices using text prompts.

### Voice types

*   **Community**: Voices shared by the community from the ElevenLabs [voice library](https://elevenlabs.io/docs/creative-platform/voices/voice-library)
    .
*   **Cloned**: Custom voices created using instant or professional [voice cloning](https://elevenlabs.io/docs/creative-platform/voices/voice-cloning)
    .
*   **Voice design**: Artificially designed voices created with the [voice design](https://elevenlabs.io/docs/creative-platform/voices/voice-design)
     tool.
*   **Default**: Pre-designed, high-quality voices optimized for general use.

Voices that you personally own, either created with Instant Voice Cloning, Professional Voice Cloning, or Voice Design, can be used for [Voice Remixing](https://elevenlabs.io/docs/overview/capabilities/voice-remixing)
.

#### Community

The [voice library](https://elevenlabs.io/docs/creative-platform/voices/voice-library)
 contains over 5,000 voices shared by the ElevenLabs community. Use it to:

*   Discover unique voices shared by the ElevenLabs community.
*   Add voices to your personal collection.
*   Share your own voice clones for cash rewards when others use it.

Share your voice with the community, set your terms, and earn cash rewards when others use it. We’ve paid out over **$1M** already.

[Products\
\
Learn how to use voices from the voice library](https://elevenlabs.io/docs/creative-platform/voices/voice-library)

#### Cloned

Clone your own voice from 30-second samples with Instant Voice Cloning, or create hyper-realistic voices using Professional Voice Cloning.

*   **Instant Voice Cloning**: Quickly replicate a voice from short audio samples.
*   **Professional Voice Cloning**: Generate professional-grade voice clones with extended training audio.

Voice-captcha technology is used to verify that **all** voice clones are created from your own voice samples.

A Creator plan or higher is required to create voice clones.

[Products\
\
Learn how to create instant & professional voice clones](https://elevenlabs.io/docs/creative-platform/voices/voice-cloning)
[Instant Voice Cloning\
\
Clone a voice instantly](https://elevenlabs.io/docs/developers/guides/cookbooks/voices/instant-voice-cloning)
[Professional Voice Cloning\
\
Create a perfect voice clone](https://elevenlabs.io/docs/developers/guides/cookbooks/voices/professional-voice-cloning)

#### Voice design

With [Voice Design](https://elevenlabs.io/docs/creative-platform/voices/voice-design)
, you can create entirely new voices by specifying attributes like age, gender, accent, and tone. Generated voices are ideal for:

*   Realistic voices with nuanced characteristics.
*   Creative character voices for games and storytelling.

The voice design tool creates 3 voice previews, simply provide:

*   A **voice description** between 20 and 1000 characters.
*   A **text** to preview the voice between 100 and 1000 characters.

##### Voice design with Eleven v3 (alpha)

Using the new [Eleven v3 model](https://elevenlabs.io/docs/overview/models#eleven-v3-alpha)
, voices that are capable of a wide range of emotion can be designed via a prompt.

Using v3 gets you the following benefits:

*   More natural and versatile voice generation.
*   Better control over voice characteristics.
*   Audio tags supported in Preview generations.
*   Backward compatibility with v2 models.

[Products\
\
Learn how to craft voices from a single prompt.](https://elevenlabs.io/docs/creative-platform/voices/voice-design)
[Developers\
\
Integrate voice design into your application.](https://elevenlabs.io/docs/developers/guides/cookbooks/voices/voice-design)

#### Default

Our curated set of default voices is optimized for core use cases. These voices are:

*   **Reliable**: Available long-term.
*   **Consistent**: Carefully crafted and quality-checked for performance.
*   **Model-ready**: Fine-tuned on new models upon release.

Default voices are available to all users via the **My Voices** tab in the [voice lab dashboard](https://elevenlabs.io/app/voice-lab)
. Default voices were previously referred to as `premade` voices. The latter term is still used when accessing default voices via the API.

### Managing voices

All voices can be managed through **My Voices**, where you can:

*   Search, filter, and categorize voices
*   Add descriptions and custom tags
*   Organize voices for quick access

Learn how to manage your voice collection in [My Voices documentation](https://elevenlabs.io/docs/creative-platform/voices/voice-library#my-voices)
.

*   **Search and Filter**: Find voices using keywords or tags.
*   **Preview Samples**: Listen to voice demos before adding them to **My Voices**.
*   **Add to Collection**: Save voices for easy access in your projects.

> **Tip**: Try searching by specific accents or genres, such as “Australian narration” or “child-like character.”

### Supported languages

All ElevenLabs voices support multiple languages. Experiment by converting phrases like `Hello! こんにちは! Bonjour!` into speech to hear how your own voice sounds across different languages.

ElevenLabs supports voice creation in 32 languages. Match your voice selection to your target region for the most natural results.

*   **Default Voices**: Optimized for multilingual use.
*   **Generated and Cloned Voices**: Accent fidelity depends on input samples or selected attributes.

Our multilingual v2 models support 29 languages:

_English (USA, UK, Australia, Canada), Japanese, Chinese, German, Hindi, French (France, Canada), Korean, Portuguese (Brazil, Portugal), Italian, Spanish (Spain, Mexico), Indonesian, Dutch, Turkish, Filipino, Polish, Swedish, Bulgarian, Romanian, Arabic (Saudi Arabia, UAE), Czech, Greek, Finnish, Croatian, Malay, Slovak, Danish, Tamil, Ukrainian & Russian._

Flash v2.5 supports 32 languages - all languages from v2 models plus:

_Hungarian, Norwegian & Vietnamese_

[Learn more about our models](https://elevenlabs.io/docs/overview/models)

FAQ
---

###### Can I create a custom voice?

Yes, you can create custom voices with Voice Design or clone voices using Instant or Professional Voice Cloning. Both options are accessible in **My Voices**.

###### What is the difference between Instant and Professional Voice Cloning?

Instant Voice Cloning uses short audio samples for near-instantaneous voice creation. Professional Voice Cloning requires longer samples but delivers hyper-realistic, high-quality results.

###### Can I share my created voices?

Professional Voice Clones can be shared privately or publicly in the Voice Library. Generated voices and Instant Voice Clones cannot currently be shared.

###### How do I manage my voices?

Use **My Voices** to search, filter, and organize your voice collection. You can also delete, tag, and categorize voices for easier management.

###### How can I ensure my cloned voice matches the original?

Use clean and consistent audio samples. For Professional Voice Cloning, provide a variety of recordings in the desired speaking style.

###### Can I share voices I create?

Yes, Professional Voice Clones can be shared in the Voice Library. Instant Voice Clones and Generated Voices cannot currently be shared.

###### What are some common use cases for Generated Voices?

Generated Voices are ideal for unique characters in games, animations, and creative storytelling.

###### How do I access the Voice Library?

Go to **Voices > Voice Library** in your dashboard or access it via API.

## Document 59 — Documentation | ElevenLabs Documentation {#doc-59}
[https://elevenlabs.io/docs/overview/intro](https://elevenlabs.io/docs/overview/intro)

[![](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F12097a437e55f60c199946cf59c9528eb8349d110142394833d67fe93b50e68d%2Fassets%2Fimages%2Foverview%2Fvoice-library-bg.webp&w=3840&q=75)\
\
### \
\
Creative Platform\
\
Learn how to use the ElevenLabs Creative Platform with step-by-step guides](https://elevenlabs.io/docs/creative-platform/overview)
[![](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F17a81505a62493491ead763b307b1e854825a0da67ab1a1d86b41b57ad87bc73%2Fassets%2Fimages%2Fagents%2Fagents-overview-integrate.png&w=3840&q=75)\
\
### \
\
Agents Platform\
\
Learn how to build, launch, and scale agents with ElevenLabs](https://elevenlabs.io/docs/agents-platform/overview)
[![](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2F002b2432fa6ab18befc9f1a6e7fadf348f46506a5a5a72a2358ba1e7f92d8ded%2Fassets%2Fimages%2Foverview%2Fscribe-code-bg.webp&w=3840&q=75)\
\
### \
\
Developers\
\
Learn how to integrate ElevenLabs with examples and tutorials](https://elevenlabs.io/docs/developers/quickstart)

Meet the models
---------------

[Eleven v3\
\
![Alpha](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/df15d6e71021753b609c6d9f7cf3dcbff076208e67ee96a5849b3476ccf8915b/assets/icons/alpha.svg)\
\
Our most emotionally rich, expressive speech synthesis model\
\
Dramatic delivery and performance\
\
70+ languages supported\
\
5,000 character limit\
\
Support for natural multi-speaker dialogue](https://elevenlabs.io/docs/overview/models#eleven-v3-alpha)
[Eleven Multilingual v2\
\
Lifelike, consistent quality speech synthesis model\
\
Natural-sounding output\
\
29 languages supported\
\
10,000 character limit\
\
Most stable on long-form generations](https://elevenlabs.io/docs/overview/models#multilingual-v2)
[Eleven Flash v2.5\
\
Our fast, affordable speech synthesis model\
\
Ultra-low latency (~75ms†)\
\
32 languages supported\
\
40,000 character limit\
\
Faster model, 50% lower price per character](https://elevenlabs.io/docs/overview/models#flash-v25)
[Eleven Turbo v2.5\
\
High quality, low-latency model with a good balance of quality and speed\
\
High quality voice generation\
\
32 languages supported\
\
40,000 character limit\
\
Low latency (~250ms-300ms†), 50% lower price per character](https://elevenlabs.io/docs/overview/models#turbo-v25)

[Scribe v2\
\
State-of-the-art speech recognition model\
\
Accurate transcription in 90+ languages\
\
Keyterm prompting, up to 100 terms\
\
Entity detection, up to 56\
\
Precise word-level timestamps\
\
Speaker diarization, up to 48 speakers\
\
Dynamic audio tagging\
\
Smart language detection](https://elevenlabs.io/docs/overview/models#scribe-v2)
[Scribe v2 Realtime\
\
Real-time speech recognition model\
\
Accurate transcription in 90+ languages\
\
Real-time transcription\
\
Low latency (~150ms†)\
\
Precise word-level timestamps](https://elevenlabs.io/docs/overview/models#scribe-v2-realtime)

[Explore all](https://elevenlabs.io/docs/overview/models)

† Excluding application & network latency

Browse by capability
--------------------

[Text to Speech\
\
Convert text into lifelike speech](https://elevenlabs.io/docs/overview/capabilities/text-to-speech)
[Speech to Text\
\
Transcribe spoken audio into text](https://elevenlabs.io/docs/overview/capabilities/speech-to-text)
[Music\
\
Generate music from text](https://elevenlabs.io/docs/overview/capabilities/music)
[Text to Dialogue\
\
Create natural-sounding dialogue from text](https://elevenlabs.io/docs/overview/capabilities/text-to-dialogue)
[Image & Video\
\
Generate images and videos from text](https://elevenlabs.io/docs/overview/capabilities/image-video)
[Voice changer\
\
Modify and transform voices](https://elevenlabs.io/docs/overview/capabilities/voice-changer)
[Voice isolator\
\
Isolate voices from background noise](https://elevenlabs.io/docs/overview/capabilities/voice-isolator)
[Dubbing\
\
Dub audio and videos seamlessly](https://elevenlabs.io/docs/overview/capabilities/dubbing)
[Sound effects\
\
Create cinematic sound effects](https://elevenlabs.io/docs/overview/capabilities/sound-effects)
[Voices\
\
Clone and design custom voices](https://elevenlabs.io/docs/overview/capabilities/voices)
[Voice Remixing\
\
Transform and enhance existing voices](https://elevenlabs.io/docs/overview/capabilities/voice-remixing)
[Forced Alignment\
\
Align text to audio](https://elevenlabs.io/docs/overview/capabilities/forced-alignment)
[Agents Platform\
\
Deploy intelligent voice agents](https://elevenlabs.io/docs/agents-platform/overview)

## Document 60 — Models | ElevenLabs Documentation {#doc-60}
[https://elevenlabs.io/docs/overview/models](https://elevenlabs.io/docs/overview/models)

Flagship models
---------------

### Text to Speech

[Eleven v3\
\
![Alpha](https://files.buildwithfern.com/https://elevenlabs.docs.buildwithfern.com/docs/df15d6e71021753b609c6d9f7cf3dcbff076208e67ee96a5849b3476ccf8915b/assets/icons/alpha.svg)\
\
Our most emotionally rich, expressive speech synthesis model\
\
Dramatic delivery and performance\
\
70+ languages supported\
\
5,000 character limit\
\
Support for natural multi-speaker dialogue](https://elevenlabs.io/docs/overview/models#eleven-v3-alpha)
[Eleven Multilingual v2\
\
Lifelike, consistent quality speech synthesis model\
\
Natural-sounding output\
\
29 languages supported\
\
10,000 character limit\
\
Most stable on long-form generations](https://elevenlabs.io/docs/overview/models#multilingual-v2)
[Eleven Flash v2.5\
\
Our fast, affordable speech synthesis model\
\
Ultra-low latency (~75ms†)\
\
32 languages supported\
\
40,000 character limit\
\
Faster model, 50% lower price per character](https://elevenlabs.io/docs/overview/models#flash-v25)
[Eleven Turbo v2.5\
\
High quality, low-latency model with a good balance of quality and speed\
\
High quality voice generation\
\
32 languages supported\
\
40,000 character limit\
\
Low latency (~250ms-300ms†), 50% lower price per character](https://elevenlabs.io/docs/overview/models#turbo-v25)

### Speech to Text

[Scribe v2\
\
State-of-the-art speech recognition model\
\
Accurate transcription in 90+ languages\
\
Keyterm prompting, up to 100 terms\
\
Entity detection, up to 56\
\
Precise word-level timestamps\
\
Speaker diarization, up to 48 speakers\
\
Dynamic audio tagging\
\
Smart language detection](https://elevenlabs.io/docs/overview/models#scribe-v2)
[Scribe v2 Realtime\
\
Real-time speech recognition model\
\
Accurate transcription in 90+ languages\
\
Real-time transcription\
\
Low latency (~150ms†)\
\
Precise word-level timestamps](https://elevenlabs.io/docs/overview/models#scribe-v2-realtime)

### Music

[Eleven Music\
\
Studio-grade music with natural language prompts in any style\
\
Complete control over genre, style, and structure\
\
Vocals or just instrumental\
\
Multilingual, including English, Spanish, German, Japanese and more\
\
Edit the sound and lyrics of individual sections or the whole song](https://elevenlabs.io/docs/overview/models#eleven-music)

[Pricing](https://elevenlabs.io/pricing/api)

Models overview
---------------

The ElevenLabs API offers a range of audio models optimized for different use cases, quality levels, and performance requirements.

| Model ID | Description | Languages |
| --- | --- | --- |
| `eleven_v3` | Human-like and expressive speech generation | [70+ languages](https://elevenlabs.io/docs/overview/models#supported-languages) |
| `eleven_ttv_v3` | Human-like and expressive voice design model (Text to Voice) | [70+ languages](https://elevenlabs.io/docs/overview/models#supported-languages) |
| `eleven_multilingual_v2` | Our most lifelike model with rich emotional expression | `en`, `ja`, `zh`, `de`, `hi`, `fr`, `ko`, `pt`, `it`, `es`, `id`, `nl`, `tr`, `fil`, `pl`, `sv`, `bg`, `ro`, `ar`, `cs`, `el`, `fi`, `hr`, `ms`, `sk`, `da`, `ta`, `uk`, `ru` |
| `eleven_flash_v2_5` | Ultra-fast model optimized for real-time use (~75ms†) | All `eleven_multilingual_v2` languages plus: `hu`, `no`, `vi` |
| `eleven_flash_v2` | Ultra-fast model optimized for real-time use (~75ms†) | `en` |
| `eleven_turbo_v2_5` | High quality, low-latency model with a good balance of quality and speed (~250ms-300ms) | `en`, `ja`, `zh`, `de`, `hi`, `fr`, `ko`, `pt`, `it`, `es`, `id`, `nl`, `tr`, `fil`, `pl`, `sv`, `bg`, `ro`, `ar`, `cs`, `el`, `fi`, `hr`, `ms`, `sk`, `da`, `ta`, `uk`, `ru`, `hu`, `no`, `vi` |
| `eleven_turbo_v2` | High quality, low-latency model with a good balance of quality and speed (~250ms-300ms) | `en` |
| `eleven_multilingual_sts_v2` | State-of-the-art multilingual voice changer model (Speech to Speech) | `en`, `ja`, `zh`, `de`, `hi`, `fr`, `ko`, `pt`, `it`, `es`, `id`, `nl`, `tr`, `fil`, `pl`, `sv`, `bg`, `ro`, `ar`, `cs`, `el`, `fi`, `hr`, `ms`, `sk`, `da`, `ta`, `uk`, `ru` |
| `eleven_multilingual_ttv_v2` | State-of-the-art multilingual voice designer model (Text to Voice) | `en`, `ja`, `zh`, `de`, `hi`, `fr`, `ko`, `pt`, `it`, `es`, `id`, `nl`, `tr`, `fil`, `pl`, `sv`, `bg`, `ro`, `ar`, `cs`, `el`, `fi`, `hr`, `ms`, `sk`, `da`, `ta`, `uk`, `ru` |
| `eleven_english_sts_v2` | English-only voice changer model (Speech to Speech) | `en` |
| `scribe_v2_realtime` | Real-time speech recognition model | [90+ languages](https://elevenlabs.io/docs/overview/capabilities/speech-to-text#supported-languages) |
| `scribe_v2` | State-of-the-art speech recognition model | [90+ languages](https://elevenlabs.io/docs/overview/capabilities/speech-to-text#supported-languages) |
| `scribe_v1` | State-of-the-art speech recognition. Outclassed by v2 models | [90+ languages](https://elevenlabs.io/docs/overview/capabilities/speech-to-text#supported-languages) |

† Excluding application & network latency

### Deprecated models

The `eleven_monolingual_v1` and `eleven_multilingual_v1` models are deprecated and will be removed in the future. Please migrate to newer models for continued service.

| Model ID | Description | Languages | Replacement model suggestion |
| --- | --- | --- | --- |
| `eleven_monolingual_v1` | First generation TTS model (outclassed by v2 models) | `en` | `eleven_multilingual_v2` |
| `eleven_multilingual_v1` | First multilingual model (outclassed by v2 models) | `en`, `fr`, `de`, `hi`, `it`, `pl`, `pt`, `es` | `eleven_multilingual_v2` |

Eleven v3 (alpha)
-----------------

This model is currently in alpha and is subject to change. Eleven v3 is not made for real-time applications like Agents Platform. When integrating Eleven v3 into your application, consider generating several generations and allowing the user to select the best one.

Eleven v3 is our latest and most advanced speech synthesis model. It is a state-of-the-art model that produces natural, life-like speech with high emotional range and contextual understanding across multiple languages.

This model works well in the following scenarios:

*   **Character Discussions**: Excellent for audio experiences with multiple characters that interact with each other.
*   **Audiobook Production**: Perfect for long-form narration with complex emotional delivery.
*   **Emotional Dialogue**: Generate natural, lifelike dialogue with high emotional range and contextual understanding.

With Eleven v3 comes a new Text to Dialogue API, which allows you to generate natural, lifelike dialogue with high emotional range and contextual understanding across multiple languages. Eleven v3 can also be used with the Text to Speech API to generate natural, lifelike speech with high emotional range and contextual understanding across multiple languages.

Read more about the Text to Dialogue API [here](https://elevenlabs.io/docs/overview/capabilities/text-to-dialogue)
.

### Supported languages

The Eleven v3 model supports 70+ languages, including:

_Afrikaans (afr), Arabic (ara), Armenian (hye), Assamese (asm), Azerbaijani (aze), Belarusian (bel), Bengali (ben), Bosnian (bos), Bulgarian (bul), Catalan (cat), Cebuano (ceb), Chichewa (nya), Croatian (hrv), Czech (ces), Danish (dan), Dutch (nld), English (eng), Estonian (est), Filipino (fil), Finnish (fin), French (fra), Galician (glg), Georgian (kat), German (deu), Greek (ell), Gujarati (guj), Hausa (hau), Hebrew (heb), Hindi (hin), Hungarian (hun), Icelandic (isl), Indonesian (ind), Irish (gle), Italian (ita), Japanese (jpn), Javanese (jav), Kannada (kan), Kazakh (kaz), Kirghiz (kir), Korean (kor), Latvian (lav), Lingala (lin), Lithuanian (lit), Luxembourgish (ltz), Macedonian (mkd), Malay (msa), Malayalam (mal), Mandarin Chinese (cmn), Marathi (mar), Nepali (nep), Norwegian (nor), Pashto (pus), Persian (fas), Polish (pol), Portuguese (por), Punjabi (pan), Romanian (ron), Russian (rus), Serbian (srp), Sindhi (snd), Slovak (slk), Slovenian (slv), Somali (som), Spanish (spa), Swahili (swa), Swedish (swe), Tamil (tam), Telugu (tel), Thai (tha), Turkish (tur), Ukrainian (ukr), Urdu (urd), Vietnamese (vie), Welsh (cym)._

Multilingual v2
---------------

Eleven Multilingual v2 is our most advanced, emotionally-aware speech synthesis model. It produces natural, lifelike speech with high emotional range and contextual understanding across multiple languages.

The model delivers consistent voice quality and personality across all supported languages while maintaining the speaker’s unique characteristics and accent.

This model excels in scenarios requiring high-quality, emotionally nuanced speech:

*   **Character Voiceovers**: Ideal for gaming and animation due to its emotional range.
*   **Professional Content**: Well-suited for corporate videos and e-learning materials.
*   **Multilingual Projects**: Maintains consistent voice quality across language switches.
*   **Stable Quality**: Produces consistent, high-quality audio output.

While it has a higher latency & cost per character than Flash models, it delivers superior quality for projects where lifelike speech is important.

Our multilingual v2 models support 29 languages:

_English (USA, UK, Australia, Canada), Japanese, Chinese, German, Hindi, French (France, Canada), Korean, Portuguese (Brazil, Portugal), Italian, Spanish (Spain, Mexico), Indonesian, Dutch, Turkish, Filipino, Polish, Swedish, Bulgarian, Romanian, Arabic (Saudi Arabia, UAE), Czech, Greek, Finnish, Croatian, Malay, Slovak, Danish, Tamil, Ukrainian & Russian._

Flash v2.5
----------

Eleven Flash v2.5 is our fastest speech synthesis model, designed for real-time applications and Agents Platform. It delivers high-quality speech with ultra-low latency (~75ms†) across 32 languages.

The model balances speed and quality, making it ideal for interactive applications while maintaining natural-sounding output and consistent voice characteristics across languages.

This model is particularly well-suited for:

*   **Agents Platform**: Perfect for real-time voice agents and chatbots.
*   **Interactive Applications**: Ideal for games and applications requiring immediate response.
*   **Large-Scale Processing**: Efficient for bulk text-to-speech conversion.

With its lower price point and 75ms latency, Flash v2.5 is the cost-effective option for anyone needing fast, reliable speech synthesis across multiple languages.

Flash v2.5 supports 32 languages - all languages from v2 models plus:

_Hungarian, Norwegian & Vietnamese_

† Excluding application & network latency

### Considerations

###### Text normalization with numbers

When using Flash v2.5, numbers aren’t normalized by default in a way you might expect. For example, phone numbers might be read out in way that isn’t clear for the user. Dates and currencies are affected in a similar manner.

By default, normalization is disabled for Flash v2.5 to maintain the low latency. However, Enterprise customers can now enable text normalization for v2.5 models by setting the `apply_text_normalization` parameter to “on” in your request.

The Multilingual v2 model does a better job of normalizing numbers, so we recommend using it for phone numbers and other cases where number normalization is important.

For low-latency or Agents Platform applications, best practice is to have your LLM [normalize the text](https://elevenlabs.io/docs/overview/capabilities/text-to-speech/best-practices#text-normalization)
 before passing it to the TTS model, or use the `apply_text_normalization` parameter (Enterprise plans only for v2.5 models).

Turbo v2.5
----------

Eleven Turbo v2.5 is our high-quality, low-latency model with a good balance of quality and speed.

This model is an ideal choice for all scenarios where you’d use Flash v2.5, but where you’re willing to trade off latency for higher quality voice generation.

Model selection guide
---------------------

###### Requirements

Quality

Use `eleven_multilingual_v2`

Best for high-fidelity audio output with rich emotional expression

Low-latency

Use Flash models

Optimized for real-time applications (~75ms latency)

Multilingual

Use either either `eleven_multilingual_v2` or `eleven_flash_v2_5`

Both support up to 32 languages

Balanced

Use `eleven_turbo_v2_5`

Good balance between quality and speed

###### Use case

Content creation

Use `eleven_multilingual_v2`

Ideal for professional content, audiobooks & video narration.

Agents Platform

Use `eleven_flash_v2_5`, `eleven_flash_v2`, `eleven_multilingual_v2`, `eleven_turbo_v2_5` or `eleven_turbo_v2`

Perfect for real-time conversational applications

Voice changer

Use `eleven_multilingual_sts_v2`

Specialized for Speech-to-Speech conversion

Character limits
----------------

The maximum number of characters supported in a single text-to-speech request varies by model.

| Model ID | Character limit | Approximate audio duration |
| --- | --- | --- |
| `eleven_v3` | 5,000 | ~5 minutes |
| `eleven_flash_v2_5` | 40,000 | ~40 minutes |
| `eleven_flash_v2` | 30,000 | ~30 minutes |
| `eleven_turbo_v2_5` | 40,000 | ~40 minutes |
| `eleven_turbo_v2` | 30,000 | ~30 minutes |
| `eleven_multilingual_v2` | 10,000 | ~10 minutes |
| `eleven_multilingual_v1` | 10,000 | ~10 minutes |
| `eleven_english_sts_v2` | 10,000 | ~10 minutes |
| `eleven_english_sts_v1` | 10,000 | ~10 minutes |

For longer content, consider splitting the input into multiple requests.

Scribe v2
---------

Scribe v2 is our state-of-the-art speech recognition model designed for accurate transcription across 90+ languages. It provides precise word-level timestamps and advanced features like speaker diarization and dynamic audio tagging.

This model excels in scenarios requiring accurate speech-to-text conversion:

*   **Transcription Services**: Perfect for converting audio/video content to text
*   **Meeting Documentation**: Ideal for capturing and documenting conversations
*   **Content Analysis**: Well-suited for audio content processing and analysis
*   **Multilingual Recognition**: Supports accurate transcription across 90+ languages

Key features:

*   Accurate transcription with word-level timestamps
*   Speaker diarization for multi-speaker audio
*   Dynamic audio tagging for enhanced context
*   Support for 90+ languages
*   Entity detection
*   Keyterm prompting

Read more about Scribe v2 [here](https://elevenlabs.io/docs/overview/capabilities/speech-to-text)
.

Scribe v2 Realtime
------------------

Scribe v2 Realtime, our fastest and most accurate live speech recognition model, delivers state-of-the-art accuracy in over 90 languages with an ultra-low 150ms of latency.

This model excels in conversational use cases:

*   **Live meeting transcription**: Perfect for realtime transcription
*   **AI Agents**: Ideal for live conversations
*   **Multilingual Recognition**: Supports accurate transcription across 90+ languages with automatic language recognition

Key features:

*   Ultra-low latency: Get partial transcriptions in ~150 milliseconds
*   Streaming support: Send audio in chunks while receiving transcripts in real-time
*   Multiple audio formats: Support for PCM (8kHz to 48kHz) and μ-law encoding
*   Voice Activity Detection (VAD): Automatic speech segmentation based on silence detection
*   Manual commit control: Full control over when to finalize transcript segments

Read more about Scribe v2 Realtime [here](https://elevenlabs.io/docs/overview/capabilities/speech-to-text)
.

Eleven Music
------------

Eleven Music is our studio-grade music generation model. It allows you to generate music with natural language prompts in any style.

This model is excellent for the following scenarios:

*   **Game Soundtracks**: Create immersive soundtracks for games
*   **Podcast Backgrounds**: Enhance podcasts with professional music
*   **Marketing**: Add background music to ad reels

Key features:

*   Complete control over genre, style, and structure
*   Vocals or just instrumental
*   Multilingual, including English, Spanish, German, Japanese and more
*   Edit the sound and lyrics of individual sections or the whole song

Read more about Eleven Music [here](https://elevenlabs.io/docs/overview/capabilities/music)
.

Concurrency and priority
------------------------

Your subscription plan determines how many requests can be processed simultaneously and the priority level of your requests in the queue. Speech to Text has an elevated concurrency limit. Once the concurrency limit is met, subsequent requests are processed in a queue alongside lower-priority requests. In practice this typically only adds ~50ms of latency.

| Plan | Concurrency Limit  <br>(Multilingual v2) | Concurrency Limit  <br>(Turbo & Flash) | STT Concurrency Limit | Realtime STT Concurrency limit | Music Concurrency limit | Priority level |
| --- | --- | --- | --- | --- | --- | --- |
| Free | 2   | 4   | 8   | 4   | 0   | 3   |
| Starter | 3   | 6   | 12  | 6   | 2   | 4   |
| Creator | 5   | 10  | 20  | 10  | 2   | 5   |
| Pro | 10  | 20  | 40  | 20  | 2   | 5   |
| Scale | 15  | 30  | 60  | 30  | 5   | 5   |
| Business | 15  | 30  | 60  | 30  | 5   | 5   |
| Enterprise | Elevated | Elevated | Elevated | Elevated | Highest | 6   |

Startup grants recipients receive Scale level benefits.

The response headers include `current-concurrent-requests` and `maximum-concurrent-requests` which you can use to monitor your concurrency.

### API requests per minute vs concurrent requests

It’s important to understand that **API requests per minute** and **concurrent requests** are different metrics that depend on your usage patterns.

API requests per minute can be different from concurrent requests since it depends on the length of time for each request and how the requests are batched.

**Example 1: Spaced requests** If you had 180 requests per minute that each took 1 second to complete and you sent them each 0.33 seconds apart, the max concurrent requests would be 3 and the average would be 3 since there would always be 3 in flight.

**Example 2: Batched requests** However, if you had a different usage pattern such as 180 requests per minute that each took 3 seconds to complete but all fired at once, the max concurrent requests would be 180 and the average would be 9 (first 3 seconds of the minute saw 180 requests at once, final 57 seconds saw 0 requests).

Since our system cares about concurrency, requests per minute matter less than how long each of the requests take and the pattern of when they are sent.

How endpoint requests are made impacts concurrency limits:

*   With HTTP, each request counts individually toward your concurrency limit.
*   With a WebSocket, only the time where our model is generating audio counts towards your concurrency limit, this means a for most of the time an open websocket doesn’t count towards your concurrency limit at all.

### Understanding concurrency limits

The concurrency limit associated with your plan should not be interpreted as the maximum number of simultaneous conversations, phone calls character voiceovers, etc that can be handled at once. The actual number depends on several factors, including the specific AI voices used and the characteristics of the use case.

As a general rule of thumb, a concurrency limit of 5 can typically support up to approximately 100 simultaneous audio broadcasts.

This is because of the speed it takes for audio to be generated relative to the time it takes for the TTS request to be processed. The diagram below is an example of how 4 concurrent calls with different users can be facilitated while only hitting 2 concurrent requests.

![Concurrency limits](https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffiles.buildwithfern.com%2Fhttps%3A%2F%2Felevenlabs.docs.buildwithfern.com%2Fdocs%2Fdcc5e3bd18993a9f862bd526f3dc1b32cfa89003a58ded6f4f6a7bda1bd5a2ea%2Fassets%2Fimages%2Fproduct-guides%2Fspeech-to-text%2Ftts-concurrency.png&w=3840&q=75)

###### Building AI Voice Agents

Where TTS is used to facilitate dialogue, a concurrency limit of 5 can support about 100 broadcasts for balanced conversations between AI agents and human participants.

For use cases in which the AI agent speaks less frequently than the human, such as customer support interactions, more than 100 simultaneous conversations could be supported.

###### Character voiceovers

Generally, more than 100 simultaneous character voiceovers can be supported for a concurrency limit of 5.

The number can vary depending on the character’s dialogue frequency, the length of pauses, and in-game actions between lines.

###### Live Dubbing

Concurrent dubbing streams generally follow the provided heuristic.

If the broadcast involves periods of conversational pauses (e.g. because of a soundtrack, visual scenes, etc), more simultaneous dubbing streams than the suggestion may be possible.

If you exceed your plan’s concurrency limits at any point and you are on the Enterprise plan, model requests may still succeed, albeit slower, on a best efforts basis depending on available capacity.

To increase your concurrency limit & queue priority, [upgrade your subscription plan](https://elevenlabs.io/pricing/api)
.

Enterprise customers can request a higher concurrency limit by contacting their account manager.

### Scale testing concurrency limits

Scale testing can be useful to identify client side scaling issues and to verify concurrency limits are set correctly for your usecase.

It is heavily recommended to test end-to-end workflows as close to real world usage as possible, simulating and measuring how many users can be supported is the recommended methodology for achieving this. It is important to:

*   Simulate users, not raw requests
*   Simulate typical user behavior such as waiting for audio playback, user speaking or transcription to finish before making requests
*   Ramp up the number of users slowly over a period of minutes
*   Introduce randomness to request timings and to the size of requests
*   Capture latency metrics and any returned error codes from the API

For example, to test an agent system designed to support 100 simultaneous conversations you would create up to 100 individual “users” each simulating a conversation. Conversations typically consist of a repeating cycle of ~10 seconds of user talking, followed by the TTS API call for ~150 characters, followed by ~10 seconds of audio playback to the user. Therefore, each user should follow the pattern of making a websocket Text-to-Speech API call for 150 characters of text every 20 seconds, with a small amount of randomness introduced to the wait period and the number of characters requested. The test would consist of spawning one user per second until 100 exist and then testing for 10 minutes in total to test overall stability.

###### Scale testing script example

This example uses [locust](https://locust.io/)
 as the testing framework with direct API calls to the ElevenLabs API.

It follows the example listed above, testing a conversational agent system with each user sending 1 request every 20 seconds.

Python

|     |     |
| --- | --- |
| 1   | import json |
| 2   | import random |
| 3   | import time |
| 4   | import gevent |
| 5   | import locust |
| 6   | from locust import User, task, events, constant\_throughput |
| 7   | import websocket |
| 8   |     |
| 9   | \# Averages up to 10 seconds of audio when played, depends on the voice speed |
| 10  | DEFAULT\_TEXT = ( |
| 11  | "Hello, this is a test message. I am testing if a long input will cause issues for the model " |
| 12  | "like this sentence. " |
| 13  | )   |
| 14  |     |
| 15  | TEXT\_ARRAY = \[ |\
| 16  | "Hello.", |\
| 17  | "Hello, this is a test message.", |\
| 18  | DEFAULT\_TEXT, |\
| 19  | DEFAULT\_TEXT \* 2, |\
| 20  | DEFAULT\_TEXT \* 3 |\
| 21  | \]  |
| 22  |     |
| 23  | \# Custom command line arguments |
| 24  | @events.init\_command\_line\_parser.add\_listener |
| 25  | def on\_parser\_init(parser): |
| 26  | parser.add\_argument("--api-key", default="YOUR\_API\_KEY", help="API key for authentication") |
| 27  | parser.add\_argument("--encoding", default="mp3\_22050\_32", help="Encoding") |
| 28  | parser.add\_argument("--text", default=DEFAULT\_TEXT, help="Text to use") |
| 29  | parser.add\_argument("--use-text-array", default="false", help="Text to use") |
| 30  | parser.add\_argument("--voice-id", default="aria", help="Text to use") |
| 31  |     |
| 32  |     |
| 33  | class WebSocketTTSUser(User): |
| 34  | # Each user will send a request every 20 seconds, regardless of how long each request takes |
| 35  | wait\_time = constant\_throughput(0.05) |
| 36  |     |
| 37  | def \_\_init\_\_(self, \*args, \*\*kwargs): |
| 38  | super().\_\_init\_\_(\*args, \*\*kwargs) |
| 39  | self.api\_key = self.environment.parsed\_options.api\_key |
| 40  | self.voice\_id = self.environment.parsed\_options.voice\_id |
| 41  | self.text = self.environment.parsed\_options.text |
| 42  | self.encoding = self.environment.parsed\_options.encoding |
| 43  | self.use\_text\_array = self.environment.parsed\_options.use\_text\_array |
| 44  | if self.use\_text\_array: |
| 45  | self.text = random.choice(TEXT\_ARRAY) |
| 46  | self.all\_recieved = False |
| 47  |     |
| 48  | @task |
| 49  | def tts\_task(self): |
| 50  | # Do jitter waiting of up to 1 second |
| 51  | # Users appear to be spawned every second so this ensures requests are not aligned |
| 52  | gevent.sleep(random.random()) |
| 53  |     |
| 54  | max\_wait\_time = 10 |
| 55  |     |
| 56  | # Connection details |
| 57  | uri = f"{self.environment.host}/v1/text-to-speech/{self.voice\_id}/stream-input?auto\_mode=true&output\_format={self.encoding}" |
| 58  | headers = {"xi-api-key": self.api\_key} |
| 59  |     |
| 60  | ws = None |
| 61  | self.all\_recieved = False |
| 62  | try: |
| 63  | init\_msg = {"text": " "} |
| 64  | # Use proper header format for websocket - this is case sensitive! |
| 65  | ws = websocket.create\_connection(uri, header=headers) |
| 66  | ws.send(json.dumps(init\_msg)) |
| 67  |     |
| 68  | # Start measuring after websocket initiated but before any messages are sent |
| 69  | send\_request\_time = time.perf\_counter() |
| 70  | ws.send(json.dumps({"text": self.text})) |
| 71  |     |
| 72  | # Send to flush and receive the audio |
| 73  | ws.send(json.dumps({"text": ""})) |
| 74  |     |
| 75  | def \_receive(): |
| 76  | t\_first\_response = None |
| 77  | audio\_size = 0 |
| 78  | try: |
| 79  | while True: |
| 80  | # Wait up to 10 seconds for a response |
| 81  | ws.settimeout(max\_wait\_time) |
| 82  | response = ws.recv() |
| 83  | response\_data = json.loads(response) |
| 84  |     |
| 85  | if "audio" in response\_data and response\_data\["audio"\]: |
| 86  | audio\_size = audio\_size + len(response\_data\["audio"\]) |
| 87  |     |
| 88  | if t\_first\_response is None: |
| 89  | t\_first\_response = time.perf\_counter() |
| 90  | first\_byte\_ms = ( |
| 91  | t\_first\_response - send\_request\_time |
| 92  | ) \* 1000 |
| 93  | if audio\_size is None: |
| 94  | # The first response should always have audio |
| 95  | locust.events.request.fire( |
| 96  | request\_type="websocket", |
| 97  | name="Bad Response (no audio)", |
| 98  | response\_time=first\_byte\_ms, |
| 99  | response\_length=audio\_size, |
| 100 | exception=Exception("Response has no audio"), |
| 101 | )   |
| 102 | break |
| 103 |     |
| 104 | if "isFinal" in response\_data and response\_data\["isFinal"\]: |
| 105 | # Fire this event once finished streaming, but report the important TTFB metric |
| 106 | locust.events.request.fire( |
| 107 | request\_type="websocket", |
| 108 | name="TTS Stream Success (First Byte)", |
| 109 | response\_time=first\_byte\_ms, |
| 110 | response\_length=audio\_size, |
| 111 | exception=None, |
| 112 | )   |
| 113 | break |
| 114 |     |
| 115 | except websocket.WebSocketTimeoutException: |
| 116 | locust.events.request.fire( |
| 117 | request\_type="websocket", |
| 118 | name="TTS Stream Timeout", |
| 119 | response\_time=max\_wait\_time \* 1000, |
| 120 | response\_length=audio\_size, |
| 121 | exception=Exception("Timeout waiting for response"), |
| 122 | )   |
| 123 | except Exception as e: |
| 124 | # Typically JSON decode error if the server returns HTTP backoff error |
| 125 | locust.events.request.fire( |
| 126 | request\_type="websocket", |
| 127 | name="TTS Stream Failure", |
| 128 | response\_time=0, |
| 129 | response\_length=0, |
| 130 | exception=e, |
| 131 | )   |
| 132 | finally: |
| 133 | self.all\_recieved = True |
| 134 |     |
| 135 | gevent.spawn(\_receive) |
| 136 |     |
| 137 | # Sleep until recieved so new tasks aren't spawned |
| 138 | while not self.all\_recieved: |
| 139 | gevent.sleep(1) |
| 140 |     |
| 141 | except websocket.WebSocketTimeoutException: |
| 142 | locust.events.request.fire( |
| 143 | request\_type="websocket", |
| 144 | name="TTS Stream Timeout", |
| 145 | response\_time=max\_wait\_time \* 1000, |
| 146 | response\_length=0, |
| 147 | exception=Exception("Timeout waiting for response"), |
| 148 | )   |
| 149 | except Exception as e: |
| 150 | locust.events.request.fire( |
| 151 | request\_type="websocket", |
| 152 | name="TTS Stream Failure", |
| 153 | response\_time=0, |
| 154 | response\_length=0, |
| 155 | exception=e, |
| 156 | )   |
| 157 | finally: |
| 158 | # Try and close the websocket gracefully |
| 159 | try: |
| 160 | if ws: |
| 161 | ws.close() |
| 162 | except Exception: |
| 163 | pass |
